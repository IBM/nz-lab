{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Netezza Performance Server Lab In this lab will explore the IBM Netezza Performance Server.","title":"Netezza Performance Server Lab"},{"location":"#netezza-performance-server-lab","text":"In this lab will explore the IBM Netezza Performance Server.","title":"Netezza Performance Server Lab"},{"location":"nz-01-NPS-CLI/","text":"Command Line Interface In this lab will explore the IBM Netezza Performance Server command line (CLI). The CLI will be our primary interface to execute the labs in the workshop. NPS and Tricks on Using the Netezza Performance Server Virtual Machines The Netezza Performance Server is designed and fine-tuned for specific Cloud Pak for Data System hardware or Netezza Performance Service server on Cloud. In order to demonstrate the system in a virtualized environment, some adaptations were made on the virtual machines. To ensure the labs run smoothly, we have listed some pointers for using the VMs: After pausing the virtual machines, nz services need to be restarted. In the case that the VMs were paused (the host operating system went into sleep or hibernation modes, the images were paused in VMware Workstation,,, etc). To continue using the VMs, run the following commands in the prompt of the Host image. Input nzstop nzstart Lab Environment The lab system will be a virtual machine running on Virtual Box. Please see the document on how to install the NPS Virtual Machine for your workstation (Windows or Mac OS). Connect to the Netezza Performance Server Use the following information on how to connect to the lab system. There are two options to access the command line: Login to the VM directly and use the terminal application available inside the VM. Local terminal application on your workstation. Login to NPS Command Line using the Virtual Machine GUI After getting the VM running (see the document on how to install the VM) login as follows: Login as the nz user by clicking in the VM and select the nz user: At the password prompt enter : nz and click Sign In Once you are signed in you can right-click the desktop and select \"Open Terminal\" Start NPS if not already started as follows: Input nzstate nzstart nzstate You are now ready to proceed to the next section of the lab. Login to NPS Command Line for Mac Users From your Mac OSX system, open the terminal application (or another terminal application like iTerm2). Open the Terminal.app. Type command + spacebar. Type \"Terminal.app\" in the Spotlight Search window. Double click \"Terminal.app\" to launch the Terminal application. From the Terminal application ssh into the NPS VM with the nz (password:nz) ID and start NPS if not already started. Input ssh nz@192.168.9.2 nzstate nzstart nzstate The remainder of the lab will use the command line as the nz user. Login to NPS Command Line for Windows Users Start a Windows PowerShell, press the Windows Key and type powersh in the Search Bar. Click \"Windows PowerShell\" From the PowerShell application ssh into the NPS VM with the nz ID and start NPS if not already started. Input ssh nz@192.168.9.2 nzstate If the system is offline: Input nzstart nzstate The remainder of the lab will use the command line as the nz user. Lab Setup This lab uses an initial setup script to make sure the correct users and databases exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. Login to the VM directly and use the terminal application available inside the VM. Connect to your Netezza Performance Server image using a terminal application (Windows PowerShell, PuTTY, Mac OSX Terminal) If you are continuing from the previous lab and are already connected to nzsql, quit the nzsql console with the \\q command. Prepare for this lab by running the setup script. To do this use the following commands: Input cd ~/labs/cli/setupLab time ./setupLab.sh Output similar to the following will be produced. Output ERROR: DROP DATABASE: object LABDB does not exist. CREATE DATABASE ERROR: DROP USER: object LABADMIN does not exist. CREATE USER ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully real 1m46.490s user 0m0.082s sys 0m0.784s The error message at the beginning is expected since the database LABDB and user LABADMIN hasn't been created yet. However, the DROP statements are in the setup script in the event the user LABADMIN and database LABDB exist to start this lab new. Dan - Check this text Note: you downloaded and installed the scripts in the lab: \"01-Setup-NPS-Virtual-Machine-Setup-Lab-Guide-Win10-OSX-Final\u201d This lab is now setup and ready to use for the remainder of the sections. Connecting to the System Database The following section details how to connect to the system database which contains all the system views that describes the database objects. Connect to the Netezza System Database Using nzsql Since we have not created any user and databases yet, we will connect to the default database as the default user, with the following credentials: Database: system Username: admin Password: password When issuing the nzsql command, the user supplies the user account, password and the database to connect to using the syntax, below is an example of how this would be done. Do not try to execute that command it is just demonstrating the syntax: nzsql \u2013d [ db_name ] \u2013u [ user ] \u2013pw [ password ] Alternatively, these values can be stored in the command shell and passed to the nzsql command when it is issued without any arguments. Let's verify the current database, user and password values stored in the command shell by issuing the printenv NZ_DATABASE , printenv NZ_USER , and printenv NZ_PASSWORD commands. Input printenv NZ_DATABASE printenv NZ_USER printenv NZ_PASSWORD The output should look similar to the following: Output [ nz@netezza ~ ] $ printenv NZ_DATABASE system [ nz@netezza ~ ] $ printenv NZ_USER admin [ nz@netezza ~ ] $ printenv NZ_PASSWORD password Since the current values correspond to our desired values, no modification is required. Next, let's take a look at what options are available to start nzsql. Type in the following command: Input nzsql -? Output This is nzsql, the IBM Netezza SQL interactive terminal. Usage: nzsql [ options ] [ security options ] [ dbname [ username ] [ password ]] Security Options: -securityLevel Security Level you wish to request ( default: preferredUnSecured ) -caCertFile ROOT CA certificate file ( default: NULL ) Options: -a Echo all input from script -A Unaligned table output mode ( -P format = unaligned ) -c <query> Run only single query ( or slash command ) and exit -d <dbname> Specify database name to connect to ( default: SYSTEM ) -D <dbname> Specify database name to connect to ( default: SYSTEM ) -schema <schemaname> Specify schema name to connect to ( default: $NZ_SCHEMA ) -e Echo queries sent to backend -E Display queries that internal commands generate -f <filename> Execute queries from file, then exit -F <string> Set field separator ( default: \"|\" ) ( -P fieldsep =) For any binary/control/non-printable character use '$' ( e.g., nzsql -F $'\\t' // for TAB ) -host <host> Specify database server host ( default: localhost. localdomain ) -h <host> Specify database server host ( default: localhost. localdomain ) -H HTML table output mode ( -P format = html ) -l List available databases, then exit -n Disable readline -o <filename> Send query output to filename ( or | pipe ) -O <filename> Send query output with errors to filename ( or | pipe ) -port <port> Specify database server port ( default: hardwired ) -P var [= arg ] Set printing option 'var' to 'arg' ( see \\p set command ) -q Run quietly ( no messages, only query output ) -r Suppress row count in query output -R <string> Set record separator ( default: newline ) ( -P recordsep =) -Rev Show version information and exit -rev Show version information and exit -s Single step mode ( confirm each query ) -S Single line mode ( newline terminates query ) -t Print rows only ( -P tuples_only ) -time Print time taken by queries -T text Set HTML table tag options ( width, border ) ( -P tableattr =) -u <username> Specify database username ( default: admin ) -U <username> Specify database username ( default: admin ) -v name = val Set nzsqlvariable 'name' to 'value' -V Show version information and exit -w Don ' t require password, other mechanisms ( Kerberos ) will supply it -W <password> Specify the database user password -pw <password> Specify the database user password -x Turn on expanded table output ( -P expanded ) -X Do not read startup file ( ~/.nzsqlrc ) -h or -? or --help Display this help For more information, type \"\\?\" ( for internal commands ) or \"\\help\" ( for SQL commands ) from within nzsql. For more information, type \"\\?\" (for internal commands) or \"\\help\" (for SQL commands) from within nzsql. \\t``` The -? option will list the usage and all options for the nzsql command. In this exercise, we will start nzsql without arguments. In the command prompt, issue the command: Input nzsql This will bring up the nzsql prompt below that shows a connection to the system database as user admin: Output [ nz@localhost ~ ] $ nzsql Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM.ADMIN ( ADMIN )= > Commonly Used Commands and SQL Statements There are commonly used commands that start with \"\\\u201d which we will demonstrate in this section. First, we will run the 2 help commands to familiarize ourselves with these handy commands. The \\h command will list the available SQL commands, while the \\? command is used to list the internal slash commands. Examine the output for both commands: Input \\h \\? From the output of the \\? command, we found the \\l internal command we can use to find out all the databases. Let's find out all the databases by entering \\l : Input \\l Output List of databases DATABASE | OWNER ----------+---------- LABDB | LABADMIN SYSTEM | ADMIN ( 10 rows ) Secondly, we will use \\dSv to find out the system views within the system database. Note: there are system tables, however, it isn't recommended to directly access those tables as there can change from release to release and are restricted from the normal user. Input: Input \\d Sv Output List of relations Schema | Name | Type | Owner --------------------+---------------------------------------+------------- +------- DEFINITION_SCHEMA | _V_ACL_DATA | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_AGGREGATE | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_ATTRIBUTE | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_ATTRIBUTE2 | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_AUTHENTICATION | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_AUTHENTICATION_SETTINGS | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_GROUP | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_GROUP_HISTORY | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_GROUP_MEMBER | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_HISTORY | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_TABLE_HISTORY | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BLOWER | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BNR_CONNECTOR | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_CLASS | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_CLASS2 | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_CLIENT_COMPATIBILITY | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_CONNECTION | SYSTEM VIEW | ADMIN --More\u2014 Note: press the space bar to scroll down the result set when you see --More-- on the screen. The list of the system views was truncated in the output above due to the length. Here are the primary views to investigate for DBAs/users new to Netezza: Output _V_GROUP _V_USER _V_SESSION_V_DATABASE _V_SCHEMA _V_TABLE _V_VIEW _V_ATTRIBUTE _V_SEQUENCE _V_SYNONYM _V_FUNCTION _V_AGGREGATE _V_PROCEDURE From the previous command, we can see that there is a user table called _V_USER . To find out what is stored in that table, we will use the describe command \\d: Input \\d _V_USER Output View \"_V_USER\" Attribute | Type | Modifier | Default Value ---------------------+------------------------+----------+--------------- OBJID | OID | NOT NULL | USERNAME | NAME | NOT NULL | OWNER | NAME | | VALIDUNTIL | TIMESTAMP | | CREATEDATE | ABSTIME | NOT NULL | ROWLIMIT | INTEGER | | ACCT_LOCKED | BOOLEAN | | INV_CONN_CNT | SMALLINT | | PWD_INVALID | BOOLEAN | | PWD_LAST_CHGED | DATE | | SESSIONTIMEOUT | INTEGER | | QUERYTIMEOUT | INTEGER | | DEF_PRIORITY | NAME | | MAX_PRIORITY | NAME | | USESYSID | INTEGER | | OBJDELIM | BOOLEAN | NOT NULL | USERESOURCEGRPID | OID | | USERESOURCEGRPNAME | NAME | | CROSS_JOINS_ALLOWED | CHARACTER VARYING ( 255 ) | | USEAUTH | CHARACTER VARYING ( 255 ) | | PWD_EXPIRY | INTEGER | | View definition: SELECT O.OBJID, O.OBJNAME AS USERNAME, O. \"OWNER\" , U.VALUNTIL AS VALIDUNTIL, O.CREATEDATE, U.USERESULTLIMIT AS ROWLIMIT, U.USELOCKED AS ACCT_LOCKED, U.USEINVCONNCNT AS INV_CONN_CNT, U.USEPWDINV AS PWD_INVALID, U. USEPWDCHGED AS PWD_LAST_CHGED, U.USESESSTIMELIMIT AS \"SESSIONTIMEOUT\" , U. USEQRYDURATION AS \"QUERYTIMEOUT\" , DP.PRILITERAL AS DEF_PRIORITY, MP.PRILITERAL AS MAX_PRIORITY, U.USESYSID, O.OBJDELIM, U.USERESOURCEGRP AS USERESOURCEGRPID, CASE WHEN ( O.OBJID = 4900 ) THEN \"NAME\" (( '_ADMIN_' :: \"NVARCHAR\" ) ::NVARCHAR ( 255 )) WHEN (( U.USERESOURCEGRP ISNULL ) OR ( U.USERESOURCEGRP = 4901 )) THEN \"NAME\" (( 'PUBLIC' :: \"NVARCHAR\" ) ::NVARCHAR ( 255 )) ELSE G.OBJNAME END AS USERESOURCEGRPNAME, CASE WHEN ( U.USECROSSJOIN ISNULL ) THEN ( 'NULL' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) WHEN ( U.USECROSSJOIN = 0 ) THEN ( 'NULL' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) WHEN ( U.USECROSSJOIN = 1 ) THEN ( 'FALSE' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) ELSE ( 'TRUE' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) END AS CROSS_JOINS_ALLOWED, CASE WHEN ( U. USEAUTH = 1 ) THEN ( 'LOCAL' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) ELSE ( 'DEFAULT' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) END AS USEAUTH, U.USEPASSWDEXPIRY AS PWD_EXPIRY FROM (((( DEFINITION_SCHEMA. \"_V_OBJ_USER\" O JOIN DEFINITION_SCHEMA. \"_T_USER\" U ON (( O. OBJID = U.OID ))) LEFT JOIN DEFINITION_SCHEMA. \"_T_PRIORITY\" DP ON (( U. USEDEFPRIORITY = DP.PRICODE ))) LEFT JOIN DEFINITION_SCHEMA. \"_T_PRIORITY\" MP ON (( U.USEMAXPRIORITY = MP.PRICODE ))) LEFT JOIN DEFINITION_SCHEMA. \"_V_OBJ_GROUP\" G ON (( U.USERESOURCEGRP = G.OBJID ))) ; This will return all the columns of the _V_USER system table. Next, we want to know the existing users stored in the table. In case too many rows are returned at once, we will first calculate the number of rows it contains by enter the following query: Input SELECT COUNT ( * ) FROM ( SELECT * FROM _V_USER ) AS \"Wrapper\" ; Output COUNT ------- 2 ( 1 row ) The query above is essentially the same as SELECT COUNT (*) FROM _V_USER , we have demonstrated the sub-select syntax in case there is a complex query that needed to have the result set evaluated. The result should show there is currently 2 entries in the user table. We can enter the following query to list the user names: Input select objid, username, owner, createdate, useauth, pwd_expiry from _v_user ; Output OBJID | USERNAME | OWNER | CREATEDATE | USEAUTH | PWD_EXPIRY --------+----------+-------+---------------------+---------+------------ 4900 | ADMIN | ADMIN | 2020 -01-24 08 :26:22 | DEFAULT | 0 210500 | LABADMIN | ADMIN | 2020 -05-11 07 :58:31 | DEFAULT | 0 Exit nzsql To exit nzsql, use the command \\q to return to the Netezza Performance Server system. Input \\q nzsql Command The nzsql command invokes a SQL command interpreter on the IBM\u00ae Netezza\u00ae host or on an IBM Netezza client system. You can use this SQL command interpreter to create database objects, run queries, and manage the database. To run the nzsql command, enter: Input nzsql [ options ] [ security options ] [ dbname [ user ] [ password ]] The following table describes the nzsql command parameters. You will be using the nzsql command line tool throughout the labs.","title":"Command Line Interface"},{"location":"nz-01-NPS-CLI/#command-line-interface","text":"In this lab will explore the IBM Netezza Performance Server command line (CLI). The CLI will be our primary interface to execute the labs in the workshop.","title":"Command Line Interface"},{"location":"nz-01-NPS-CLI/#nps-and-tricks-on-using-the-netezza-performance-server-virtual-machines","text":"The Netezza Performance Server is designed and fine-tuned for specific Cloud Pak for Data System hardware or Netezza Performance Service server on Cloud. In order to demonstrate the system in a virtualized environment, some adaptations were made on the virtual machines. To ensure the labs run smoothly, we have listed some pointers for using the VMs: After pausing the virtual machines, nz services need to be restarted. In the case that the VMs were paused (the host operating system went into sleep or hibernation modes, the images were paused in VMware Workstation,,, etc). To continue using the VMs, run the following commands in the prompt of the Host image. Input nzstop nzstart","title":"NPS and Tricks on Using the Netezza Performance Server Virtual Machines"},{"location":"nz-01-NPS-CLI/#lab-environment","text":"The lab system will be a virtual machine running on Virtual Box. Please see the document on how to install the NPS Virtual Machine for your workstation (Windows or Mac OS).","title":"Lab Environment"},{"location":"nz-01-NPS-CLI/#connect-to-the-netezza-performance-server","text":"Use the following information on how to connect to the lab system. There are two options to access the command line: Login to the VM directly and use the terminal application available inside the VM. Local terminal application on your workstation.","title":"Connect to the Netezza Performance Server"},{"location":"nz-01-NPS-CLI/#login-to-nps-command-line-using-the-virtual-machine-gui","text":"After getting the VM running (see the document on how to install the VM) login as follows: Login as the nz user by clicking in the VM and select the nz user: At the password prompt enter : nz and click Sign In Once you are signed in you can right-click the desktop and select \"Open Terminal\" Start NPS if not already started as follows: Input nzstate nzstart nzstate You are now ready to proceed to the next section of the lab.","title":"Login to NPS Command Line using the Virtual Machine GUI"},{"location":"nz-01-NPS-CLI/#login-to-nps-command-line-for-mac-users","text":"From your Mac OSX system, open the terminal application (or another terminal application like iTerm2). Open the Terminal.app. Type command + spacebar. Type \"Terminal.app\" in the Spotlight Search window. Double click \"Terminal.app\" to launch the Terminal application. From the Terminal application ssh into the NPS VM with the nz (password:nz) ID and start NPS if not already started. Input ssh nz@192.168.9.2 nzstate nzstart nzstate The remainder of the lab will use the command line as the nz user.","title":"Login to NPS Command Line for Mac Users"},{"location":"nz-01-NPS-CLI/#login-to-nps-command-line-for-windows-users","text":"Start a Windows PowerShell, press the Windows Key and type powersh in the Search Bar. Click \"Windows PowerShell\" From the PowerShell application ssh into the NPS VM with the nz ID and start NPS if not already started. Input ssh nz@192.168.9.2 nzstate If the system is offline: Input nzstart nzstate The remainder of the lab will use the command line as the nz user.","title":"Login to NPS Command Line for Windows Users"},{"location":"nz-01-NPS-CLI/#lab-setup","text":"This lab uses an initial setup script to make sure the correct users and databases exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. Login to the VM directly and use the terminal application available inside the VM. Connect to your Netezza Performance Server image using a terminal application (Windows PowerShell, PuTTY, Mac OSX Terminal) If you are continuing from the previous lab and are already connected to nzsql, quit the nzsql console with the \\q command. Prepare for this lab by running the setup script. To do this use the following commands: Input cd ~/labs/cli/setupLab time ./setupLab.sh Output similar to the following will be produced. Output ERROR: DROP DATABASE: object LABDB does not exist. CREATE DATABASE ERROR: DROP USER: object LABADMIN does not exist. CREATE USER ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully real 1m46.490s user 0m0.082s sys 0m0.784s The error message at the beginning is expected since the database LABDB and user LABADMIN hasn't been created yet. However, the DROP statements are in the setup script in the event the user LABADMIN and database LABDB exist to start this lab new. Dan - Check this text Note: you downloaded and installed the scripts in the lab: \"01-Setup-NPS-Virtual-Machine-Setup-Lab-Guide-Win10-OSX-Final\u201d This lab is now setup and ready to use for the remainder of the sections.","title":"Lab Setup"},{"location":"nz-01-NPS-CLI/#connecting-to-the-system-database","text":"The following section details how to connect to the system database which contains all the system views that describes the database objects.","title":"Connecting to the System Database"},{"location":"nz-01-NPS-CLI/#connect-to-the-netezza-system-database-using-nzsql","text":"Since we have not created any user and databases yet, we will connect to the default database as the default user, with the following credentials: Database: system Username: admin Password: password When issuing the nzsql command, the user supplies the user account, password and the database to connect to using the syntax, below is an example of how this would be done. Do not try to execute that command it is just demonstrating the syntax: nzsql \u2013d [ db_name ] \u2013u [ user ] \u2013pw [ password ] Alternatively, these values can be stored in the command shell and passed to the nzsql command when it is issued without any arguments. Let's verify the current database, user and password values stored in the command shell by issuing the printenv NZ_DATABASE , printenv NZ_USER , and printenv NZ_PASSWORD commands. Input printenv NZ_DATABASE printenv NZ_USER printenv NZ_PASSWORD The output should look similar to the following: Output [ nz@netezza ~ ] $ printenv NZ_DATABASE system [ nz@netezza ~ ] $ printenv NZ_USER admin [ nz@netezza ~ ] $ printenv NZ_PASSWORD password Since the current values correspond to our desired values, no modification is required. Next, let's take a look at what options are available to start nzsql. Type in the following command: Input nzsql -? Output This is nzsql, the IBM Netezza SQL interactive terminal. Usage: nzsql [ options ] [ security options ] [ dbname [ username ] [ password ]] Security Options: -securityLevel Security Level you wish to request ( default: preferredUnSecured ) -caCertFile ROOT CA certificate file ( default: NULL ) Options: -a Echo all input from script -A Unaligned table output mode ( -P format = unaligned ) -c <query> Run only single query ( or slash command ) and exit -d <dbname> Specify database name to connect to ( default: SYSTEM ) -D <dbname> Specify database name to connect to ( default: SYSTEM ) -schema <schemaname> Specify schema name to connect to ( default: $NZ_SCHEMA ) -e Echo queries sent to backend -E Display queries that internal commands generate -f <filename> Execute queries from file, then exit -F <string> Set field separator ( default: \"|\" ) ( -P fieldsep =) For any binary/control/non-printable character use '$' ( e.g., nzsql -F $'\\t' // for TAB ) -host <host> Specify database server host ( default: localhost. localdomain ) -h <host> Specify database server host ( default: localhost. localdomain ) -H HTML table output mode ( -P format = html ) -l List available databases, then exit -n Disable readline -o <filename> Send query output to filename ( or | pipe ) -O <filename> Send query output with errors to filename ( or | pipe ) -port <port> Specify database server port ( default: hardwired ) -P var [= arg ] Set printing option 'var' to 'arg' ( see \\p set command ) -q Run quietly ( no messages, only query output ) -r Suppress row count in query output -R <string> Set record separator ( default: newline ) ( -P recordsep =) -Rev Show version information and exit -rev Show version information and exit -s Single step mode ( confirm each query ) -S Single line mode ( newline terminates query ) -t Print rows only ( -P tuples_only ) -time Print time taken by queries -T text Set HTML table tag options ( width, border ) ( -P tableattr =) -u <username> Specify database username ( default: admin ) -U <username> Specify database username ( default: admin ) -v name = val Set nzsqlvariable 'name' to 'value' -V Show version information and exit -w Don ' t require password, other mechanisms ( Kerberos ) will supply it -W <password> Specify the database user password -pw <password> Specify the database user password -x Turn on expanded table output ( -P expanded ) -X Do not read startup file ( ~/.nzsqlrc ) -h or -? or --help Display this help For more information, type \"\\?\" ( for internal commands ) or \"\\help\" ( for SQL commands ) from within nzsql. For more information, type \"\\?\" (for internal commands) or \"\\help\" (for SQL commands) from within nzsql. \\t``` The -? option will list the usage and all options for the nzsql command. In this exercise, we will start nzsql without arguments. In the command prompt, issue the command: Input nzsql This will bring up the nzsql prompt below that shows a connection to the system database as user admin: Output [ nz@localhost ~ ] $ nzsql Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM.ADMIN ( ADMIN )= >","title":"Connect to the Netezza System Database Using nzsql"},{"location":"nz-01-NPS-CLI/#commonly-used-commands-and-sql-statements","text":"There are commonly used commands that start with \"\\\u201d which we will demonstrate in this section. First, we will run the 2 help commands to familiarize ourselves with these handy commands. The \\h command will list the available SQL commands, while the \\? command is used to list the internal slash commands. Examine the output for both commands: Input \\h \\? From the output of the \\? command, we found the \\l internal command we can use to find out all the databases. Let's find out all the databases by entering \\l : Input \\l Output List of databases DATABASE | OWNER ----------+---------- LABDB | LABADMIN SYSTEM | ADMIN ( 10 rows ) Secondly, we will use \\dSv to find out the system views within the system database. Note: there are system tables, however, it isn't recommended to directly access those tables as there can change from release to release and are restricted from the normal user. Input: Input \\d Sv Output List of relations Schema | Name | Type | Owner --------------------+---------------------------------------+------------- +------- DEFINITION_SCHEMA | _V_ACL_DATA | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_AGGREGATE | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_ATTRIBUTE | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_ATTRIBUTE2 | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_AUTHENTICATION | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_AUTHENTICATION_SETTINGS | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_GROUP | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_GROUP_HISTORY | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_GROUP_MEMBER | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_HISTORY | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BACKUP_TABLE_HISTORY | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BLOWER | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_BNR_CONNECTOR | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_CLASS | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_CLASS2 | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_CLIENT_COMPATIBILITY | SYSTEM VIEW | ADMIN DEFINITION_SCHEMA | _V_CONNECTION | SYSTEM VIEW | ADMIN --More\u2014 Note: press the space bar to scroll down the result set when you see --More-- on the screen. The list of the system views was truncated in the output above due to the length. Here are the primary views to investigate for DBAs/users new to Netezza: Output _V_GROUP _V_USER _V_SESSION_V_DATABASE _V_SCHEMA _V_TABLE _V_VIEW _V_ATTRIBUTE _V_SEQUENCE _V_SYNONYM _V_FUNCTION _V_AGGREGATE _V_PROCEDURE From the previous command, we can see that there is a user table called _V_USER . To find out what is stored in that table, we will use the describe command \\d: Input \\d _V_USER Output View \"_V_USER\" Attribute | Type | Modifier | Default Value ---------------------+------------------------+----------+--------------- OBJID | OID | NOT NULL | USERNAME | NAME | NOT NULL | OWNER | NAME | | VALIDUNTIL | TIMESTAMP | | CREATEDATE | ABSTIME | NOT NULL | ROWLIMIT | INTEGER | | ACCT_LOCKED | BOOLEAN | | INV_CONN_CNT | SMALLINT | | PWD_INVALID | BOOLEAN | | PWD_LAST_CHGED | DATE | | SESSIONTIMEOUT | INTEGER | | QUERYTIMEOUT | INTEGER | | DEF_PRIORITY | NAME | | MAX_PRIORITY | NAME | | USESYSID | INTEGER | | OBJDELIM | BOOLEAN | NOT NULL | USERESOURCEGRPID | OID | | USERESOURCEGRPNAME | NAME | | CROSS_JOINS_ALLOWED | CHARACTER VARYING ( 255 ) | | USEAUTH | CHARACTER VARYING ( 255 ) | | PWD_EXPIRY | INTEGER | | View definition: SELECT O.OBJID, O.OBJNAME AS USERNAME, O. \"OWNER\" , U.VALUNTIL AS VALIDUNTIL, O.CREATEDATE, U.USERESULTLIMIT AS ROWLIMIT, U.USELOCKED AS ACCT_LOCKED, U.USEINVCONNCNT AS INV_CONN_CNT, U.USEPWDINV AS PWD_INVALID, U. USEPWDCHGED AS PWD_LAST_CHGED, U.USESESSTIMELIMIT AS \"SESSIONTIMEOUT\" , U. USEQRYDURATION AS \"QUERYTIMEOUT\" , DP.PRILITERAL AS DEF_PRIORITY, MP.PRILITERAL AS MAX_PRIORITY, U.USESYSID, O.OBJDELIM, U.USERESOURCEGRP AS USERESOURCEGRPID, CASE WHEN ( O.OBJID = 4900 ) THEN \"NAME\" (( '_ADMIN_' :: \"NVARCHAR\" ) ::NVARCHAR ( 255 )) WHEN (( U.USERESOURCEGRP ISNULL ) OR ( U.USERESOURCEGRP = 4901 )) THEN \"NAME\" (( 'PUBLIC' :: \"NVARCHAR\" ) ::NVARCHAR ( 255 )) ELSE G.OBJNAME END AS USERESOURCEGRPNAME, CASE WHEN ( U.USECROSSJOIN ISNULL ) THEN ( 'NULL' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) WHEN ( U.USECROSSJOIN = 0 ) THEN ( 'NULL' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) WHEN ( U.USECROSSJOIN = 1 ) THEN ( 'FALSE' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) ELSE ( 'TRUE' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) END AS CROSS_JOINS_ALLOWED, CASE WHEN ( U. USEAUTH = 1 ) THEN ( 'LOCAL' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) ELSE ( 'DEFAULT' :: \"VARCHAR\" ) ::VARCHAR ( 255 ) END AS USEAUTH, U.USEPASSWDEXPIRY AS PWD_EXPIRY FROM (((( DEFINITION_SCHEMA. \"_V_OBJ_USER\" O JOIN DEFINITION_SCHEMA. \"_T_USER\" U ON (( O. OBJID = U.OID ))) LEFT JOIN DEFINITION_SCHEMA. \"_T_PRIORITY\" DP ON (( U. USEDEFPRIORITY = DP.PRICODE ))) LEFT JOIN DEFINITION_SCHEMA. \"_T_PRIORITY\" MP ON (( U.USEMAXPRIORITY = MP.PRICODE ))) LEFT JOIN DEFINITION_SCHEMA. \"_V_OBJ_GROUP\" G ON (( U.USERESOURCEGRP = G.OBJID ))) ; This will return all the columns of the _V_USER system table. Next, we want to know the existing users stored in the table. In case too many rows are returned at once, we will first calculate the number of rows it contains by enter the following query: Input SELECT COUNT ( * ) FROM ( SELECT * FROM _V_USER ) AS \"Wrapper\" ; Output COUNT ------- 2 ( 1 row ) The query above is essentially the same as SELECT COUNT (*) FROM _V_USER , we have demonstrated the sub-select syntax in case there is a complex query that needed to have the result set evaluated. The result should show there is currently 2 entries in the user table. We can enter the following query to list the user names: Input select objid, username, owner, createdate, useauth, pwd_expiry from _v_user ; Output OBJID | USERNAME | OWNER | CREATEDATE | USEAUTH | PWD_EXPIRY --------+----------+-------+---------------------+---------+------------ 4900 | ADMIN | ADMIN | 2020 -01-24 08 :26:22 | DEFAULT | 0 210500 | LABADMIN | ADMIN | 2020 -05-11 07 :58:31 | DEFAULT | 0","title":"Commonly Used Commands and SQL Statements"},{"location":"nz-01-NPS-CLI/#exit-nzsql","text":"To exit nzsql, use the command \\q to return to the Netezza Performance Server system. Input \\q","title":"Exit nzsql"},{"location":"nz-01-NPS-CLI/#nzsql-command","text":"The nzsql command invokes a SQL command interpreter on the IBM\u00ae Netezza\u00ae host or on an IBM Netezza client system. You can use this SQL command interpreter to create database objects, run queries, and manage the database. To run the nzsql command, enter: Input nzsql [ options ] [ security options ] [ dbname [ user ] [ password ]] The following table describes the nzsql command parameters. You will be using the nzsql command line tool throughout the labs.","title":"nzsql Command"},{"location":"nz-02-WebConsole/","text":"1 Web Console In this lab we will explore the features of the IBM Netezza Performance Server console. The console is a HTML-based application that allows users to manage the system, obtain hardware information and status, and manage various aspects of user databases, tables, and objects. The VMWare image we are using in the labs differs significantly from a normal Netezza Performance Server. Each Netezza Performance Server has the Web Console running as an OpenShift containerized application on the control plane/master nodes. In this environment, the Web Console is running natively on the NPSVB virtual machine. 2 Lab Setup This lab uses an initial setup script to make sure the correct users and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. Login to the VM directly and use the terminal application available inside the VM. Connect to your Netezza Performance Server image using a terminal command, for example PuTTY (Windows) or Terminal (Mac). See the Setup Lab for more details on accessing the Linux command line. If you are continuing from the previous lab and are already connected to nzsql quit the console with the \\q command. Prepare for this lab by running the setup script. To do this use the following commands: Input cd ~/labs/console/setupLab/ ./setupLab.sh Output DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN does not exist. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully While the setup script is running proceed to the next step. 2.1 Starting NPS Web Console After booting up the NPSVB virtual machine with Virtual Box follow these steps to start the Web Console application inside the VM: Open a terminal application from the VM Desktop or remotely with PuTTY (Windows) or Terminal (Mac). VM Desktop : open a \u201cTerminal\u201d, right click Desktop and select \u201cOpen Terminal\u201d VM Desktop : switch to the root user su \u2013 password: netezza Remotely via ssh ssh root@192.168.9.2 password: netezza Start the service pods start-cyclops.sh Check that the two pods are running docker ps To stop the console, run the following command for each container docker stop <container-id> 2.2 Launch the NPS Console If you already configured the access to the NPS in the Setup Lab, you can skip this chapter. Open a browser on your host machine or from the virtual machine, use https://192.168.9.2:8443 as URL. Add an the local NPS instance Click Add instance Enter the following information: Name: NPS VM Local Host: 192.168.9.2 Port: 5480 Admin Username: admin Admin Password: password Click Save . If you see the following error on the top right of your browser then Netezza is not active Go to the VM Linux command line and run nzstart as the nz user. Click the three vertical ellipes to right of the newly created instance and select Add credentials . Note: There are four available options: Add credentials Edit Rename Remove Enter the following values (admin is the default superuser for NPS) Username: admin Password: password Click: Save This is a database user defined inside of NPS by the create user SQL statement. admin is the default superuser for NPS. The instance should now have a hot link for the host URL. You can select the three vertical ellipses to reveal the following options: Add DB user credential Edit system credential Rename Remove Click on the Host URL to gain access to the NPS Console for Instance: NPS VM Local. Navigation menu to expose all menu options. Dashboard \u2013 the default view, shows the over system usage over different or custom time horizons (1 hour, 6 hours, 12 hours, 24 hours, 3 days, Custom). Query Editor \u2013 Run/review SQL statements of the active Netezza instance, display results and optional display plan graph. Queries \u2013 Recent queries, Encumbrance or Stored Queries Data \u2013 manage database objects Administration \u2013 User Management, Recovery events, Event rules, Schedulre rules, History Configuration Resource consumption \u2013 system resources: Memory, Compute, Storage, Fabric Query throughput \u2013 Number of queries completed over time Query performance \u2013 Average query performance over time Sessions \u2013 number of sessions connections over time Query runtime distribution \u2013 number of queries run for a given time range: 0-2 seconds, 2 second \u2013 1 minute, 1 \u2013 5 minutes, 10 \u2013 30 minutes, 30 minutes \u2013 1 hour, 1 hour \u2013 5 hours, > 5 hours Real time resource usage \u2013 Storage, Compute Memory, Memory Running Queries \u2013 Long running query, recent queries Congratulations you now have access to the NPS Console running inside the NPS VM. 3 Troubleshooting the Web Console If you receive this error when attempting to access the NPS Console: \u201cThis site can\u2019t be reached\u201d follow these steps to resolve the issue. Stop active containers: docker ps Take note of the container IDs. Stop active containers: docker stop <container-ID> Remove the inactive containers: docker ps -a Take note of the container IDs. Stop active containers: docker rm <container-ID> Reinstall NPS Console: /root/cyclops\\_dockerrun/standalone-install.sh 4 Monitor System Performance At this point you should be connected to the NPS console. The main screen after logging on displays information on the system usage and throughput. In the main screen you will be able to monitor some of the key performance metrics: System utilization Throughput Average query performance Number of active queries Scroll down to retrieve information on active sessions in the system: Use the slider below the \"Query throughput\" tile to display information for the last two hours: Notice how the displayed information changes: Click this icon to open the Query editor menu option under the Navigation menu. (Top Left) This allows you to execute queries against a database. You can store the queries as well. In addition to the result of the query you can also look at the explain plan. Let\u2019s create a query called testquery (1) against database LABDB (2) and schema ADMIN (3). Enter the following SQL statement (4) and click Run (5) to execute the query. Input: SELECT SUM(L_QUANTITY), AVG(L_TAX) FROM LINEITEM WHERE EXTRACT(MONTH FROM L_SHIPDATE) = 4 Note: Typically you terminate the SQL statement with a semicolon ( ; ). In this example omit the semicolon (reason: issue with Plan Graph when using a semicolon, next step). The result is displayed on the right side of the page: Clicking Plan graph tab to displays information on the access methods used for this statement: You can access the different parts of the console through the \"Navigation\" menu in the upper left corner. 5 Retrieve Information on NPS Configuration From the Navigation menu select \u201cDashboard\u201d. Select the Hardware tab to access the hardware monitoring portion of the NPS console. The Overview tab displays: Summary System state Authentication Version Spare disks SPUs (Snippet Processing Units) Total Online Active This is a Netezza Performance Server VM which only has 1 SPU. A typical Netezza Performance Server will have many SPUs. System disk status Total disk space Used Free Data slices over 90% disk space Storage utilization Minimum Average Maximum Click SPU units to retrieve information on the SPUs defined in the VM. The one data slices in your system can displayed as well: Click Data Slices 6 Manage Databases Click the Navigation menu and select the click Data to access the Databases screen of the NPS console. The following screen opens: You see that the two databases in the system are displayed: The SYSTEM database and the LABDB database that you created earlier. Click LABDB . An overview of the database object is displayed. In total, this database has 11 objects: three schemas and eight tables. Click to open the overview of the database tables. Use the sort option of the view to find the table with the most rows, click Record count header. As you can see, table LINEITEM has over 6 million rows. For the PART table select the Kebab (three dots) to the right. This allows you to run several operations against the table, like RENAME or GROOM. Click Groom . A new screen opens that allows you define the mode to be used by GROOM and if a reclaim backup set should be used. Do not start the GROOM, but instead go back to the main page of the database menu by clicking Databases from the breadcrumbs. Click Create database to open a new pane that allows you create a new database. Create a new database TESTDB . Enter TESTDB (1) in the name field and click Create (2) . The console opens a new pane, with information on the database objects in the newly created database TESTDB. Go to the Tables view of this screen. Click Create table to add a new table. Choose ADMIN (1) as the schema and TESTTABLE (2) as the table name. Click Create + (3) in the columns section to add a column called C1 (4) of type BIGINT. Enter a second column called D1 (5) as type DATE . Select C1 (6) as the distribution column. The console shows you the DDL statement that will be used to create the table: Click Create (7) to create the table. In the next screen, go back to the database overview: Click Databases You can easily administer databases in the console. Let\u2019s delete the new database TESTDB now. Move your mouse to the end of the line for the database TESTDB . Click the Kebab menu (1) for the TESTDB database and select Drop . (2) Confirm the action in the pop-up menu, click Drop . Back in the database screen, you see only the two original databases. 7 Queries In the Navigation menu of the console go to Queries \u2192 Recent queries . This opens a screen with information on the last queries that were executed in NPS. On the left side of the screen, you can filter the result set. Filter to all by Status (Running, Success) , Database, User. Select the following filters: Status: Success, Database: LABDB and User: ADMIN The result set is automatically filtered. Select the Metrics tab for one of the queries to receive more details about a successful query. Select a Plan ID for one the queries. You are now on the query editor for the selected plan. You can run the query and/or get a Plan graph. 8 Explore There are many other options available from the Console, please use this time to explore. You can connect to the NPS on Cloud instance previously added to the instance list to monitor a simulated workload and explore the history database. Administration User/Group Management Recovery events Event rules Scheduler rules History configuration","title":"Web Console"},{"location":"nz-02-WebConsole/#1-web-console","text":"In this lab we will explore the features of the IBM Netezza Performance Server console. The console is a HTML-based application that allows users to manage the system, obtain hardware information and status, and manage various aspects of user databases, tables, and objects. The VMWare image we are using in the labs differs significantly from a normal Netezza Performance Server. Each Netezza Performance Server has the Web Console running as an OpenShift containerized application on the control plane/master nodes. In this environment, the Web Console is running natively on the NPSVB virtual machine.","title":"1 Web Console"},{"location":"nz-02-WebConsole/#2-lab-setup","text":"This lab uses an initial setup script to make sure the correct users and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. Login to the VM directly and use the terminal application available inside the VM. Connect to your Netezza Performance Server image using a terminal command, for example PuTTY (Windows) or Terminal (Mac). See the Setup Lab for more details on accessing the Linux command line. If you are continuing from the previous lab and are already connected to nzsql quit the console with the \\q command. Prepare for this lab by running the setup script. To do this use the following commands: Input cd ~/labs/console/setupLab/ ./setupLab.sh Output DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN does not exist. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully While the setup script is running proceed to the next step.","title":"2 Lab Setup"},{"location":"nz-02-WebConsole/#21-starting-nps-web-console","text":"After booting up the NPSVB virtual machine with Virtual Box follow these steps to start the Web Console application inside the VM: Open a terminal application from the VM Desktop or remotely with PuTTY (Windows) or Terminal (Mac). VM Desktop : open a \u201cTerminal\u201d, right click Desktop and select \u201cOpen Terminal\u201d VM Desktop : switch to the root user su \u2013 password: netezza Remotely via ssh ssh root@192.168.9.2 password: netezza Start the service pods start-cyclops.sh Check that the two pods are running docker ps To stop the console, run the following command for each container docker stop <container-id>","title":"2.1 Starting NPS Web Console"},{"location":"nz-02-WebConsole/#22-launch-the-nps-console","text":"If you already configured the access to the NPS in the Setup Lab, you can skip this chapter. Open a browser on your host machine or from the virtual machine, use https://192.168.9.2:8443 as URL. Add an the local NPS instance Click Add instance Enter the following information: Name: NPS VM Local Host: 192.168.9.2 Port: 5480 Admin Username: admin Admin Password: password Click Save . If you see the following error on the top right of your browser then Netezza is not active Go to the VM Linux command line and run nzstart as the nz user. Click the three vertical ellipes to right of the newly created instance and select Add credentials . Note: There are four available options: Add credentials Edit Rename Remove Enter the following values (admin is the default superuser for NPS) Username: admin Password: password Click: Save This is a database user defined inside of NPS by the create user SQL statement. admin is the default superuser for NPS. The instance should now have a hot link for the host URL. You can select the three vertical ellipses to reveal the following options: Add DB user credential Edit system credential Rename Remove Click on the Host URL to gain access to the NPS Console for Instance: NPS VM Local. Navigation menu to expose all menu options. Dashboard \u2013 the default view, shows the over system usage over different or custom time horizons (1 hour, 6 hours, 12 hours, 24 hours, 3 days, Custom). Query Editor \u2013 Run/review SQL statements of the active Netezza instance, display results and optional display plan graph. Queries \u2013 Recent queries, Encumbrance or Stored Queries Data \u2013 manage database objects Administration \u2013 User Management, Recovery events, Event rules, Schedulre rules, History Configuration Resource consumption \u2013 system resources: Memory, Compute, Storage, Fabric Query throughput \u2013 Number of queries completed over time Query performance \u2013 Average query performance over time Sessions \u2013 number of sessions connections over time Query runtime distribution \u2013 number of queries run for a given time range: 0-2 seconds, 2 second \u2013 1 minute, 1 \u2013 5 minutes, 10 \u2013 30 minutes, 30 minutes \u2013 1 hour, 1 hour \u2013 5 hours, > 5 hours Real time resource usage \u2013 Storage, Compute Memory, Memory Running Queries \u2013 Long running query, recent queries Congratulations you now have access to the NPS Console running inside the NPS VM.","title":"2.2 Launch the NPS Console"},{"location":"nz-02-WebConsole/#3-troubleshooting-the-web-console","text":"If you receive this error when attempting to access the NPS Console: \u201cThis site can\u2019t be reached\u201d follow these steps to resolve the issue. Stop active containers: docker ps Take note of the container IDs. Stop active containers: docker stop <container-ID> Remove the inactive containers: docker ps -a Take note of the container IDs. Stop active containers: docker rm <container-ID> Reinstall NPS Console: /root/cyclops\\_dockerrun/standalone-install.sh","title":"3 Troubleshooting the Web Console"},{"location":"nz-02-WebConsole/#4-monitor-system-performance","text":"At this point you should be connected to the NPS console. The main screen after logging on displays information on the system usage and throughput. In the main screen you will be able to monitor some of the key performance metrics: System utilization Throughput Average query performance Number of active queries Scroll down to retrieve information on active sessions in the system: Use the slider below the \"Query throughput\" tile to display information for the last two hours: Notice how the displayed information changes: Click this icon to open the Query editor menu option under the Navigation menu. (Top Left) This allows you to execute queries against a database. You can store the queries as well. In addition to the result of the query you can also look at the explain plan. Let\u2019s create a query called testquery (1) against database LABDB (2) and schema ADMIN (3). Enter the following SQL statement (4) and click Run (5) to execute the query. Input: SELECT SUM(L_QUANTITY), AVG(L_TAX) FROM LINEITEM WHERE EXTRACT(MONTH FROM L_SHIPDATE) = 4 Note: Typically you terminate the SQL statement with a semicolon ( ; ). In this example omit the semicolon (reason: issue with Plan Graph when using a semicolon, next step). The result is displayed on the right side of the page: Clicking Plan graph tab to displays information on the access methods used for this statement: You can access the different parts of the console through the \"Navigation\" menu in the upper left corner.","title":"4 Monitor System Performance"},{"location":"nz-02-WebConsole/#5-retrieve-information-on-nps-configuration","text":"From the Navigation menu select \u201cDashboard\u201d. Select the Hardware tab to access the hardware monitoring portion of the NPS console. The Overview tab displays: Summary System state Authentication Version Spare disks SPUs (Snippet Processing Units) Total Online Active This is a Netezza Performance Server VM which only has 1 SPU. A typical Netezza Performance Server will have many SPUs. System disk status Total disk space Used Free Data slices over 90% disk space Storage utilization Minimum Average Maximum Click SPU units to retrieve information on the SPUs defined in the VM. The one data slices in your system can displayed as well: Click Data Slices","title":"5 Retrieve Information on NPS Configuration"},{"location":"nz-02-WebConsole/#6-manage-databases","text":"Click the Navigation menu and select the click Data to access the Databases screen of the NPS console. The following screen opens: You see that the two databases in the system are displayed: The SYSTEM database and the LABDB database that you created earlier. Click LABDB . An overview of the database object is displayed. In total, this database has 11 objects: three schemas and eight tables. Click to open the overview of the database tables. Use the sort option of the view to find the table with the most rows, click Record count header. As you can see, table LINEITEM has over 6 million rows. For the PART table select the Kebab (three dots) to the right. This allows you to run several operations against the table, like RENAME or GROOM. Click Groom . A new screen opens that allows you define the mode to be used by GROOM and if a reclaim backup set should be used. Do not start the GROOM, but instead go back to the main page of the database menu by clicking Databases from the breadcrumbs. Click Create database to open a new pane that allows you create a new database. Create a new database TESTDB . Enter TESTDB (1) in the name field and click Create (2) . The console opens a new pane, with information on the database objects in the newly created database TESTDB. Go to the Tables view of this screen. Click Create table to add a new table. Choose ADMIN (1) as the schema and TESTTABLE (2) as the table name. Click Create + (3) in the columns section to add a column called C1 (4) of type BIGINT. Enter a second column called D1 (5) as type DATE . Select C1 (6) as the distribution column. The console shows you the DDL statement that will be used to create the table: Click Create (7) to create the table. In the next screen, go back to the database overview: Click Databases You can easily administer databases in the console. Let\u2019s delete the new database TESTDB now. Move your mouse to the end of the line for the database TESTDB . Click the Kebab menu (1) for the TESTDB database and select Drop . (2) Confirm the action in the pop-up menu, click Drop . Back in the database screen, you see only the two original databases.","title":"6 Manage Databases"},{"location":"nz-02-WebConsole/#7-queries","text":"In the Navigation menu of the console go to Queries \u2192 Recent queries . This opens a screen with information on the last queries that were executed in NPS. On the left side of the screen, you can filter the result set. Filter to all by Status (Running, Success) , Database, User. Select the following filters: Status: Success, Database: LABDB and User: ADMIN The result set is automatically filtered. Select the Metrics tab for one of the queries to receive more details about a successful query. Select a Plan ID for one the queries. You are now on the query editor for the selected plan. You can run the query and/or get a Plan graph.","title":"7 Queries"},{"location":"nz-02-WebConsole/#8-explore","text":"There are many other options available from the Console, please use this time to explore. You can connect to the NPS on Cloud instance previously added to the instance list to monitor a simulated workload and explore the history database. Administration User/Group Management Recovery events Event rules Scheduler rules History configuration","title":"8 Explore"},{"location":"nz-03-Data-Distribution/","text":"1 Data Distribution Netezza Performance Server is a family of data-warehousing appliances that combine high performance with low administrative effort. Due to the unique data warehousing centric architecture of Netezza Performance Server, most performance tuning tasks are either not necessary or automated. Unlike normal data warehousing solutions, no tablespaces need to be created or tuned, there are also no indexes, buffer pools or partitions. Since Netezza Performance Server is built on a massively parallel architecture that distributes data and workloads over many processing and data nodes, the single most important tuning factor is picking the right distribution key. The distribution key governs which data rows of a table are distributed to which data slice and it is very important to pick an optimal distribution key to avoid data skew, processing skew and to make joins co-located whenever possible. 1.1 Objectives In this lab we will cover a typical scenario in a POC or customer engagement which involves an existing data warehouse for customer transactions. Figure 1. LABDB database Figure 1 shows a visualization of the tables in the data warehouse and the relationships between the tables. The warehouse contains the customers of the company, their orders, and the line items that are part of the order. The warehouse also has a list of suppliers, providing the parts that are part of the shipped line items. For this lab we already have the DDLs for creation of the tables and load files containing the warehouse data. Both are already in a format usable by the Netezza Performance Server System. In this lab we will define the distribution keys for these tables. In addition to the data and the DDLs we also have received a couple of queries from the customer that are usually run against the warehouse. Those are important input as well for picking optimal distribution keys. 2 Check the Number of Data Slices By default, the NPS VirtualBox virtual machine only comes with 1 data slice. To perform the steps in this lab and the other performance related labs you will need 4 data slices. It is not recommended to run more than 4 data slices. If you did not configure your virtual machine for 4 data slices, go back to the lab called: 01-Setup-NPS-Virtual-Machine-Setup-Lab-Guide-Win10-OSX-Final and follow the section called: \" Increase the Number of Data Slices \". To check your data slices, login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using a terminal application (Windows PowerShell, PuTTY, Mac OSX Terminal) If you are continuing from the previous lab and are already connected to nzsql , quit the nzsql console with the \\q command. Input Output nzstate # if NPS is offline, run nzstart nzhw nzds [nz@localhost labs]$ nzstate System state is 'Online'. [nz@localhost labs]$ nzhw Description HW ID Location Role State Security ----------- ----- ---------- ------ ------ -------- Rack 1001 rack1 Active Ok N/A SPA 1002 spa1 Active Ok N/A SPU 1003 spa1.spu1 Active Online N/A Disk 1004 spa1.disk1 Active Ok N/A Disk 1005 spa1.disk2 Active Ok N/A Disk 1006 spa1.disk3 Active Ok N/A Disk 1007 spa1.disk4 Active Ok N/A [nz@localhost labs]$ nzds Data Slice Status SPU Partition Size (GiB) % Used Supporting Disks ---------- ------- ---- --------- ---------- ------ ---------------- 1 Unknown 1003 0 16 0.84 1004 2 Unknown 1003 1 16 0.84 1005 3 Unknown 1003 2 16 0.84 1006 4 Unknown 1003 3 16 0.84 1007 With the proper data slices continue to the next section. 3 Lab Setup This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using a terminal application (Windows PowerShell, PuTTY, Mac OSX Terminal) If you are continuing from the previous lab and are already connected to nzsql quit the nzsql console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input Output cd ~/labs/dataDistribution/setupLab ./setupLab.sh ERROR: DROP DATABASE: object LABDB does not exist. CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE The error message at the beginning is expected since the script tries to clean up existing LINEITEM tables. This lab is now setup to use for the remainder of the sections. 4 Skew Tables in Netezza Performance Server are distributed across data slices based on the distribution method and key. If a bad data distribution method has been picked, it will result in skewed tables or processing skew. Data skew occurs when the distribution method puts significantly more records of a table on one data slice than on other data slices. Apart from bad performance this also results in a situation where the Netezza Performance Server can hold significantly less data than expected. Processing skew occurs if processing of queries is mainly taking place on some data slices for example because queries only apply to data on those data slices. Both types of skew result in suboptimal performance since in a parallel system the slowest node defines the total execution time. 4.1 Data Skew The first table we will create is LINEITEM , the main fact table of the schema. It contains roughly 6 million rows. To create the LINEITEM table, switch to the lab directory ~/labs/dataDistribution . To do this use the following command: (Notice that you can use bash auto complete by using the Tab key to complete folder and files names) Input Output cd ~/labs/dataDistribution ls -l [nz@localhost dataDistribution]$ ls -l total 24 -rwxr-xr-x. 1 nz nz 177 Apr 20 2020 create_lineitem_1.sh -rwxr-xr-x. 1 nz nz 170 Dec 9 2015 create_orders_1.sh -rwxr-xr-x. 1 nz nz 612 Dec 9 2015 create_remaining.sh -rwxr-xr-x. 1 nz nz 656 Mar 29 13:44 lineitem.sql -rwxr-xr-x. 1 nz nz 374 Mar 29 13:44 orders.sql -rwxr-xr-x. 1 nz nz 1660 Mar 29 13:44 remaining_tables.sql drwxr-xr-x. 2 nz nz 91 Mar 29 2020 setupLab The command line prompt changes to reflect the directory you are in ( dataDistribution ). Create the LINEITEM table by using the following script. Since the fact table is quite large this can take several minutes. Input Output ./create_lineitem_1.sh ERROR: relation does not exist LABDB.ADMIN.LINEITEM CREATE TABLE Load session of table 'LINEITEM' completed successfully The error message at the beginning is expected since the script tries to clean up existing LINEITEM tables. Also note that a load log file has been created in the ~/labs/dataDistribution directory named LINEITEM.ADMIN.LABDB.nzlog . ( ls \\*.nzlog ) Review the load logfile as time and interest permits. This load will take some time, if you want to see the status of the load run the following SQL from another terminal session. Input Output nzsql -c \"select * from _v_load_status;\" PLANID | DATABASENAME | TABLENAME | SCHEMANAME | USERNAME | BYTESPROCESSED | ROWSINSERTED | ROWSREJECTED | BYTESDOWNLOADED -------+--------------+-----------+------------+----------+----------------+--------------+--------------+----------------- 592 | LABDB | LINEITEM | ADMIN | LABADMIN | 704643063 | 5522182 | 0 | 718896512 (1 row) Now let's have a look at the created table, open the nzsql console by entering the command: Input Output nzsql Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM.ADMIN(ADMIN)=> Connect to the database LABDB as user LABADMIN by typing the following command: Input Output \\c LABDB LABADMIN You are now connected to database LABDB as user LABADMIM LABDB.ADMIN(LABADMIN)=> You should now be connected to the LABDB database as the LABADMIN user. Let's have a look at the table we just created. First, we want to see a description of its columns and distribution key. Use the NZSQL describe command \\d LINEITEM to get a description of the table. Input Output \\d LINEITEM Table \"LINEITEM\" Attribute | Type | Modifier | Default Value -----------------+-----------------------+----------+--------------- L_ORDERKEY | INTEGER | NOT NULL | L_PARTKEY | INTEGER | NOT NULL | L_SUPPKEY | INTEGER | NOT NULL | L_LINENUMBER | INTEGER | NOT NULL | L_QUANTITY | NUMERIC(15,2) | NOT NULL | L_EXTENDEDPRICE | NUMERIC(15,2) | NOT NULL | L_DISCOUNT | NUMERIC(15,2) | NOT NULL | L_TAX | NUMERIC(15,2) | NOT NULL | L_RETURNFLAG | CHARACTER(1) | NOT NULL | L_LINESTATUS | CHARACTER(1) | NOT NULL | L_SHIPDATE | DATE | NOT NULL | L_COMMITDATE | DATE | NOT NULL | L_RECEIPTDATE | DATE | NOT NULL | L_SHIPINSTRUCT | CHARACTER(25) | NOT NULL | L_SHIPMODE | CHARACTER(10) | NOT NULL | L_COMMENT | CHARACTER VARYING(44) | NOT NULL | Distributed on hash: \"L_LINESTATUS\" We can see that the LINEITEM table has 16 columns with different data types. Some of the columns have a \"key\" suffix and substrings containing the names of other tables and are most likely foreign keys of dimension tables. The distribution key is L_LINESTATUS , which is of a CHAR(1) data type. Now let's have a look at the data in the table. To return a limited number of rows you can use the limit keyword in your select queries. Execute the following select command to return 10 rows of the LINEITEM table. For readability we only select a couple of columns including the order key ( L_ORDERKEY ), the ship date ( L_SHIPDATE ) and the line status ( L_LINESTATUS ) distribution key: Input Output SELECT L_ORDERKEY, L_QUANTITY, L_SHIPDATE, L_LINESTATUS FROM LINEITEM LIMIT 10; L_ORDERKEY | L_QUANTITY | L_SHIPDATE | L_LINESTATUS ------------+------------+------------+-------------- 3 | 45.00 | 1994-02-02 | F 3 | 49.00 | 1993-11-09 | F 3 | 27.00 | 1994-01-16 | F 3 | 2.00 | 1993-12-04 | F 3 | 28.00 | 1993-12-14 | F 3 | 26.00 | 1993-10-29 | F 5 | 15.00 | 1994-10-31 | F 5 | 26.00 | 1994-10-16 | F 5 | 50.00 | 1994-08-08 | F 6 | 37.00 | 1992-04-27 | F (10 rows) From this limited sample we cannot make any definite judgement, but we can make a couple of assumptions. While the L_ORDERKEY column is not unique, it seems to have a lot of distinct values. The L_SHIPDATE column also appears to have a lot of distinct shipping date values. On the other hand, our current distribution key L_LINESTATUS has only two values, which may make it a bad distribution key. It is possible that you get different results. Since a database table is an unordered set, it is probable that you get different results. For example, only O or F values may be in the L_LINESTATUS column. We will now verify the number of distinct values in the L_LINESTATUS column with a SELECT DISTINCT statement. To return a list of all distinct values that are in the L_LINESTATUS column execute the following SQL command: Input Output SELECT DISTINCT L_LINESTATUS FROM LINEITEM; L_LINESTATUS -------------- O F (2 rows) We can see that the L_LINESTATUS column only contains two distinct values. As a distribution key, this will result in a table that is only distributed to two of the available data slices. We verify this by executing the following SQL call, which will return a list of all data slices which contain rows of the LINEITEM table, and the corresponding number of rows stored in them: Input Output SELECT DATASLICEID, COUNT(*) FROM LINEITEM GROUP BY DATASLICEID; DATASLICEID | COUNT -------------+--------- 4 | 2996217 1 | 3004998 (2 rows) Every Netezza Performance Server table has a hidden column DATASLICEID , which contains the id of the data slice the selected row is being stored in. By executing a SQL query that does a GROUP BY on this column and counts the number of rows for each DATASLICEID , data skew can be detected. In this case the table has been distributed to only two of the available four data slices. This means that only half of the available space and compute is used, which will result in low performance during most query executions. In general, a good distribution key should have many unique values, with an even distribution of rows in the data slices. Columns with few unique values, especially boolean columns or date columns, should not be considered as distribution keys. 4.2 Processing Skew Even in tables that are distributed evenly across data slices, data processing for queries can be concentrated or skewed to a limited number of data slices. One way for processing skew to occur for a query is if there is a predicate ( WHERE condition) on the distribution key, and the distribution key has few unique values. Using a DATE columns for a distribution key is notorious for causing processing skew, and usually should be avoided. The next section of the lab will show this effect. First, we will pick a new distribution key. As we have seen it should have many unique (distinct) values. One of the columns that did fit this description was the L_SHIPDATE column. Check the number of distinct values in the L_SHIPDATE column with the COUNT(DISTINCT ) statement: Input Output SELECT COUNT(DISTINCT L_SHIPDATE) FROM LINEITEM; COUNT ------- 2526 (1 row) The column has over 2500 distinct values and therefore has more than enough unique values to guarantee a good data distribution on 4 data slices. Now let's reload the LINEITEM table with the new distribution key. For this we need to change the SQL of the load script we executed at the beginning of the lab. Exit the nzsql console by entering: \\q . You should now be in the lab directory ~/labs/dataDistribution . The table creation statement is in the lineitem.sql file. We will need to make changes to the file with a text editor. Open the file with the default Linux text editor vi. To do this enter the following command: Input vi lineitem.sql The vi editor has two modes, a command mode used to save files, quit the editor etc. and an insert mode. Initially you will be in the command mode. To change the lineitem.sql file you need to switch to the insert mode by pressing \"i\". The editor will show an -- INSERT -- at the bottom of the screen. Use the delete key to remove l_linestatus and type l_shipdate . Note If you are not comfortable with vi you can used gedit from the virtual machine desktop. Input gedit lineitem.sql You can now use the cursor keys to navigate to the DISTRIBUTE ON clause at the bottom of the create command. Change the distribution key to L_SHIPDATE . The editor should now look like the following: create table lineitem ( l_orderkey integer not null , l_partkey integer not null , l_suppkey integer not null , l_linenumber integer not null , l_quantity decimal(15,2) not null , l_extendedprice decimal(15,2) not null , l_discount decimal(15,2) not null , l_tax decimal(15,2) not null , l_returnflag char(1) not null , l_linestatus char(1) not null , l_shipdate date not null , l_commitdate date not null , l_receiptdate date not null , l_shipinstruct char(25) not null , l_shipmode char(10) not null , l_comment varchar(44) not null ) DISTRIBUTE ON (l_shipdate); ~ ~ -- INSERT -- We will now save our changes. Press Esc to switch back into command mode. You should see that the ---INSERT--- string at the bottom of the screen vanishes. Enter :wq! and press enter to write the file and quit the editor without any questions. If you made a mistake editing and would like to undo it press Esc then enter :q! and go back to step 3. Now repeat steps 3-5 of section 2.1 Data Skew: a. Recreate and load the LINEITEM table with your new distribution key by executing the ./create_lineitem_1.sh command b. Use the nzsql command to enter the command console c. Switch to the LABDB database by using the \\c LABDB LABADMIN command. d. Validate the distribution key is L_SHIPDATE using the \\d LINEITEM command. Now we verify that the new distribution key results in a good data distribution. For this we will repeat the query, which returns the number of rows for each datasliceid of the LINEITEM table. Execute the following command: Input Output SELECT DATASLICEID, COUNT(*) FROM LINEITEM GROUP BY DATASLICEID ORDER BY 1; DATASLICEID | COUNT -------------+--------- 1 | 1499990 2 | 1497649 3 | 1501760 4 | 1501816 (4 rows) We can see that the data distribution is much better now. All four data slices have a roughly equal number of rows. Now that we have a database table with a good data distribution let's look at a couple of queries. The following query is executed regularly by the end user. It returns the average quantity shipped on a given day grouped by the shipping mode. Execute the following query: Input Output SELECT AVG(L_QUANTITY) AS AVG_Q, L_SHIPMODE FROM LINEITEM WHERE L_SHIPDATE = '1996-03-29' GROUP BY L_SHIPMODE; AVG_Q | L_SHIPMODE -----------+------------ 26.045455 | MAIL 24.494186 | REG AIR 25.562500 | SHIP 24.780282 | RAIL 27.147826 | TRUCK 25.708556 | AIR 26.038567 | FOB (7 rows) This query will take all rows from the 29th March of 1996 and compute the average value of the L_QUANTITY column for each L_SHIPMODE value. It is a typical warehousing query insofar as a date column is used to restrict the row set that is taken as input for computation. In this example most rows of the LINEITEM table will be filtered away, only rows that have the specified date will be used as input for computation of the AVG aggregation. Execute the following SQL statement to see on which data slice we can find the rows from the 29th March of 1996: Input Output SELECT COUNT(\\*), DATASLICEID FROM LINEITEM WHERE L_SHIPDATE = '1996-03-29' GROUP BY DATASLICEID; COUNT | DATASLICEID -------+------------- 2501 | 2 (1 row) Since we used the shipping date column as a distribution key, all rows from a specific date can be found on one data slice and therefore also one SPU. This means that for our previous query all rows on other data slices are unused and the computation takes place only on one data slice and SPU. This is known as processing skew. While this one SPU is working the other SPUs will be idle. Columns that are often used in WHERE conditions should not be used as distribution keys, since this can easily result in processing skew. In warehousing environments this is especially true for DATE columns. Good distribution keys are key columns (join columns); they have lots of unique values and rarely result in processing skew. In our example we have a couple of distribution keys to choose from: L_SUPPKEY , L_ORDERKEY , L_PARTKEY . All columns have many distinct values. 5 Co-Location The most basic warehouse schema consists of a fact table containing a list of all business transactions and a set of dimension tables that contain the different actors, objects, locations and time points that have taken part in these transactions. This means that most queries will not only access one database table but will require joins between many tables. In Netezza Performance Server tables are distributed over a large numbers of data slices on different SPUs. This means that during a join of two tables there are two possibilities. Rows of the two tables that are joined together are on the same data slice, which means the rows are co-located and can be joined locally. Rows of the two tables that are joined together are on different data slices, which means that rows need to be redistributed (moved) so that rows being joined are on the same data slice. 5.1 Investigation Co-location has big performance advantages. In the following section we will demonstrate this by introducing a second table called ORDERS. Switch to the Linux command line, if you are in the NZSQL console. Do this with the \\q command. Switch to the data distribution lab directory with the cd ~/labs/dataDistribution . command, create and load the ORDERS table by executing the ./create_orders_1.sh script Input Output cd ~/labs/dataDistribution/ ./create_orders_1.sh ERROR: relation does not exist LABDB.ADMIN.ORDERS CREATE TABLE Load session of table 'ORDERS' completed successfully Enter the NZSQL console with the nzsql labdb labadmin command and take a look at the ORDERS table with the \\d orders command. Input Output nzsql labdb LABDB.ADMIN(LABADMIN)=> \\d orders Table \"ORDERS\" Attribute | Type | Modifier | Default Value -----------------+-----------------------+----------+--------------- O_ORDERKEY | INTEGER | NOT NULL | O_CUSTKEY | INTEGER | NOT NULL | O_ORDERSTATUS | CHARACTER(1) | NOT NULL | O_TOTALPRICE | NUMERIC(15,2) | NOT NULL | O_ORDERDATE | DATE | NOT NULL | O_ORDERPRIORITY | CHARACTER(15) | NOT NULL | O_CLERK | CHARACTER(15) | NOT NULL | O_SHIPPRIORITY | INTEGER | NOT NULL | O_COMMENT | CHARACTER VARYING(79) | NOT NULL | Distributed on random: (round-robin) The ORDERS table has a key column O_ORDERKEY that is the primary key of the table. The ORDERS table contains information on the order value, priority and date, and has been distributed on random. This means that Netezza Performance Server doesn't use a hash-based algorithm to distribute the data. Instead, rows are distributed randomly on the available data slices in a round-robin fashion. You can check the data distribution of the table, using the methods we have used before for the LINEITEM table. The data distribution will be perfect. For example, try the following: Input Output SELECT COUNT(*), DATASLICEID FROM ORDERS GROUP BY DATASLICEID ORDER BY 1; COUNT | DATASLICEID --------+------------- 374393 | 3 374518 | 1 375517 | 2 375572 | 4 (4 rows) There will also not be any processing skew for queries on the single table, since in a random distribution there can be no correlation between any WHERE condition and the distribution key. We have received another typical query from our customer. It returns the average total price and item quantity of all orders grouped by the shipping priority. This query has to join together the LINEITEM and ORDERS tables to get the total order cost from the orders table and the quantity for each shipped item from the LINEITEM table. The tables are joined with an inner join on the L_ORDERKEY column. Execute the following query and note the approximate execution time: Input Output \\time SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY, O.O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; PRICE | QUANTITY | O_ORDERPRIORITY ---------------+-----------+----------------- 189219.594349 | 25.532474 | 5-LOW 189285.029553 | 25.526186 | 2-HIGH 188546.457203 | 25.472923 | 4-NOT SPECIFIED 189026.093657 | 25.494518 | 3-MEDIUM 189093.608965 | 25.513563 | 1-URGENT (5 rows) Elapsed time: 0m4.293s Notice that the query takes about 4 seconds to complete on our machine. The actual execution times on your machine will be different. Remember that the ORDERS table was distributed randomly and the LINEITEM table is still distributed by the L_SHIPDATE column. The join on the other hand is taking place on the L_ORDERKEY and O_ORDERKEY columns. We will now have a quick look at what is happening inside the Netezza Performance Server system in this scenario. To do this we use the Netezza Performance Server EXPLAIN function. This will be more thoroughly covered in the Optimization lab. Execute the following command: Input EXPLAIN VERBOSE SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY, O.O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; You will get a long output. Scroll up until you see your command in the text window. The start of the EXPLAIN output should look like the following: Output EXPLAIN VERBOSE SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY, O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; QUERY VERBOSE PLAN: Node 1. [SPU Sequential Scan table \"ORDERS\" as \"O\" {}] -- Estimated Rows = 1500000, Width = 27, Cost = 0.0 .. 1653.2, Conf = 100.0 Projections: 1:O.O_TOTALPRICE 2:O.O_ORDERPRIORITY 3:O.O_ORDERKEY [SPU Distribute on {(O.O_ORDERKEY)}] [HashIt for Join] Node 2. [SPU Sequential Scan table \"LINEITEM\" as \"L\" {(L.L_SHIPDATE)}] -- Estimated Rows = 6001215, Width = 12, Cost = 0.0 .. 6907.1, Conf = 100.0 Projections: 1:L.L_QUANTITY 2:L.L_ORDERKEY [SPU Distribute on {(L.L_ORDERKEY)}] Node 3. [SPU Hash Join Stream \"Node 2\" with Temp \"Node 1\" {(O.O_ORDERKEY,L.L_ORDERKEY)}] -- Estimated Rows = 90018225000, Width = 31, Cost = 1653.2 .. 928769.9, Conf = 80.0 Restrictions: (L.L_ORDERKEY = O.O_ORDERKEY) Projections: 1:O.O_TOTALPRICE 2:L.L_QUANTITY 3:O.O_ORDERPRIORITY [SPU Fabric Join] ..<Rest of EXPLAIN Plan>.. The EXPLAIN functionality will be covered in detail in a following chapter, but it is easy to see what is happening here. What's happening is the system is redistributing both the ORDERS and LINEITEM tables so that the rows can be joined together. This is very bad because both tables are of significant size so there is a considerable overhead. This inefficient double redistribution occurs because neither of the tables are distributed on the join key. In the next section we will fix this. 5.2 Co-Located Joins In the last section we have seen that a query using joins can result in costly data redistribution during join execution when the joined tables are not distributed on the join key. In this section we will reload the tables based on the mutual join key to enhance performance during joins. Exit the NZSQL console with the [\\q command. If not already in the dataDistribution directory run the command cd ~/labs/dataDistribution to enter the directory. Change the distribution key in the lineitem.sql file to L_ORDERKEY : a. Open the file with the vi editor (or gedit from the VM desktop) by executing the command: vi lineitem.sql ( gedit lineitem.sql ). b. In vi switch to INSERT mode by pressing \"i\" c. Navigate with the cursor keys to the DISTRIBUTE ON clause and change it to DISTRIBUTE ON (L_ORDERKEY) d. Exit the INSERT mode by pressing ESC e. Enter :wq! In the command line of the vi editor and press enter. Before pressing enter your screen should look like the following: create table lineitem ( l_orderkey integer not null , l_partkey integer not null , l_suppkey integer not null , l_linenumber integer not null , l_quantity decimal(15,2) not null , l_extendedprice decimal(15,2) not null , l_discount decimal(15,2) not null , l_tax decimal(15,2) not null , l_returnflag char(1) not null , l_linestatus char(1) not null , l_shipdate date not null , l_commitdate date not null , l_receiptdate date not null , l_shipinstruct char(25) not null , l_shipmode char(10) not null , l_comment varchar(44) not null ) DISTRIBUTE ON (l_orderkey); Change the Distribution key in the orders.sql file to O_ORDERKEY . a. Open the file with the vi editor by executing the command: vi orders.sql b. Switch to INSERT mode by pressing \"i\" c. Navigate with the cursor keys to the DISTRIBUTE ON clause and change it to DISTRIBUTE ON (O_ORDERKEY) d. Exit the INSERT mode by pressing ESC e. Enter :wq! In the command line of the VI editor and Press Enter. Before pressing Enter your screen should look like the following: create table orders ( o_orderkey integer not null , o_custkey integer not null , o_orderstatus char(1) not null , o_totalprice decimal(15,2) not null , o_orderdate date not null , o_orderpriority char(15) not null , o_clerk char(15) not null , o_shippriority integer not null , o_comment varchar(79) not null ) DISTRIBUTE ON (O_ORDERKEY); Recreate and load the LINEITEM table with the distribution key L_ORDERKEY by executing the ./create_lineitem_1.sh script. Input Output cd ~/labs/dataDistribution/ ./create_lineitem_1.sh DROP TABLE CREATE TABLE Load session of table 'LINEITEM' completed successfully Recreate and load the ORDERS table with the distribution key O_ORDERKEY by executing the ./create_orders_1.sh script. Input Output ./create_orders_1.sh DROP TABLE CREATE TABLE Load session of table 'ORDERS' completed successfully Enter the NZSQL console by executing the nzsql labdb labadmin command and check the distribution key using the \\d lineitem and \\d orders commands. Input Output Input Output nzsql labdb labadmin \\d lineitem Table \"LINEITEM\" Attribute | Type | Modifier | Default Value -----------------+-----------------------+----------+--------------- L_ORDERKEY | INTEGER | NOT NULL | L_PARTKEY | INTEGER | NOT NULL | L_SUPPKEY | INTEGER | NOT NULL | L_LINENUMBER | INTEGER | NOT NULL | L_QUANTITY | NUMERIC(15,2) | NOT NULL | L_EXTENDEDPRICE | NUMERIC(15,2) | NOT NULL | L_DISCOUNT | NUMERIC(15,2) | NOT NULL | L_TAX | NUMERIC(15,2) | NOT NULL | L_RETURNFLAG | CHARACTER(1) | NOT NULL | L_LINESTATUS | CHARACTER(1) | NOT NULL | L_SHIPDATE | DATE | NOT NULL | L_COMMITDATE | DATE | NOT NULL | L_RECEIPTDATE | DATE | NOT NULL | L_SHIPINSTRUCT | CHARACTER(25) | NOT NULL | L_SHIPMODE | CHARACTER(10) | NOT NULL | L_COMMENT | CHARACTER VARYING(44) | NOT NULL | Distributed on hash: \"L_ORDERKEY\" nzsql labdb labadmin \\d orders Table \"ORDERS\" Attribute | Type | Modifier | Default Value -----------------+-----------------------+----------+--------------- O_ORDERKEY | INTEGER | NOT NULL | O_CUSTKEY | INTEGER | NOT NULL | O_ORDERSTATUS | CHARACTER(1) | NOT NULL | O_TOTALPRICE | NUMERIC(15,2) | NOT NULL | O_ORDERDATE | DATE | NOT NULL | O_ORDERPRIORITY | CHARACTER(15) | NOT NULL | O_CLERK | CHARACTER(15) | NOT NULL | O_SHIPPRIORITY | INTEGER | NOT NULL | O_COMMENT | CHARACTER VARYING(79) | NOT NULL | Distributed on hash: \u201cO_ORDERKEY\u201d Repeat executing the explain of our join query from the previous section by executing the following command: Input Output EXPLAIN VERBOSE SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY,O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; EXPLAIN VERBOSE SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY, O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; QUERY VERBOSE PLAN: Node 1. [SPU Sequential Scan table \"ORDERS\" as \"O\" {(O.O_ORDERKEY)}] -- Estimated Rows = 1500000, Width = 27, Cost = 0.0 .. 1653.2, Conf = 100.0 Projections: 1:O.O_TOTALPRICE 2:O.O_ORDERPRIORITY 3:O.O_ORDERKEY [HashIt for Join] Node 2. [SPU Sequential Scan table \"LINEITEM\" as \"L\" {(L.L_ORDERKEY)}] -- Estimated Rows = 6001215, Width = 12, Cost = 0.0 .. 6907.1, Conf = 100.0 Projections: 1:L.L_QUANTITY 2:L.L_ORDERKEY Node 3. [SPU Hash Join Stream \"Node 2\" with Temp \"Node 1\" {(O.O_ORDERKEY,L.L_ORDERKEY)}] -- Estimated Rows = 90018225000, Width = 31, Cost = 1653.2 .. 892653.2, Conf = 80.0 Restrictions: (L.L_ORDERKEY = O.O_ORDERKEY) Projections: 1:O.O_TOTALPRICE 2:L.L_QUANTITY 3:O.O_ORDERPRIORITY ..<Rest of EXPLAIN Plan>.. The query itself has not been changed. The only changes are in the distribution keys of the involved tables. You will again see a long output. Scroll up to the start of the output, directly after your query. Again, we do not want to make a complete analysis of the explain output. We will cover that in more detail in later chapters. But if you compare the output with the output of the last section you will see that the [SPU Distribute on O.O_ORDERKEY)}] nodes have vanished. The reason is that the join is now co-located because both tables are distributed on the join key. All rows to be joined from both tables are on the same data slices. You may see a distribution node further below during the execution of the group by clause, but this is estimated to distribute only one hundred rows which has no negative performance influence. Finally execute the joined query again: Input Output \\time SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY,O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; PRICE | QUANTITY | O_ORDERPRIORITY ---------------+-----------+----------------- 189219.594349 | 25.532474 | 5-LOW 189285.029553 | 25.526186 | 2-HIGH 189093.608965 | 25.513563 | 1-URGENT 189026.093657 | 25.494518 | 3-MEDIUM 188546.457203 | 25.472923 | 4-NOT SPECIFIED (5 rows) Elapsed time: 0m1.051s The query should return the same results as in the previous section but run faster even in the VM environment. In a real Netezza Performance Server with 8, 16 or more SPUs the difference would be much more significant. You now have loaded the LINEITEM and ORDERS table into your Performance Server appliance using the optimal distribution key for these tables for most situations. a. Both tables are distributed evenly across data slices, so there is no data skew. b. The distribution key is highly unlikely to result in processing skew, since most WHERE conditions will restrict a key column evenly c. Since ORDERS is a parent table of LINEITEM , with a foreign key relationship between them, most queries joining them together will utilize the join key. These queries will be co-located. Now we will pick the distribution keys of the full schema. 6 Schema Creation Now that we have created the ORDERS and LINEITEM tables we need to pick the distribution keys for the remaining tables as well. 6.1 Investigation Figure 2 LABDB database You will notice that it is much harder to find optimal distribution keys in a more complicated schema like this. In many situations you will be forced to choose between enabling co-located joins between one set of tables or another one. The following provides some details on the remaining tables: Table Number of Rows Primary Key REGION 5 R_REGIONKEY NATION 25 N_NATIONKEY CUSTOMER 150000 C_CUSTKEY ORDERS 1500000 O_ORDERKEY SUPPLIER 10000 S_SUPPKEY PART 200000 P_PARTKEY PARTSUPP 800000 - LINEITEM 6000000 - And on the involved relationships: Parent Table Child Table Parent table Join Column Child table Join Column REGION NATION R_REGIONKEY N_REGIONKEY NATION CUSTOMER N_NATIONKEY C_NATIONKEY NATION SUPPLIER N_NATIONKEY S_NATIONKEY CUSTOMER ORDERS C_CUSTKEY O_CUSTKEY ORDERS LINEITEM O_ORDERKEY L_ORDERKEY SUPPLIER LINEITEM S_SUPPKEY L_SUPPKEY SUPPLIER PARTSUPP S_SUPPKEY PS_SUPPKEY PART LINEITEM P_PARTKEY L_PARTKEY PART PARTSUPP P_PARTKEY PS_PARTKEY Given all that you heard in the presentation and lab, try to fill in the distribution keys in the chart below. Let's assume that we will not change the distribution keys for LINEITEM and ORDERS anymore. Table Distribution Key (up to 4 columns) or Random REGION NATION CUSTOMER SUPPLIER PART PARTSUPP ORDERS O_ORDERKEY LINEITEM L_ORDERKEY 6.2 Solution It is important to note that there is no optimal way to pick distribution keys. It always depends on the queries that run against the database. Without these queries it is only possible to follow some general rules: Co-Location between big tables (esp. if a fact table is involved) is more important than between small tables. Very small tables can be broadcast by the system with little performance penalty. If one table of a join is broadcast the other will not need to be redistributed. If you suspect that there will be lots of queries joining two big tables but you cannot distribute both of them on the expected join key, distributing one table on the join key is better than nothing, since it will lead to a single redistribute instead of a double redistribute. If we break down the problem, we can see that PART and PARTSUPP are the biggest two of the remaining tables. Recall that we have already distributed the LINEITEM and ORDERS tables based on the join key between the two tables as seen in available customer queries. So, it might make sense to distribute PART and PARTSUPP on the join key between these two tables. CUSTOMER is big as well and has two relationships. The first relationship is with the very small NATION table that is easily broadcasted by the system. The second relationship is with the ORDERS table which is big as well but already distributed by the order key. But as mentioned above a single redistribute is better than a double redistribute. Therefore, it makes sense to distribute the CUSTOMER table on the customer key, which is also the join key of this relationship. The situation is very similar for the SUPPLIER table. It has two very large child tables PARTSUPP and LINEITEM which are both related to it through the supplier key, so it should be distributed on this key. NATION and REGION are both very small and will most likely be broadcasted by the Optimizer. You could distribute those tables randomly, on their primary keys, on their join keys. In this case we have decided to distribute both on their primary keys but there is no definite right or wrong approaches. One possible solution for the distribution keys could be the following. Table Distribution Key (up to 4 columns) or Random REGION R_REGIONKEY NATION N_NATIONKEY CUSTOMER C_CUSTKEY SUPPLIER S_SUPPKEY PART P_PARTKEY PARTSUPP PS_PARTKEY ORDERS O_ORDERKEY LINEITEM L_ORDERKEY Finally, we will load the remaining tables. You should still be connected to the LABDB database. We now need to recreate the NATION and REGION tables with a new distribution key. To drop the old table versions execute the following commands. Input DROP TABLE NATION; DROP TABLE REGION; Quit the NZSQL console with the \\q command. Navigate to the lab folder by executing the cd ~/labs/dataDistribution command. Verify the SQL script creating the remaining 6 tables with the more remaining_tables.sql command. Now create the remaining tables and load the data into it with the following script: Input Output ./create_remaining.sh ERROR: relation does not exist LABDB.ADMIN.NATION CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully The error message at the top is expected since the script tries to clean up any old tables of the same name in case a reload is necessary. Also note that load logfiles have been created for these tables too. ( ls *.nzlog ). Review the load logfiles as time and interest permit. Input Output ls -l *.nzlog -rw-rw-r--. 1 nz nz 2315 Mar 30 03:55 CUSTOMER.ADMIN.LABDB.4178.nzlog -rw-rw-r--. 1 nz nz 2321 Mar 29 13:51 LINEITEM.ADMIN.LABDB.29339.nzlog -rw-rw-r--. 1 nz nz 2320 Mar 29 14:05 LINEITEM.ADMIN.LABDB.32336.nzlog -rw-rw-r--. 1 nz nz 2320 Mar 30 03:16 LINEITEM.ADMIN.LABDB.370.nzlog -rw-rw-r--. 1 nz nz 2320 Mar 29 15:49 LINEITEM.ADMIN.LABDB.9042.nzlog -rw-rw-r--. 1 nz nz 2209 Mar 30 03:55 NATION.ADMIN.LABDB.4134.nzlog -rw-rw-r--. 1 nz nz 2317 Mar 30 02:58 ORDERS.ADMIN.LABDB.31185.nzlog -rw-rw-r--. 1 nz nz 2317 Mar 30 03:18 ORDERS.ADMIN.LABDB.581.nzlog -rw-rw-r--. 1 nz nz 2306 Mar 30 03:55 PART.ADMIN.LABDB.4227.nzlog -rw-rw-r--. 1 nz nz 2318 Mar 30 03:56 PARTSUPP.ADMIN.LABDB.4254.nzlog -rw-rw-r--. 1 nz nz 2206 Mar 30 03:55 REGION.ADMIN.LABDB.4156.nzlog -rw-rw-r--. 1 nz nz 2223 Mar 30 03:55 SUPPLIER.ADMIN.LABDB.4205.nzlog Success Congratulations! You just have defined data distribution keys for a customer data schema in Netezza Performance Server. You can have a look at the created tables and their definitions with the commands you used in the previous chapters. We will continue to use the tables we created in the following labs.","title":"Data Distribution"},{"location":"nz-03-Data-Distribution/#1-data-distribution","text":"Netezza Performance Server is a family of data-warehousing appliances that combine high performance with low administrative effort. Due to the unique data warehousing centric architecture of Netezza Performance Server, most performance tuning tasks are either not necessary or automated. Unlike normal data warehousing solutions, no tablespaces need to be created or tuned, there are also no indexes, buffer pools or partitions. Since Netezza Performance Server is built on a massively parallel architecture that distributes data and workloads over many processing and data nodes, the single most important tuning factor is picking the right distribution key. The distribution key governs which data rows of a table are distributed to which data slice and it is very important to pick an optimal distribution key to avoid data skew, processing skew and to make joins co-located whenever possible.","title":"1 Data Distribution"},{"location":"nz-03-Data-Distribution/#11-objectives","text":"In this lab we will cover a typical scenario in a POC or customer engagement which involves an existing data warehouse for customer transactions. Figure 1. LABDB database Figure 1 shows a visualization of the tables in the data warehouse and the relationships between the tables. The warehouse contains the customers of the company, their orders, and the line items that are part of the order. The warehouse also has a list of suppliers, providing the parts that are part of the shipped line items. For this lab we already have the DDLs for creation of the tables and load files containing the warehouse data. Both are already in a format usable by the Netezza Performance Server System. In this lab we will define the distribution keys for these tables. In addition to the data and the DDLs we also have received a couple of queries from the customer that are usually run against the warehouse. Those are important input as well for picking optimal distribution keys.","title":"1.1 Objectives"},{"location":"nz-03-Data-Distribution/#2-check-the-number-of-data-slices","text":"By default, the NPS VirtualBox virtual machine only comes with 1 data slice. To perform the steps in this lab and the other performance related labs you will need 4 data slices. It is not recommended to run more than 4 data slices. If you did not configure your virtual machine for 4 data slices, go back to the lab called: 01-Setup-NPS-Virtual-Machine-Setup-Lab-Guide-Win10-OSX-Final and follow the section called: \" Increase the Number of Data Slices \". To check your data slices, login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using a terminal application (Windows PowerShell, PuTTY, Mac OSX Terminal) If you are continuing from the previous lab and are already connected to nzsql , quit the nzsql console with the \\q command. Input Output nzstate # if NPS is offline, run nzstart nzhw nzds [nz@localhost labs]$ nzstate System state is 'Online'. [nz@localhost labs]$ nzhw Description HW ID Location Role State Security ----------- ----- ---------- ------ ------ -------- Rack 1001 rack1 Active Ok N/A SPA 1002 spa1 Active Ok N/A SPU 1003 spa1.spu1 Active Online N/A Disk 1004 spa1.disk1 Active Ok N/A Disk 1005 spa1.disk2 Active Ok N/A Disk 1006 spa1.disk3 Active Ok N/A Disk 1007 spa1.disk4 Active Ok N/A [nz@localhost labs]$ nzds Data Slice Status SPU Partition Size (GiB) % Used Supporting Disks ---------- ------- ---- --------- ---------- ------ ---------------- 1 Unknown 1003 0 16 0.84 1004 2 Unknown 1003 1 16 0.84 1005 3 Unknown 1003 2 16 0.84 1006 4 Unknown 1003 3 16 0.84 1007 With the proper data slices continue to the next section.","title":"2 Check the Number of Data Slices"},{"location":"nz-03-Data-Distribution/#3-lab-setup","text":"This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using a terminal application (Windows PowerShell, PuTTY, Mac OSX Terminal) If you are continuing from the previous lab and are already connected to nzsql quit the nzsql console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input Output cd ~/labs/dataDistribution/setupLab ./setupLab.sh ERROR: DROP DATABASE: object LABDB does not exist. CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE The error message at the beginning is expected since the script tries to clean up existing LINEITEM tables. This lab is now setup to use for the remainder of the sections.","title":"3 Lab Setup"},{"location":"nz-03-Data-Distribution/#4-skew","text":"Tables in Netezza Performance Server are distributed across data slices based on the distribution method and key. If a bad data distribution method has been picked, it will result in skewed tables or processing skew. Data skew occurs when the distribution method puts significantly more records of a table on one data slice than on other data slices. Apart from bad performance this also results in a situation where the Netezza Performance Server can hold significantly less data than expected. Processing skew occurs if processing of queries is mainly taking place on some data slices for example because queries only apply to data on those data slices. Both types of skew result in suboptimal performance since in a parallel system the slowest node defines the total execution time.","title":"4 Skew"},{"location":"nz-03-Data-Distribution/#41-data-skew","text":"The first table we will create is LINEITEM , the main fact table of the schema. It contains roughly 6 million rows. To create the LINEITEM table, switch to the lab directory ~/labs/dataDistribution . To do this use the following command: (Notice that you can use bash auto complete by using the Tab key to complete folder and files names) Input Output cd ~/labs/dataDistribution ls -l [nz@localhost dataDistribution]$ ls -l total 24 -rwxr-xr-x. 1 nz nz 177 Apr 20 2020 create_lineitem_1.sh -rwxr-xr-x. 1 nz nz 170 Dec 9 2015 create_orders_1.sh -rwxr-xr-x. 1 nz nz 612 Dec 9 2015 create_remaining.sh -rwxr-xr-x. 1 nz nz 656 Mar 29 13:44 lineitem.sql -rwxr-xr-x. 1 nz nz 374 Mar 29 13:44 orders.sql -rwxr-xr-x. 1 nz nz 1660 Mar 29 13:44 remaining_tables.sql drwxr-xr-x. 2 nz nz 91 Mar 29 2020 setupLab The command line prompt changes to reflect the directory you are in ( dataDistribution ). Create the LINEITEM table by using the following script. Since the fact table is quite large this can take several minutes. Input Output ./create_lineitem_1.sh ERROR: relation does not exist LABDB.ADMIN.LINEITEM CREATE TABLE Load session of table 'LINEITEM' completed successfully The error message at the beginning is expected since the script tries to clean up existing LINEITEM tables. Also note that a load log file has been created in the ~/labs/dataDistribution directory named LINEITEM.ADMIN.LABDB.nzlog . ( ls \\*.nzlog ) Review the load logfile as time and interest permits. This load will take some time, if you want to see the status of the load run the following SQL from another terminal session. Input Output nzsql -c \"select * from _v_load_status;\" PLANID | DATABASENAME | TABLENAME | SCHEMANAME | USERNAME | BYTESPROCESSED | ROWSINSERTED | ROWSREJECTED | BYTESDOWNLOADED -------+--------------+-----------+------------+----------+----------------+--------------+--------------+----------------- 592 | LABDB | LINEITEM | ADMIN | LABADMIN | 704643063 | 5522182 | 0 | 718896512 (1 row) Now let's have a look at the created table, open the nzsql console by entering the command: Input Output nzsql Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM.ADMIN(ADMIN)=> Connect to the database LABDB as user LABADMIN by typing the following command: Input Output \\c LABDB LABADMIN You are now connected to database LABDB as user LABADMIM LABDB.ADMIN(LABADMIN)=> You should now be connected to the LABDB database as the LABADMIN user. Let's have a look at the table we just created. First, we want to see a description of its columns and distribution key. Use the NZSQL describe command \\d LINEITEM to get a description of the table. Input Output \\d LINEITEM Table \"LINEITEM\" Attribute | Type | Modifier | Default Value -----------------+-----------------------+----------+--------------- L_ORDERKEY | INTEGER | NOT NULL | L_PARTKEY | INTEGER | NOT NULL | L_SUPPKEY | INTEGER | NOT NULL | L_LINENUMBER | INTEGER | NOT NULL | L_QUANTITY | NUMERIC(15,2) | NOT NULL | L_EXTENDEDPRICE | NUMERIC(15,2) | NOT NULL | L_DISCOUNT | NUMERIC(15,2) | NOT NULL | L_TAX | NUMERIC(15,2) | NOT NULL | L_RETURNFLAG | CHARACTER(1) | NOT NULL | L_LINESTATUS | CHARACTER(1) | NOT NULL | L_SHIPDATE | DATE | NOT NULL | L_COMMITDATE | DATE | NOT NULL | L_RECEIPTDATE | DATE | NOT NULL | L_SHIPINSTRUCT | CHARACTER(25) | NOT NULL | L_SHIPMODE | CHARACTER(10) | NOT NULL | L_COMMENT | CHARACTER VARYING(44) | NOT NULL | Distributed on hash: \"L_LINESTATUS\" We can see that the LINEITEM table has 16 columns with different data types. Some of the columns have a \"key\" suffix and substrings containing the names of other tables and are most likely foreign keys of dimension tables. The distribution key is L_LINESTATUS , which is of a CHAR(1) data type. Now let's have a look at the data in the table. To return a limited number of rows you can use the limit keyword in your select queries. Execute the following select command to return 10 rows of the LINEITEM table. For readability we only select a couple of columns including the order key ( L_ORDERKEY ), the ship date ( L_SHIPDATE ) and the line status ( L_LINESTATUS ) distribution key: Input Output SELECT L_ORDERKEY, L_QUANTITY, L_SHIPDATE, L_LINESTATUS FROM LINEITEM LIMIT 10; L_ORDERKEY | L_QUANTITY | L_SHIPDATE | L_LINESTATUS ------------+------------+------------+-------------- 3 | 45.00 | 1994-02-02 | F 3 | 49.00 | 1993-11-09 | F 3 | 27.00 | 1994-01-16 | F 3 | 2.00 | 1993-12-04 | F 3 | 28.00 | 1993-12-14 | F 3 | 26.00 | 1993-10-29 | F 5 | 15.00 | 1994-10-31 | F 5 | 26.00 | 1994-10-16 | F 5 | 50.00 | 1994-08-08 | F 6 | 37.00 | 1992-04-27 | F (10 rows) From this limited sample we cannot make any definite judgement, but we can make a couple of assumptions. While the L_ORDERKEY column is not unique, it seems to have a lot of distinct values. The L_SHIPDATE column also appears to have a lot of distinct shipping date values. On the other hand, our current distribution key L_LINESTATUS has only two values, which may make it a bad distribution key. It is possible that you get different results. Since a database table is an unordered set, it is probable that you get different results. For example, only O or F values may be in the L_LINESTATUS column. We will now verify the number of distinct values in the L_LINESTATUS column with a SELECT DISTINCT statement. To return a list of all distinct values that are in the L_LINESTATUS column execute the following SQL command: Input Output SELECT DISTINCT L_LINESTATUS FROM LINEITEM; L_LINESTATUS -------------- O F (2 rows) We can see that the L_LINESTATUS column only contains two distinct values. As a distribution key, this will result in a table that is only distributed to two of the available data slices. We verify this by executing the following SQL call, which will return a list of all data slices which contain rows of the LINEITEM table, and the corresponding number of rows stored in them: Input Output SELECT DATASLICEID, COUNT(*) FROM LINEITEM GROUP BY DATASLICEID; DATASLICEID | COUNT -------------+--------- 4 | 2996217 1 | 3004998 (2 rows) Every Netezza Performance Server table has a hidden column DATASLICEID , which contains the id of the data slice the selected row is being stored in. By executing a SQL query that does a GROUP BY on this column and counts the number of rows for each DATASLICEID , data skew can be detected. In this case the table has been distributed to only two of the available four data slices. This means that only half of the available space and compute is used, which will result in low performance during most query executions. In general, a good distribution key should have many unique values, with an even distribution of rows in the data slices. Columns with few unique values, especially boolean columns or date columns, should not be considered as distribution keys.","title":"4.1 Data Skew"},{"location":"nz-03-Data-Distribution/#42-processing-skew","text":"Even in tables that are distributed evenly across data slices, data processing for queries can be concentrated or skewed to a limited number of data slices. One way for processing skew to occur for a query is if there is a predicate ( WHERE condition) on the distribution key, and the distribution key has few unique values. Using a DATE columns for a distribution key is notorious for causing processing skew, and usually should be avoided. The next section of the lab will show this effect. First, we will pick a new distribution key. As we have seen it should have many unique (distinct) values. One of the columns that did fit this description was the L_SHIPDATE column. Check the number of distinct values in the L_SHIPDATE column with the COUNT(DISTINCT ) statement: Input Output SELECT COUNT(DISTINCT L_SHIPDATE) FROM LINEITEM; COUNT ------- 2526 (1 row) The column has over 2500 distinct values and therefore has more than enough unique values to guarantee a good data distribution on 4 data slices. Now let's reload the LINEITEM table with the new distribution key. For this we need to change the SQL of the load script we executed at the beginning of the lab. Exit the nzsql console by entering: \\q . You should now be in the lab directory ~/labs/dataDistribution . The table creation statement is in the lineitem.sql file. We will need to make changes to the file with a text editor. Open the file with the default Linux text editor vi. To do this enter the following command: Input vi lineitem.sql The vi editor has two modes, a command mode used to save files, quit the editor etc. and an insert mode. Initially you will be in the command mode. To change the lineitem.sql file you need to switch to the insert mode by pressing \"i\". The editor will show an -- INSERT -- at the bottom of the screen. Use the delete key to remove l_linestatus and type l_shipdate . Note If you are not comfortable with vi you can used gedit from the virtual machine desktop. Input gedit lineitem.sql You can now use the cursor keys to navigate to the DISTRIBUTE ON clause at the bottom of the create command. Change the distribution key to L_SHIPDATE . The editor should now look like the following: create table lineitem ( l_orderkey integer not null , l_partkey integer not null , l_suppkey integer not null , l_linenumber integer not null , l_quantity decimal(15,2) not null , l_extendedprice decimal(15,2) not null , l_discount decimal(15,2) not null , l_tax decimal(15,2) not null , l_returnflag char(1) not null , l_linestatus char(1) not null , l_shipdate date not null , l_commitdate date not null , l_receiptdate date not null , l_shipinstruct char(25) not null , l_shipmode char(10) not null , l_comment varchar(44) not null ) DISTRIBUTE ON (l_shipdate); ~ ~ -- INSERT -- We will now save our changes. Press Esc to switch back into command mode. You should see that the ---INSERT--- string at the bottom of the screen vanishes. Enter :wq! and press enter to write the file and quit the editor without any questions. If you made a mistake editing and would like to undo it press Esc then enter :q! and go back to step 3. Now repeat steps 3-5 of section 2.1 Data Skew: a. Recreate and load the LINEITEM table with your new distribution key by executing the ./create_lineitem_1.sh command b. Use the nzsql command to enter the command console c. Switch to the LABDB database by using the \\c LABDB LABADMIN command. d. Validate the distribution key is L_SHIPDATE using the \\d LINEITEM command. Now we verify that the new distribution key results in a good data distribution. For this we will repeat the query, which returns the number of rows for each datasliceid of the LINEITEM table. Execute the following command: Input Output SELECT DATASLICEID, COUNT(*) FROM LINEITEM GROUP BY DATASLICEID ORDER BY 1; DATASLICEID | COUNT -------------+--------- 1 | 1499990 2 | 1497649 3 | 1501760 4 | 1501816 (4 rows) We can see that the data distribution is much better now. All four data slices have a roughly equal number of rows. Now that we have a database table with a good data distribution let's look at a couple of queries. The following query is executed regularly by the end user. It returns the average quantity shipped on a given day grouped by the shipping mode. Execute the following query: Input Output SELECT AVG(L_QUANTITY) AS AVG_Q, L_SHIPMODE FROM LINEITEM WHERE L_SHIPDATE = '1996-03-29' GROUP BY L_SHIPMODE; AVG_Q | L_SHIPMODE -----------+------------ 26.045455 | MAIL 24.494186 | REG AIR 25.562500 | SHIP 24.780282 | RAIL 27.147826 | TRUCK 25.708556 | AIR 26.038567 | FOB (7 rows) This query will take all rows from the 29th March of 1996 and compute the average value of the L_QUANTITY column for each L_SHIPMODE value. It is a typical warehousing query insofar as a date column is used to restrict the row set that is taken as input for computation. In this example most rows of the LINEITEM table will be filtered away, only rows that have the specified date will be used as input for computation of the AVG aggregation. Execute the following SQL statement to see on which data slice we can find the rows from the 29th March of 1996: Input Output SELECT COUNT(\\*), DATASLICEID FROM LINEITEM WHERE L_SHIPDATE = '1996-03-29' GROUP BY DATASLICEID; COUNT | DATASLICEID -------+------------- 2501 | 2 (1 row) Since we used the shipping date column as a distribution key, all rows from a specific date can be found on one data slice and therefore also one SPU. This means that for our previous query all rows on other data slices are unused and the computation takes place only on one data slice and SPU. This is known as processing skew. While this one SPU is working the other SPUs will be idle. Columns that are often used in WHERE conditions should not be used as distribution keys, since this can easily result in processing skew. In warehousing environments this is especially true for DATE columns. Good distribution keys are key columns (join columns); they have lots of unique values and rarely result in processing skew. In our example we have a couple of distribution keys to choose from: L_SUPPKEY , L_ORDERKEY , L_PARTKEY . All columns have many distinct values.","title":"4.2 Processing Skew"},{"location":"nz-03-Data-Distribution/#5-co-location","text":"The most basic warehouse schema consists of a fact table containing a list of all business transactions and a set of dimension tables that contain the different actors, objects, locations and time points that have taken part in these transactions. This means that most queries will not only access one database table but will require joins between many tables. In Netezza Performance Server tables are distributed over a large numbers of data slices on different SPUs. This means that during a join of two tables there are two possibilities. Rows of the two tables that are joined together are on the same data slice, which means the rows are co-located and can be joined locally. Rows of the two tables that are joined together are on different data slices, which means that rows need to be redistributed (moved) so that rows being joined are on the same data slice.","title":"5 Co-Location"},{"location":"nz-03-Data-Distribution/#51-investigation","text":"Co-location has big performance advantages. In the following section we will demonstrate this by introducing a second table called ORDERS. Switch to the Linux command line, if you are in the NZSQL console. Do this with the \\q command. Switch to the data distribution lab directory with the cd ~/labs/dataDistribution . command, create and load the ORDERS table by executing the ./create_orders_1.sh script Input Output cd ~/labs/dataDistribution/ ./create_orders_1.sh ERROR: relation does not exist LABDB.ADMIN.ORDERS CREATE TABLE Load session of table 'ORDERS' completed successfully Enter the NZSQL console with the nzsql labdb labadmin command and take a look at the ORDERS table with the \\d orders command. Input Output nzsql labdb LABDB.ADMIN(LABADMIN)=> \\d orders Table \"ORDERS\" Attribute | Type | Modifier | Default Value -----------------+-----------------------+----------+--------------- O_ORDERKEY | INTEGER | NOT NULL | O_CUSTKEY | INTEGER | NOT NULL | O_ORDERSTATUS | CHARACTER(1) | NOT NULL | O_TOTALPRICE | NUMERIC(15,2) | NOT NULL | O_ORDERDATE | DATE | NOT NULL | O_ORDERPRIORITY | CHARACTER(15) | NOT NULL | O_CLERK | CHARACTER(15) | NOT NULL | O_SHIPPRIORITY | INTEGER | NOT NULL | O_COMMENT | CHARACTER VARYING(79) | NOT NULL | Distributed on random: (round-robin) The ORDERS table has a key column O_ORDERKEY that is the primary key of the table. The ORDERS table contains information on the order value, priority and date, and has been distributed on random. This means that Netezza Performance Server doesn't use a hash-based algorithm to distribute the data. Instead, rows are distributed randomly on the available data slices in a round-robin fashion. You can check the data distribution of the table, using the methods we have used before for the LINEITEM table. The data distribution will be perfect. For example, try the following: Input Output SELECT COUNT(*), DATASLICEID FROM ORDERS GROUP BY DATASLICEID ORDER BY 1; COUNT | DATASLICEID --------+------------- 374393 | 3 374518 | 1 375517 | 2 375572 | 4 (4 rows) There will also not be any processing skew for queries on the single table, since in a random distribution there can be no correlation between any WHERE condition and the distribution key. We have received another typical query from our customer. It returns the average total price and item quantity of all orders grouped by the shipping priority. This query has to join together the LINEITEM and ORDERS tables to get the total order cost from the orders table and the quantity for each shipped item from the LINEITEM table. The tables are joined with an inner join on the L_ORDERKEY column. Execute the following query and note the approximate execution time: Input Output \\time SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY, O.O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; PRICE | QUANTITY | O_ORDERPRIORITY ---------------+-----------+----------------- 189219.594349 | 25.532474 | 5-LOW 189285.029553 | 25.526186 | 2-HIGH 188546.457203 | 25.472923 | 4-NOT SPECIFIED 189026.093657 | 25.494518 | 3-MEDIUM 189093.608965 | 25.513563 | 1-URGENT (5 rows) Elapsed time: 0m4.293s Notice that the query takes about 4 seconds to complete on our machine. The actual execution times on your machine will be different. Remember that the ORDERS table was distributed randomly and the LINEITEM table is still distributed by the L_SHIPDATE column. The join on the other hand is taking place on the L_ORDERKEY and O_ORDERKEY columns. We will now have a quick look at what is happening inside the Netezza Performance Server system in this scenario. To do this we use the Netezza Performance Server EXPLAIN function. This will be more thoroughly covered in the Optimization lab. Execute the following command: Input EXPLAIN VERBOSE SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY, O.O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; You will get a long output. Scroll up until you see your command in the text window. The start of the EXPLAIN output should look like the following: Output EXPLAIN VERBOSE SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY, O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; QUERY VERBOSE PLAN: Node 1. [SPU Sequential Scan table \"ORDERS\" as \"O\" {}] -- Estimated Rows = 1500000, Width = 27, Cost = 0.0 .. 1653.2, Conf = 100.0 Projections: 1:O.O_TOTALPRICE 2:O.O_ORDERPRIORITY 3:O.O_ORDERKEY [SPU Distribute on {(O.O_ORDERKEY)}] [HashIt for Join] Node 2. [SPU Sequential Scan table \"LINEITEM\" as \"L\" {(L.L_SHIPDATE)}] -- Estimated Rows = 6001215, Width = 12, Cost = 0.0 .. 6907.1, Conf = 100.0 Projections: 1:L.L_QUANTITY 2:L.L_ORDERKEY [SPU Distribute on {(L.L_ORDERKEY)}] Node 3. [SPU Hash Join Stream \"Node 2\" with Temp \"Node 1\" {(O.O_ORDERKEY,L.L_ORDERKEY)}] -- Estimated Rows = 90018225000, Width = 31, Cost = 1653.2 .. 928769.9, Conf = 80.0 Restrictions: (L.L_ORDERKEY = O.O_ORDERKEY) Projections: 1:O.O_TOTALPRICE 2:L.L_QUANTITY 3:O.O_ORDERPRIORITY [SPU Fabric Join] ..<Rest of EXPLAIN Plan>.. The EXPLAIN functionality will be covered in detail in a following chapter, but it is easy to see what is happening here. What's happening is the system is redistributing both the ORDERS and LINEITEM tables so that the rows can be joined together. This is very bad because both tables are of significant size so there is a considerable overhead. This inefficient double redistribution occurs because neither of the tables are distributed on the join key. In the next section we will fix this.","title":"5.1 Investigation"},{"location":"nz-03-Data-Distribution/#52-co-located-joins","text":"In the last section we have seen that a query using joins can result in costly data redistribution during join execution when the joined tables are not distributed on the join key. In this section we will reload the tables based on the mutual join key to enhance performance during joins. Exit the NZSQL console with the [\\q command. If not already in the dataDistribution directory run the command cd ~/labs/dataDistribution to enter the directory. Change the distribution key in the lineitem.sql file to L_ORDERKEY : a. Open the file with the vi editor (or gedit from the VM desktop) by executing the command: vi lineitem.sql ( gedit lineitem.sql ). b. In vi switch to INSERT mode by pressing \"i\" c. Navigate with the cursor keys to the DISTRIBUTE ON clause and change it to DISTRIBUTE ON (L_ORDERKEY) d. Exit the INSERT mode by pressing ESC e. Enter :wq! In the command line of the vi editor and press enter. Before pressing enter your screen should look like the following: create table lineitem ( l_orderkey integer not null , l_partkey integer not null , l_suppkey integer not null , l_linenumber integer not null , l_quantity decimal(15,2) not null , l_extendedprice decimal(15,2) not null , l_discount decimal(15,2) not null , l_tax decimal(15,2) not null , l_returnflag char(1) not null , l_linestatus char(1) not null , l_shipdate date not null , l_commitdate date not null , l_receiptdate date not null , l_shipinstruct char(25) not null , l_shipmode char(10) not null , l_comment varchar(44) not null ) DISTRIBUTE ON (l_orderkey); Change the Distribution key in the orders.sql file to O_ORDERKEY . a. Open the file with the vi editor by executing the command: vi orders.sql b. Switch to INSERT mode by pressing \"i\" c. Navigate with the cursor keys to the DISTRIBUTE ON clause and change it to DISTRIBUTE ON (O_ORDERKEY) d. Exit the INSERT mode by pressing ESC e. Enter :wq! In the command line of the VI editor and Press Enter. Before pressing Enter your screen should look like the following: create table orders ( o_orderkey integer not null , o_custkey integer not null , o_orderstatus char(1) not null , o_totalprice decimal(15,2) not null , o_orderdate date not null , o_orderpriority char(15) not null , o_clerk char(15) not null , o_shippriority integer not null , o_comment varchar(79) not null ) DISTRIBUTE ON (O_ORDERKEY); Recreate and load the LINEITEM table with the distribution key L_ORDERKEY by executing the ./create_lineitem_1.sh script. Input Output cd ~/labs/dataDistribution/ ./create_lineitem_1.sh DROP TABLE CREATE TABLE Load session of table 'LINEITEM' completed successfully Recreate and load the ORDERS table with the distribution key O_ORDERKEY by executing the ./create_orders_1.sh script. Input Output ./create_orders_1.sh DROP TABLE CREATE TABLE Load session of table 'ORDERS' completed successfully Enter the NZSQL console by executing the nzsql labdb labadmin command and check the distribution key using the \\d lineitem and \\d orders commands. Input Output Input Output nzsql labdb labadmin \\d lineitem Table \"LINEITEM\" Attribute | Type | Modifier | Default Value -----------------+-----------------------+----------+--------------- L_ORDERKEY | INTEGER | NOT NULL | L_PARTKEY | INTEGER | NOT NULL | L_SUPPKEY | INTEGER | NOT NULL | L_LINENUMBER | INTEGER | NOT NULL | L_QUANTITY | NUMERIC(15,2) | NOT NULL | L_EXTENDEDPRICE | NUMERIC(15,2) | NOT NULL | L_DISCOUNT | NUMERIC(15,2) | NOT NULL | L_TAX | NUMERIC(15,2) | NOT NULL | L_RETURNFLAG | CHARACTER(1) | NOT NULL | L_LINESTATUS | CHARACTER(1) | NOT NULL | L_SHIPDATE | DATE | NOT NULL | L_COMMITDATE | DATE | NOT NULL | L_RECEIPTDATE | DATE | NOT NULL | L_SHIPINSTRUCT | CHARACTER(25) | NOT NULL | L_SHIPMODE | CHARACTER(10) | NOT NULL | L_COMMENT | CHARACTER VARYING(44) | NOT NULL | Distributed on hash: \"L_ORDERKEY\" nzsql labdb labadmin \\d orders Table \"ORDERS\" Attribute | Type | Modifier | Default Value -----------------+-----------------------+----------+--------------- O_ORDERKEY | INTEGER | NOT NULL | O_CUSTKEY | INTEGER | NOT NULL | O_ORDERSTATUS | CHARACTER(1) | NOT NULL | O_TOTALPRICE | NUMERIC(15,2) | NOT NULL | O_ORDERDATE | DATE | NOT NULL | O_ORDERPRIORITY | CHARACTER(15) | NOT NULL | O_CLERK | CHARACTER(15) | NOT NULL | O_SHIPPRIORITY | INTEGER | NOT NULL | O_COMMENT | CHARACTER VARYING(79) | NOT NULL | Distributed on hash: \u201cO_ORDERKEY\u201d Repeat executing the explain of our join query from the previous section by executing the following command: Input Output EXPLAIN VERBOSE SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY,O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; EXPLAIN VERBOSE SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY, O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; QUERY VERBOSE PLAN: Node 1. [SPU Sequential Scan table \"ORDERS\" as \"O\" {(O.O_ORDERKEY)}] -- Estimated Rows = 1500000, Width = 27, Cost = 0.0 .. 1653.2, Conf = 100.0 Projections: 1:O.O_TOTALPRICE 2:O.O_ORDERPRIORITY 3:O.O_ORDERKEY [HashIt for Join] Node 2. [SPU Sequential Scan table \"LINEITEM\" as \"L\" {(L.L_ORDERKEY)}] -- Estimated Rows = 6001215, Width = 12, Cost = 0.0 .. 6907.1, Conf = 100.0 Projections: 1:L.L_QUANTITY 2:L.L_ORDERKEY Node 3. [SPU Hash Join Stream \"Node 2\" with Temp \"Node 1\" {(O.O_ORDERKEY,L.L_ORDERKEY)}] -- Estimated Rows = 90018225000, Width = 31, Cost = 1653.2 .. 892653.2, Conf = 80.0 Restrictions: (L.L_ORDERKEY = O.O_ORDERKEY) Projections: 1:O.O_TOTALPRICE 2:L.L_QUANTITY 3:O.O_ORDERPRIORITY ..<Rest of EXPLAIN Plan>.. The query itself has not been changed. The only changes are in the distribution keys of the involved tables. You will again see a long output. Scroll up to the start of the output, directly after your query. Again, we do not want to make a complete analysis of the explain output. We will cover that in more detail in later chapters. But if you compare the output with the output of the last section you will see that the [SPU Distribute on O.O_ORDERKEY)}] nodes have vanished. The reason is that the join is now co-located because both tables are distributed on the join key. All rows to be joined from both tables are on the same data slices. You may see a distribution node further below during the execution of the group by clause, but this is estimated to distribute only one hundred rows which has no negative performance influence. Finally execute the joined query again: Input Output \\time SELECT AVG(O.O_TOTALPRICE) AS PRICE, AVG(L.L_QUANTITY) AS QUANTITY,O_ORDERPRIORITY FROM ORDERS AS O, LINEITEM AS L WHERE L.L_ORDERKEY=O.O_ORDERKEY GROUP BY O_ORDERPRIORITY; PRICE | QUANTITY | O_ORDERPRIORITY ---------------+-----------+----------------- 189219.594349 | 25.532474 | 5-LOW 189285.029553 | 25.526186 | 2-HIGH 189093.608965 | 25.513563 | 1-URGENT 189026.093657 | 25.494518 | 3-MEDIUM 188546.457203 | 25.472923 | 4-NOT SPECIFIED (5 rows) Elapsed time: 0m1.051s The query should return the same results as in the previous section but run faster even in the VM environment. In a real Netezza Performance Server with 8, 16 or more SPUs the difference would be much more significant. You now have loaded the LINEITEM and ORDERS table into your Performance Server appliance using the optimal distribution key for these tables for most situations. a. Both tables are distributed evenly across data slices, so there is no data skew. b. The distribution key is highly unlikely to result in processing skew, since most WHERE conditions will restrict a key column evenly c. Since ORDERS is a parent table of LINEITEM , with a foreign key relationship between them, most queries joining them together will utilize the join key. These queries will be co-located. Now we will pick the distribution keys of the full schema.","title":"5.2 Co-Located Joins"},{"location":"nz-03-Data-Distribution/#6-schema-creation","text":"Now that we have created the ORDERS and LINEITEM tables we need to pick the distribution keys for the remaining tables as well.","title":"6 Schema Creation"},{"location":"nz-03-Data-Distribution/#61-investigation","text":"Figure 2 LABDB database You will notice that it is much harder to find optimal distribution keys in a more complicated schema like this. In many situations you will be forced to choose between enabling co-located joins between one set of tables or another one. The following provides some details on the remaining tables: Table Number of Rows Primary Key REGION 5 R_REGIONKEY NATION 25 N_NATIONKEY CUSTOMER 150000 C_CUSTKEY ORDERS 1500000 O_ORDERKEY SUPPLIER 10000 S_SUPPKEY PART 200000 P_PARTKEY PARTSUPP 800000 - LINEITEM 6000000 - And on the involved relationships: Parent Table Child Table Parent table Join Column Child table Join Column REGION NATION R_REGIONKEY N_REGIONKEY NATION CUSTOMER N_NATIONKEY C_NATIONKEY NATION SUPPLIER N_NATIONKEY S_NATIONKEY CUSTOMER ORDERS C_CUSTKEY O_CUSTKEY ORDERS LINEITEM O_ORDERKEY L_ORDERKEY SUPPLIER LINEITEM S_SUPPKEY L_SUPPKEY SUPPLIER PARTSUPP S_SUPPKEY PS_SUPPKEY PART LINEITEM P_PARTKEY L_PARTKEY PART PARTSUPP P_PARTKEY PS_PARTKEY Given all that you heard in the presentation and lab, try to fill in the distribution keys in the chart below. Let's assume that we will not change the distribution keys for LINEITEM and ORDERS anymore. Table Distribution Key (up to 4 columns) or Random REGION NATION CUSTOMER SUPPLIER PART PARTSUPP ORDERS O_ORDERKEY LINEITEM L_ORDERKEY","title":"6.1 Investigation"},{"location":"nz-03-Data-Distribution/#62-solution","text":"It is important to note that there is no optimal way to pick distribution keys. It always depends on the queries that run against the database. Without these queries it is only possible to follow some general rules: Co-Location between big tables (esp. if a fact table is involved) is more important than between small tables. Very small tables can be broadcast by the system with little performance penalty. If one table of a join is broadcast the other will not need to be redistributed. If you suspect that there will be lots of queries joining two big tables but you cannot distribute both of them on the expected join key, distributing one table on the join key is better than nothing, since it will lead to a single redistribute instead of a double redistribute. If we break down the problem, we can see that PART and PARTSUPP are the biggest two of the remaining tables. Recall that we have already distributed the LINEITEM and ORDERS tables based on the join key between the two tables as seen in available customer queries. So, it might make sense to distribute PART and PARTSUPP on the join key between these two tables. CUSTOMER is big as well and has two relationships. The first relationship is with the very small NATION table that is easily broadcasted by the system. The second relationship is with the ORDERS table which is big as well but already distributed by the order key. But as mentioned above a single redistribute is better than a double redistribute. Therefore, it makes sense to distribute the CUSTOMER table on the customer key, which is also the join key of this relationship. The situation is very similar for the SUPPLIER table. It has two very large child tables PARTSUPP and LINEITEM which are both related to it through the supplier key, so it should be distributed on this key. NATION and REGION are both very small and will most likely be broadcasted by the Optimizer. You could distribute those tables randomly, on their primary keys, on their join keys. In this case we have decided to distribute both on their primary keys but there is no definite right or wrong approaches. One possible solution for the distribution keys could be the following. Table Distribution Key (up to 4 columns) or Random REGION R_REGIONKEY NATION N_NATIONKEY CUSTOMER C_CUSTKEY SUPPLIER S_SUPPKEY PART P_PARTKEY PARTSUPP PS_PARTKEY ORDERS O_ORDERKEY LINEITEM L_ORDERKEY Finally, we will load the remaining tables. You should still be connected to the LABDB database. We now need to recreate the NATION and REGION tables with a new distribution key. To drop the old table versions execute the following commands. Input DROP TABLE NATION; DROP TABLE REGION; Quit the NZSQL console with the \\q command. Navigate to the lab folder by executing the cd ~/labs/dataDistribution command. Verify the SQL script creating the remaining 6 tables with the more remaining_tables.sql command. Now create the remaining tables and load the data into it with the following script: Input Output ./create_remaining.sh ERROR: relation does not exist LABDB.ADMIN.NATION CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully The error message at the top is expected since the script tries to clean up any old tables of the same name in case a reload is necessary. Also note that load logfiles have been created for these tables too. ( ls *.nzlog ). Review the load logfiles as time and interest permit. Input Output ls -l *.nzlog -rw-rw-r--. 1 nz nz 2315 Mar 30 03:55 CUSTOMER.ADMIN.LABDB.4178.nzlog -rw-rw-r--. 1 nz nz 2321 Mar 29 13:51 LINEITEM.ADMIN.LABDB.29339.nzlog -rw-rw-r--. 1 nz nz 2320 Mar 29 14:05 LINEITEM.ADMIN.LABDB.32336.nzlog -rw-rw-r--. 1 nz nz 2320 Mar 30 03:16 LINEITEM.ADMIN.LABDB.370.nzlog -rw-rw-r--. 1 nz nz 2320 Mar 29 15:49 LINEITEM.ADMIN.LABDB.9042.nzlog -rw-rw-r--. 1 nz nz 2209 Mar 30 03:55 NATION.ADMIN.LABDB.4134.nzlog -rw-rw-r--. 1 nz nz 2317 Mar 30 02:58 ORDERS.ADMIN.LABDB.31185.nzlog -rw-rw-r--. 1 nz nz 2317 Mar 30 03:18 ORDERS.ADMIN.LABDB.581.nzlog -rw-rw-r--. 1 nz nz 2306 Mar 30 03:55 PART.ADMIN.LABDB.4227.nzlog -rw-rw-r--. 1 nz nz 2318 Mar 30 03:56 PARTSUPP.ADMIN.LABDB.4254.nzlog -rw-rw-r--. 1 nz nz 2206 Mar 30 03:55 REGION.ADMIN.LABDB.4156.nzlog -rw-rw-r--. 1 nz nz 2223 Mar 30 03:55 SUPPLIER.ADMIN.LABDB.4205.nzlog Success Congratulations! You just have defined data distribution keys for a customer data schema in Netezza Performance Server. You can have a look at the created tables and their definitions with the commands you used in the previous chapters. We will continue to use the tables we created in the following labs.","title":"6.2 Solution"},{"location":"nz-04-Database-Admin/","text":"Database Administration A factory-configured and installed IBM Netezza Performance Server will include some of the following components: An IBM Netezza Performance Server warehouse appliance with pre-installed IBM Netezza Performance Server software A preconfigured Linux operating system (with IBM Netezza Performance Server modifications). Several preconfigured Linux users and groups: User Password Description nz nz Linux user with login permission to the IBM Netezza Performance Server host container. nz has ownership of various IPS files in the installation directory. root netezza Linux superuser with login permission to the IBM Netezza Performance Server host container. Has ownership of various kernel and operating system files. An IBM Netezza Performance Server database user named ADMIN is the database super-user, and has full access to all system functions and objects User Password Description admin password IPS database superuser, with full access to all database administration privileges and objects A preconfigured database group named PUBLIC. All database users are automatically placed in the group PUBLIC and therefore inherit all of its privileges The IBM Netezza Performance Server warehouse appliance includes a highly optimized SQL dialect called IBM Netezza Performance Server Structured Query Language (SQL). You can use SQL commands to create and manage your IBM Netezza Performance Server databases, user access, and permissions for the databases, as well as to query and modify the contents of the databases On a new IBM Netezza Performance Server system, there is typically one main database, SYSTEM, and a database template, MASTER_DB. IBM Netezza Performance Server uses the MASTER_DB as a template for all other user databases that are created on the system. Initially, only the ADMIN user can create new databases, but the ADMIN user can grant other users' permission to create databases as well. The ADMIN user can also make another user the owner of a database, which gives that user ADMIN-like control over that database and its contents. The database creator becomes the default owner of the database. The owner can remove the database and all its objects, even if other users own objects within the database. Within a database, permitted users can create tables and populate them with data and query its contents. Objectives This lab will guide you through the typical steps to create and manage new IBM Netezza Performance Server users and groups after an IBM Netezza Performance Server has been delivered and configured. This will include creating a new database and assigning the appropriate privileges. The users and the database that you create in this lab will be used as a basis for the remaining labs in this bootcamp. After this lab you will have a basic understanding on how to plan and create an IBM Netezza Performance Server database environment. The first part of this lab will examine creating IBM Netezza Performance Server users and groups The second part of this lab will explore creating and using a database and tables. The table schema to be used within this bootcamp will be explained in the Data Distribution lab. Lab Setup Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/databaseAdministration/setupLab ./setupLab.sh Output drop database labdb ; DROP DATABASE drop user labadmin ; DROP USER drop user labuser ; nzsql:drop/drop_users:2: ERROR: DROP USER: object LABUSER does not exist. drop user dbuser ; nzsql:drop/drop_users:3: ERROR: DROP USER: object DBUSER does not exist. drop group lagrp ; nzsql:drop/drop_groups:1: ERROR: DROP GROUP: object LAGRP does not exist. drop group lugrp ; nzsql:drop/drop_groups:2: ERROR: DROP GROUP: object LUGRP does not exist. Errors are expected if this is the first time running this lab on this virtual machine. Connect to the system database as the IBM Netezza Performance Server database super-user, ADMIN, using the nzsql interface: Input nzsql or nzsql -d system -u admin -pw password There are different options you can use with the nzsql interface. Here we present two options, where the first option uses information set in the NZ environment variables, NZ_DATABASE, NZ_USER, and NZ_PASSWORD. You can add these the environment variables (set in .bashrc) as follows: export NZ_DATATASE=system export NZ_USER=admin export NZ_PASSWORD=password Now you do not need to specify the database name or the user. In the second option the information is explicitly stated using the -d, -u, and -pw options, which specifies the database name, the user, and the user's password, respectively. This option is useful when you want to connect to a different database or use a different user than specified in the NZ environment variables. Output Welcome to nzsql, the Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM ( ADMIN )= > Creating IBM Performance server Users and Groups The initial task after an IBM Netezza Performance Server has been set up is to create the database environment. This typically begins by creating a new set of database users and user groups before creating the database. You will use the ADMIN user to start creating additional database users and user groups. Then you will assign the appropriate authorities after the database has been created in the next section. The ADMIN user should only be used to perform administrative tasks within the IBM Netezza Performance Server and is not recommended for regular use. Also, it is highly advisable to develop a security access model to control user access against the database and the database objects in an IBM Netezza Performance Server. This will involve creating separate users to perform certain tasks. The security access model for this bootcamp environment will use three IBM Netezza Performance Server database users and two groups: Users: LABADMIN LABUSER DBUSER Groups: LAGRP LUGRP Connect to the Netezza image using PuTTy or another terminal program. Login to as user nz with password nz. (192.168.9.2) Creating New IBM Netezza Performance Server Users and Groups The three new IBM Netezza Performance Server database users will be initially created using the ADMIN user. The LABADMIN user will be the full owner of the bootcamp database. The LABUSER user will be allowed to perform data manipulation language (DML) operations (INSERT, UPDATE, DELETE) against all of the tables in the database, but they will not be allowed to create new objects like tables in the database. And lastly, the DBUSER user will only be allowed to read tables in the database, that is, they will only have LIST and SELECT privilege against tables in the database. The basic syntax to create a user is: CREATE USER username WITH PASSWORD 'string'; As the IBM Netezza Performance Server database super-user, ADMIN, you can now start to create the first user, LABADMIN, which will be the administrator of the database: (Note user and group names are not case sensitive) unless surrounded by quotes. Note: The default case is Upper case, meaning all objects created will be created as upper case unless quoted, in which case the case will be maintained. The IPS system could be reinit with Lower case as the default. Creating New IBM Netezza Performance Server Users: Input create user labadmin with password 'password' ; Output CREATE USER Later in this lab you will assign administrative ownership of the lab database to this user. Now you will create two additional IBM Netezza Performance Server database users that will have restricted access to the database. The first user, LABUSER, will have full DML access to the data in the tables, but will not be able to create or alter tables. For now, you will just create the user. We will set the privileges after the database is created: Input create user labuser with password 'password' ; Output CREATE USER Finally, we create the user DBUSER. This user will have even more limited access to the database since it will only be allowed to select data from the tables within the database. Again, you will set the privileges after the database is created: Input create user dbuser with password 'password' ; Output CREATE USER To list the existing database users in the environment, use the \\du internal slash option: Input \\d u Output List of Users USERNAME | VALIDUNTIL | ROWLIMIT | SESSIONTIMEOUT | QUERYTIMEOUT | DEF_PRIORITY | MAX_PRIORITY | USERESOURCEGRPID | USERESOURCEGRPNAME | CROSS_JOINS_ALLOWED ----------+------------+----------+----------------+-------------- +--------------+--------------+------------------+-------------------- +--------------------- ADMIN | | | 0 | 0 | NONE | NONE | | _ADMIN_ | NULL DBUSER | | 0 | 0 | 0 | NONE | NONE | 4901 | PUBLIC | NULL LABADMIN | | 0 | 0 | 0 | NONE | NONE | 4901 | PUBLIC | NULL LABUSER | | 0 | 0 | 0 | NONE | NONE | 4901 | PUBLIC | NULL ( 4 rows ) The other columns are explained in the WLM presentation. Creating New IBM Netezza Performance Server Groups IBM Netezza Performance Server database user groups are useful for organizing and managing database users. By default, IBM Netezza Performance Server contains one group with the name PUBLIC. All users are members in the PUBLIC group when they are created. Users can be members of other groups as well. In this section we will create two new IBM Netezza Performance Server database user groups. They will be initially created by the ADMIN user. We will create an administrative group LAGRP which is short for Lab Admin Group. This group will contain the LABADMIN user. The second group we create will be the LUGRP or Lab User Group. This group will contain the users LABUSER and DBUSER. Two different methods will be used to add the existing users to the newly created groups. Alternatively, the groups could be created first and then the users. The basic command to create a group is: CREATE GROUP groupname; As the IBM Netezza Performance Server database super-user, ADMIN, you will now create the first group, LAGRP, which will be the administrative group for the LABADMIN user: Input create group lagrp ; Output CREATE GROUP After the LAGRP group is created you will now add the LABADMIN user to this group. This is accomplished by using the ALTER statement. You can either ALTER the user or the group, for this task you will ALTER the group to add the LABADMIN user to the LAGRP group: Input alter group lagrp with add user labadmin ; Output ALTER GROUP To ALTER the user, you would use the following command: alter user LABADMIN with in group LAGRP; Now you will create the second group, LUGRP, which will be the user group for both the LABUSER and DBUSER users. You can specify the users to be included in the group when creating the group: Input create group lugrp with add user labuser dbuser ; Output CREATE GROUP To list the existing IBM Netezza Performance Server groups in the environment, use the \\dg internal slash option: Input \\d g Output List of Groups GROUPNAME | ROWLIMIT | SESSIONTIMEOUT | QUERYTIMEOUT | DEF_PRIORITY | MAX_PRIORITY | GRORSGPERCENT | RSGMAXPERCENT | JOBMAX | CROSS_JOINS_ALLOWED -----------+----------+----------------+--------------+-------------- +--------------+---------------+---------------+--------+--------------------- LAGRP | 0 | 0 | 0 | NONE | NONE | 0 | 100 | 0 | NULL LUGRP | 0 | 0 | 0 | NONE | NONE | 0 | 100 | 0 | NULL PUBLIC | 0 | 0 | 0 | NONE | NONE | 20 | 100 | 0 | NULL ( 3 rows ) The default database group is PUBLIC. The other columns are explained in the WLM presentation. To list the users in a group you can use one of the two internal slash options, \\dG, or \\dU. The internal slash option \\dG will list the groups with the associated users: Input \\d G Output List of Users in a Group GROUPNAME | USERNAME -----------+---------- LAGRP | LABADMIN LUGRP | DBUSER LUGRP | LABUSER PUBLIC | DBUSER PUBLIC | LABADMIN PUBLIC | LABUSER ( 6 rows ) The internal slash option \\dU will list the users with the associated group: Input \\d U Output List of Groups a User is a member USERNAME | GROUPNAME ----------+----------- DBUSER | LUGRP DBUSER | PUBLIC LABADMIN | LAGRP LABADMIN | PUBLIC LABUSER | LUGRP LABUSER | PUBLIC ( 6 rows ) Creating an IBM Netezza Performance Server Database The next step after the Netezza Performance Server database users and user groups have been created is to create the lab database. You will continue to use the ADMIN user to create the lab database then assign the appropriate authorities and privileges to the users created in the previous sections. The ADMIN user can also be used to create tables within the new database. However, the ADMIN user should only be used to perform administrative tasks. After the appropriate privileges have been assigned by the ADMIN user, the database can be handed over to the end-users to start creating and populating the tables in the database. Creating a Database and Transferring Ownership The lab database that will be created will be named LABDB. It will be initially created by the ADMIN user and then ownership of the database will be transferred to the LABADMIN user. The LABADMIN user will have full administrative privileges against the LABDB database. The basic syntax to create a database is: CREATE DATABASE database_name; As the Netezza Performance Server database super-user, ADMIN, you will create the first database, LABDB, using the CREATE DATABASE command: Input create database labdb ; Output CREATE DATABASE The database LABDB has been created. To view the existing databases use the internal slash option \\l : Input \\l Output List of databases DATABASE | OWNER ----------+------- LABDB | ADMIN SYSTEM | ADMIN ( 2 rows ) The owner of the newly created LABDB database is the ADMIN user. The other databases are the default database SYSTEM and the template database MASTER_DB. At this point you could continue by creating new tables as the ADMIN user. However, the ADMIN user should only be used to create users, groups, and databases, and to assign authorities and privileges. Therefore, we will transfer ownership of the LABDB database from the ADMIN user to the LABADMIN user we created previously. The ALTER DATABASE command is used to transfer ownership of an existing database: Input alter database labdb owner to labadmin ; Output ALTER DATABASE This is the only method to transfer ownership of a database to an existing user. The CREATE DATABASE command does not include this option. Check that the owner of the LABDB database is now the LABADMIN user: Input \\l Output List of databases DATABASE | OWNER ----------+---------- LABDB | LABADMIN SYSTEM | ADMIN ( 2 rows ) The owner of the LABDB database is now the LABADMIN user. The LABDB database is now created and the LABADMIN user has full privileges on the LABDB database. The user can create and alter objects within the database. You could now continue and start creating tables as the LABADMIN user. However, we will first finish assigning privileges to the two remaining database users that were created in the previous section. Assigning Authorities and Privileges One last task for the ADMIN user is to assign the privileges to the two users we created earlier, LABUSER and DBUSER. LABUSER user will have full DML rights against all tables in the LABDB database. It will not be allowed to create or alter tables within the database. User DBUSER will have more restricted access in the database and will only be allowed to read data from the tables in the database. The privileges will be controlled by a combination of setting the privileges at the group and user level. The LUGRP user group will be granted LIST and SELECT privileges against the database and tables within the database. So any member of the LUGRP will have these privileges. The full data manipulation privileges will be granted individually to the LABUSER user. The GRANT command that is used to assign object privileges has the following syntax: GRANT object_privilege [, ...] ON object [, ...] TO { PUBLIC | GROUP group | username } [ WITH GRANT OPTION ] GRANT admin_privilege [, ...] [ IN { [database_name.]schema_name | database_name.ALL | ALL.ALL } ] TO { PUBLIC | GROUP group | username } [ WITH GRANT OPTION ] object_privilege ALL, ABORT, ALTER, DELETE, DROP, EXECUTE, EXECUTE AS, GENSTATS, GROOM, INSERT, LABEL ACCESS, LABEL RESTRICT, LABEL EXPAND, LIST, SELECT, TRUNCATE, UPDATE admin_privilege ALL ADMIN, BACKUP, [ CREATE ] DATABASE, [ CREATE ] GROUP, [ CREATE ] INDEX, [ CREATE ] SCHEMA, [ CREATE ] SEQUENCE, [ CREATE ] SYNONYM, [ CREATE ] TABLE, [ CREATE ] EXTERNAL TABLE, [ CREATE ] TEMP TABLE, [ CREATE ] FUNCTION, [ CREATE ] AGGREGATE, [ CREATE ] USER, [ CREATE ] VIEW, [ CREATE ] MATERIALIZED VIEW, [CREATE] PROCEDURE, [ CREATE ] LIBRARY, [ CREATE ] SCHEDULER RULE, [ MANAGE ] HARDWARE, [ MANAGE ] SYSTEM, [ MANAGE ] SECURITY, RESTORE, UNFENCE, VACUUM As the Netezza Performance Server database super-user, ADMIN, connect to the LABDB database using the internal slash option \\c Input \\c labdb admin password Output You are now connected to database labdb as user admin. You will notice that the database name in command prompt has changed from SYSTEM to LABDB. First you will grant LIST privilege on the LABDB database to the group LUGRP. This will allow members of the LUGRP to view and connect to the LABDB database : Input grant list on labdb to lugrp ; Output GRANT To list the object permissions for a group use the following internal slash option, \\dpg : Input \\d pg lugrp Output Group object permissions for group 'LUGRP' Database Name | Schema Name | Object Name | L S I U D T L A D B L G O E C R X A | D G U S T E X Q Y V M I B R C S H F A L P N S R ---------------+-------------+-------------+-------------------------------------+------------------------------------------------- GLOBAL | GLOBAL | LABDB | X | ( 1 rows ) Object Privileges ( L ) ist ( S ) elect ( I ) nsert ( U ) pdate ( D ) elete ( T ) runcate ( L ) ock ( A ) lter ( D ) rop a ( B ) ort ( L ) oad ( G ) enstats Gr ( O ) om ( E ) xecute Label-A ( C ) cess Label- ( R ) estrict Label-E ( X ) pand Execute- ( A ) s Administration Privilege ( D ) atabase ( G ) roup ( U ) ser ( S ) chema ( T ) able T ( E ) mp E ( X ) ternal Se ( Q ) uence S ( Y ) nonym ( V ) iew ( M ) aterialized View ( I ) ndex ( B ) ackup ( R ) estore va ( C ) uum ( S ) ystem ( H ) ardware ( F ) unction ( A ) ggregate ( L ) ibrary ( P ) rocedure U ( N ) fence ( S ) ecurity Scheduler ( R ) ule The X in the L column of the list denotes that the LUGRP group has LIST object privileges on the LABDB global object. With the current privileges set for the LABUSER and DBUSER, they can now view and connect to the LABDB database as members of the LUGRP group. But these two users have no privileges to access any of the objects within the database. So you will grant LIST and SELECT privilege to the tables within the LABDB database to the members of the LUGRP : Input grant list, select on table to lugrp ; Output GRANT View the object permissions for the LUGRP group : Input \\d pg lugrp Output Group object permissions for group 'LUGRP' Database Name | Schema Name | Object Name | L S I U D T L A D B L G O E C R X A | D G U S T E X Q Y V M I B R C S H F A L P N S R ---------------+-------------+-------------+-------------------------------------+------------------------------------------------- GLOBAL | GLOBAL | LABDB | X | LABDB | ADMIN | TABLE | X X | ( 2 rows ) Object Privileges ( L ) ist ( S ) elect ( I ) nsert ( U ) pdate ( D ) elete ( T ) runcate ( L ) ock ( A ) lter ( D ) rop a ( B ) ort ( L ) oad ( G ) enstats Gr ( O ) om ( E ) xecute Label-A ( C ) cess Label- ( R ) estrict Label-E ( X ) pand Execute- ( A ) s Administration Privilege ( D ) atabase ( G ) roup ( U ) ser ( S ) chema ( T ) able T ( E ) mp E ( X ) ternal Se ( Q ) uence S ( Y ) nonym ( V ) iew ( M ) aterialized View ( I ) ndex ( B ) ackup ( R ) estore va ( C ) uum ( S ) ystem ( H ) ardware ( F ) unction ( A ) ggregate ( L ) ibrary ( P ) rocedure U ( N ) fence ( S ) ecurity Scheduler ( R ) ule The X in the L and S column denotes that the LUGRP group has both LIST and SELECT privileges on all of the tables in the LABDB database. (The LIST privilege is used to allow users to view the tables using the internal slash opton \\d.) The current privileges satisfy the DBUSER user requirements, which is to allow access to the LABDB database and SELECT access to all the tables in the database. But these privileges do not satisfy the requirements for the LABUSER user. The LABUSER user is to have full DML access to all the tables in the database. So you will grant SELECT, INSERT, UPDATE, DELETE, LIST, and TRUNCATE privileges on tables in the LABDB database to the LABUSER user: Input grant select , insert, update, delete, list, truncate on table to labuser ; Output GRANT To list the object permissions for a user use the \\dpu internal slash option,: Input \\d pu labuser Output bash User object permissions for user 'LABUSER' Database Name | Schema Name | Object Name | L S I U D T L A D B L G O E C R X A | D G U S T E X Q Y V M I B R C S H F A L P N S R ---------------+-------------+-------------+-------------------------------------+------------------------------------------------- LABDB | ADMIN | TABLE | X X X X X X | (1 rows) Object Privileges (L)ist (S)elect (I)nsert (U)pdate (D)elete (T)runcate (L)ock (A)lter (D)rop a(B)ort (L)oad (G)enstats Gr(O)om (E)xecute Label-A(C)cess Label-(R)estrict Label-E(X)pand Execute-(A)s Administration Privilege (D)atabase (G)roup (U)ser (S)chema (T)able T(E)mp E(X)ternal Se(Q)uence S(Y)nonym (V)iew (M)aterialized View (I)ndex (B)ackup (R)estore va(C)uum (S)ystem (H)ardware (F)unction (A)ggregate (L)ibrary (P)rocedure U(N)fence (S)ecurity Scheduler (R)ule The X under the L, S, I, U, D, T columns indicates that the LABUSER user has LIST, SELECT, INSERT, UPDATE, DELETE, and TRUNCATE privileges on all of the tables in the LABDB database. Now that all of the privileges have been set by the ADMIN user the LABDB database can be handed over to the end-users. The end-users can use the LABADMIN user to create objects, which include tables, in the database and also maintain the database. Creating Tables The LABADMIN user will be used to create tables in the LABDB database instead of the ADMIN user. Two tables will be created in this lab. The remaining tables for the LABDB database schema will be created in the Data Distribution lab. Data Distribution is an important aspect that should be considered when creating tables. This concept is not covered in this lab since it is discussed separately in the Data Distribution presentation. The two tables that will be created are the REGION and NATION tables. These two tables will be populated with data in the next section using LABUSER user. Two methods will be utilized to create these tables. The basic syntax to create a table is: General syntax: CREATE [ TEMPORARY | TEMP ] TABLE [IF NOT EXISTS] <table> ( <col> <type> [<col_constraint>][,<col> <type> [<col_constraint>]...] [<table_constraint>[,<table_constraint>... ] ) [ DISTRIBUTE ON { RANDOM | [HASH] (<col>[,<col>...]) } ] [ ORGANIZE ON { (<col>) | NONE } ] [ ROW SECURITY ] Where represents: [ CONSTRAINT <constraint_name> ] {NOT NULL | NULL | UNIQUE | PRIMARY KEY | DEFAULT <value> | <ref>} [ [ [ NOT ] DEFERRABLE ] { INITIALLY DEFERRED | INITIALLY IMMEDIATE } | [ INITIALLY DEFERRED | INITIALLY IMMEDIATE ] [ NOT ] DEFERRABLE ] Where represents: [ CONSTRAINT <constraint_name> ] {UNIQUE (<col>[,<col>...] ) | PRIMARY KEY (<pkcol_name>[,<pkcol_name>...] ) | FOREIGN KEY (<fkcol_name>[,<fkcol_name>...] ) <ref>} [ [ [ NOT ] DEFERRABLE ] { INITIALLY DEFERRED | INITIALLY IMMEDIATE } | [ INITIALLY DEFERRED | INITIALLY IMMEDIATE ] [ NOT ] DEFERRABLE ] Where represents: REFERENCES <reftable> [ (<refcol_name>[,<refcol_name>...] ) ] [ MATCH FULL ] [ ON UPDATE {CASCADE | RESTRICT | SET NULL | SET DEFAULT | NO ACTION} ] [ ON DELETE {CASCADE | RESTRICT | SET NULL | SET DEFAULT | NO ACTION} ] Connect to the LABDB database as the LABADMIN user using the internal slash option \\c: Input \\c labdb labadmin password Output You are now connected to database labdb as user labadmin. You will notice that the username in the command prompt has changed from ADMIN to LABADMIN. Since you already had an opened session, you could use the internal slash option \\c to connect to the database. However, if you had handed over this environment to the end user they would need to initiate a new connection using the nzsql interface. To use the nzsql interface to connect to the LABDB database as the LABADMIN user you could use the following options (no need to run these command, informational): nzsql -d labdb -u labadmin -pw password or with the short form, omitting the options: nzsql labdb labadmin password or you could set the environment variables to the following values and issue nzsql without options. export NZ_DATABASE=LABDB export NZ_USER=LABADMIN export NZ_PASSWORD=password In further labs we will often leave out the password parameter since it has been set to the same value \"password\" for all users. Now you can create the first table in the LABDB database. The first table you will create is the REGION table with the following columns and datatypes : Column Name Data Type R_REGIONKEY INTEGER R_NAME CHAR(25) R_COMMENT VARCHAR(152) To create the above table execute the following command: Input create table region ( r_regionkey integer, r_name char ( 25 ) , r_comment varchar ( 152 ) ) ; Output CREATE TABLE To list the tables in the LABDB database use the \\dt internal slash option: Input \\d t Output List of relations Schema | Name | Type | Owner --------+--------+-------+---------- ADMIN | REGION | TABLE | LABADMIN ( 1 row ) To describe a table you can use the internal slash option \\d <table name> : Input \\d region Output Table \"REGION\" Attribute | Type | Modifier | Default Value -------------+------------------------+----------+--------------- R_REGIONKEY | INTEGER | | R_NAME | CHARACTER ( 25 ) | | R_COMMENT | CHARACTER VARYING ( 152 ) | | Distributed on hash: \"R_REGIONKEY\" The Distributed on hash clause is the distribution method used by the table. If you do not explicitly specify a distribution method, a default distribution is used. In our system this is hash distributed on the first column R_REGIONKEY. This concept is discussed in the Data Distribution presentation and lab. Note: the default distribution can be change to RANDOM. This might be a preferred default distribution when the Netezza environment is supporting ad-hoc user table creation. In the event a user forgets to add a distribution key, RANDOM would at a minimum guarantee even distribution. However, you table if joined to other tables would require data movement to perform the join. Instead of typing out the entire create table statement at the nzsql command line you can read and execute commands from a file. You'll use this method to create the NATION table in the LABDB database with the following columns and data types: Column Name Data Type Constraint N_NATIONKEY INTEGER NOT NULL N_NAME CHAR(25) NOT NULL N_REGIONKEY INTEGER NOT NULL N_COMMENT VARCHAR(152) --- The full create table statement for the NATION table: create table nation ( n_nationkey integer not null, n_name char(25) not null, n_regionkey integer not null, n_comment varchar(152) ) distribute on random; The statement can be found in the nation.ddl file under the /home/nz/labs/databaseAdministration directory. To read and execute commands from a file use the \\i < file > internal slash option: Input \\i /home/nz/labs/databaseAdministration/nation.ddl Output CREATE TABLE You can run Linux commands while in the nzsql console. For example: list the nation.ddl file Input ! ls -l /home/nz/labs/databaseAdministration/nation.ddl Output -rwxr-xr-x. 1 nz nz 176 May 13 2020 /home/nz/labs/databaseAdministration/nation.ddl List all the tables in the LABDB database: Input \\d t Output List of relations Schema | Name | Type | Owner --------+--------+-------+---------- ADMIN | NATION | TABLE | LABADMIN ADMIN | REGION | TABLE | LABADMIN ( 2 rows ) Describe the NATION table : Input \\d nation Output Table \"NATION\" Attribute | Type | Modifier | Default Value -------------+------------------------+----------+--------------- N_NATIONKEY | INTEGER | NOT NULL | N_NAME | CHARACTER ( 25 ) | NOT NULL | N_REGIONKEY | INTEGER | NOT NULL | N_COMMENT | CHARACTER VARYING ( 152 ) | | Distributed on random: ( round-robin ) The distributed on random is the distribution method used, in this case the rows in the NATION table are distributed in round-robin fashion. This concept will be discussed separately in the Data Distribution presentation and lab. It is possible to continue to use LABADMIN user to perform DML queries since it is the owner of the database and holds all privileges on all of the objects in the databases. However, the LABUSER and DBUSER users will be used to perform DML queries against the tables in the database. Using DML Queries We will now use the LABUSER user to populate data into both the REGION and NATION tables. This user has full data manipulation language (DML) privileges in the database, but no data definition language (DDL) privileges. Only the LABADMIN has full DDL privileges in the database. Later in this course more efficient methods to populate tables with data are discussed. The DBUSER will also be used to read data from the tables, but it can not insert data into the tables since it has limited DML privileges in the database. Connect to the LABDB database as the LABUSER user using the internal slash option, \\c: Input \\c labdb labuser password Output You are now connected to database LABDB as user labuser. LABDB.ADMIN ( LABUSER )= > You will notice that the user name in the command prompt has changed from LABADMIN to LABUSER. First check which tables exist in the LABDB database using the \\dt internal slash option: Input \\d t Output List of relations Schema | Name | Type | Owner --------+--------+-------+---------- ADMIN | NATION | TABLE | LABADMIN ADMIN | REGION | TABLE | LABADMIN ( 2 rows ) Remember that the LABUSER user is a member of the LUGRP group which was granted LIST privileges on the tables in the LABDB database. This is the reason why it can list and view the tables in the LABDB database. If it did not have this privilege, it would not be able to see any of the tables in the LABDB database. The LABUSER user was created to perform DML operations against the tables in the LABDB database. However, it was restricted on performing DDL operations against the database. Let's see what happens when you try create a new table, t1, with one column, C1, using the INTEGER data type: Input create table t1 ( c1 integer ) ; Output ERROR: CREATE TABLE: permission denied. As expected, the create table statement is not allowed since LABUSER user does not have the privilege to create tables in the LABDB database. Let's continue by performing DML operations that the LABUSER user is allowed to perform against the tables in the LABDB database. Insert a new row into the REGION table: Input ```bash insert into region values (1, 'NA', 'north america'); Output INSERT 0 1 As expected, this operation is successful. The output of the INSERT gives feedback about the number of successfully inserted rows. Issue the SELECT statement against the REGION table to check the new row you just added to the table: Input select * from region ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+--------------- 1 | NA | north america ( 1 row ) Instead of typing DML statements at the nzsql command line, you can read and execute statements from a file. You will use this method to add the following three rows to the REGION table: R_REGIONKEY R_NAME R_COMMENT 2 SA South America 3 EMEA Europe, Middle East, Africa 4 AP Asia Pacific This is done with a SQL script containing the following commands: insert into region values (2, 'sa', 'south america'); insert into region values (3, 'emea', 'europe, middle east, africa'); insert into region values (4, 'ap', 'asia pacific'); It can be found in the region.dml file under the /home/nz/labs/databaseAdministration directory. To read and execute commands from a file use the \\i < file > internal slash option: Input \\i /home/nz/labs/databaseAdministration/region.dml Output INSERT 0 1 INSERT 0 1 INSERT 0 1 You can see from the output that the SQL script contained three INSERT statements. You will load data into the NATION table using an external table with the following command: Input insert into nation select * from external '/home/nz/labs/databaseAdministration/nation.del' ; Output INSERT 0 14 You loaded 14 rows into the table. Loading data into a table is covered in the Loading and Unloading Data presentation and lab. Now you will switch over to the DBUSER user, who only has SELECT privilege on the tables in the LABDB database. This privilege is granted to this user since he is a member of the LUGRP group. Use the internal slash option, \\c to connect to the LABDB database as the DBUSER user: Input \\c labdb dbuser password Output You are now connected to database labdb as user dbuser. You will notice that the username in the command prompt changes from LABUSER to DBUSER. Before trying to view rows from tables in the LABDB database, try to add a new row to the REGION table: Input insert into region values ( 5 , 'NP' , 'north pole' ) ; Output ERROR: Permission denied on \"REGION\" . As expected, the INSERT statement is not allowed since the DBUSER does not have the privilege to add rows to any tables in the LABDB database. Now select all rows from the REGION table: Input select * from region ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | NA | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific ( 4 rows ) Finally let's run a slightly more complex query. We want to return all nation names in Asia Pacific, together with their region name. To do this you need to execute a simple join using the NATION and REGION tables. The join key will be the region key, and to restrict results on the AP region you need to add a WHERE condition: Input select n_name, r_name from nation, region where n_regionkey = r_regionkey and r_name = 'ap' ; Output N_NAME | R_NAME ---------------------------+--------------------------- australia | ap japan | ap macau | ap hong kong | ap new zealand | ap ( 5 rows ) This should return the following results, containing all countries from the ap region. Roles IBM Netezza Performance Server introduced a new object called role. A role is a potential grantee or grantor of privileges and of other roles. Similar to a user, a role can own schemas and other objects. Roles can own database objects such as tables or functions. By using roles, you can also assign privileges. You can create roles that are available across all databases on the IBM Netezza Performance Server system. Similar to group objects, roles are also [global objects] . A user or group can be grant permission to a role. A session can then be set to run under a ROLE's authorization. Once a session is set to a ROLE the permissions used are from that ROLE's authorization and/or privileges, not a user or group permissions. This allows a user/application to isolate a database session to execute with a ROLE's permission. In this section of the lab we will create roles to demonstrate how roles can be used. Creating role Roles are create using the following syntax: CREATE ROLE role name [ WITH ADMIN owner name ] [ AS ADMIN ] 1. Login to the NPS command line as the nz user. See the section: Login to the NPS Command Line for details on how to access the command line for your given lab system. As the NPS database super-user, ADMIN, use nzsql to create the first role, LAROLE, which will be the administrator of the system: (Note: roles are not case sensitive) Input create role DBAROLE AS ADMIN ; Output CREATE ROLE This role will act as the super-user ADMIN and have all permissions on the database system. This is a simple way to give ADMIN rights to a user through the use of a role. Display roles Display the newly created role with the following command: Input show role ; Output ROLENAME | ROLEGRANTOR | ASADMIN ----------+-------------+--------- DBAROLE | ADMIN | t ( 1 row ) Create a new user without any permissions Create a new DBA user without any permissions. Input create user DBAUSER password 'password' ; Output CREATE USER Connect to the system database as DBAUSER and CREATE a database: Input \\c system dbauser password Output You are now connected to database system as user dbauser. Input create database testdb ; Output ERROR: CREATE DATABASE: permission denied. Notice that DBUSER cannot create a database as is expected since we didn't grant DATABASE permission to the DBUSER. Connect to the system database as ADMIN: Input \\c system Output You are now connected to database system. Grant permission to use a role Grant LIST on DBAROLE to DBAUSER: Input grant list on dbarole to DBAUSER ; Output GRANT This gives the DBAUSER permission to use the role DBAROLE which has ADMIN rights. Connect to the system database as DBAUSER: Input \\c system dbauser password Output You are now connected to database system as user dbauser. List the roles as DBUSER: Input show role ; Output ROLENAME | ROLEGRANTOR | ASADMIN ----------+-------------+--------- DBAROLE | ADMIN | t ( 1 row ) The LIST permission gives the DBAUSER the ability to show and set roles. As the DBUSER set the session to the role DBAROLE: Input set role dbarole ; Output SET ROLE All commands will run with ADMIN rights, thus, full control over the database system. Create a database with the DBAROLE set: Input create database testdb ; Output CREATE DATABASE List the database with the slash: option \\l: Input \\l Output List of databases DATABASE | OWNER ----------+------- LABDB | ADMIN SYSTEM | ADMIN TESTDB | ADMIN ( 3 rows ) Notice the owner of the TESTDB database is ADMIN. This is because the role DBAROLE (current session set to DBAROLE) was created with the AS ADMIN option. Drop the database TESTDB: Input drop database testdb ; Output DROP DATABASE Reset session from a role Return your nzsql session to the current user with the following command: set role none; Input set role none ; Output SET ROLE You are now back to using the permission of DBAUSER. Try creating the TESTDB database again and see what happens: Input create database testdb ; Output ERROR: CREATE DATABASE: permission denied. The command was run with the DBAUSER privileges and the DBAUSER doesn't have permissions to create a database. Create a second user Connect to the system database as ADMIN: Input \\c system Output You are now connected to database system. Create a second user called DBOPS: Input create user dbops with password 'password' ; Output CREATE USER Create a new role without permissions Create a new role called DBOPSROLE: Input create role dbopsrole with admin dbops ; Output CREATE ROLE Connect to the system database as the LABADMIN user: Input \\c system dbops password Output You are now connected to database system as user dbops. Show the role: Input show role ; Output ROLENAME | ROLEGRANTOR | ASADMIN -----------+-------------+--------- DBOPSROLE | DBOPS | f ( 1 row ) DBOPS can see the DBOPSROLE because the role was created with DBOPS user as the admin. DBOPS user is the GRANTOR for the role. Set the session role to DBOPSROLE as the DBOPS user: Input set role dbopsrole ; Output SET ROLE Create a database called TESTDB: Input create database testdb ; Output ERROR: CREATE DATABASE: permission denied. The CREATE DATABASE failed because the DBOPS user does not have any permissions on the database system other than the ADMIN of the role DBOPSROLE (i.e.: drop). After creating a role that is not defined with the AS ADMIN option you will need to GRANT the appropriate permissions similar to how you grant permissions to users and groups. You completed the section on Users, Groups and Roles. You should now understand how Users, Groups and Roles are all related and used. Congratulations you have completed the lab. You have successfully created the lab database, 2 tables, and database users, groups and roles with various privileges. You also ran a couple of simple queries.","title":"Database Administration"},{"location":"nz-04-Database-Admin/#database-administration","text":"A factory-configured and installed IBM Netezza Performance Server will include some of the following components: An IBM Netezza Performance Server warehouse appliance with pre-installed IBM Netezza Performance Server software A preconfigured Linux operating system (with IBM Netezza Performance Server modifications). Several preconfigured Linux users and groups: User Password Description nz nz Linux user with login permission to the IBM Netezza Performance Server host container. nz has ownership of various IPS files in the installation directory. root netezza Linux superuser with login permission to the IBM Netezza Performance Server host container. Has ownership of various kernel and operating system files. An IBM Netezza Performance Server database user named ADMIN is the database super-user, and has full access to all system functions and objects User Password Description admin password IPS database superuser, with full access to all database administration privileges and objects A preconfigured database group named PUBLIC. All database users are automatically placed in the group PUBLIC and therefore inherit all of its privileges The IBM Netezza Performance Server warehouse appliance includes a highly optimized SQL dialect called IBM Netezza Performance Server Structured Query Language (SQL). You can use SQL commands to create and manage your IBM Netezza Performance Server databases, user access, and permissions for the databases, as well as to query and modify the contents of the databases On a new IBM Netezza Performance Server system, there is typically one main database, SYSTEM, and a database template, MASTER_DB. IBM Netezza Performance Server uses the MASTER_DB as a template for all other user databases that are created on the system. Initially, only the ADMIN user can create new databases, but the ADMIN user can grant other users' permission to create databases as well. The ADMIN user can also make another user the owner of a database, which gives that user ADMIN-like control over that database and its contents. The database creator becomes the default owner of the database. The owner can remove the database and all its objects, even if other users own objects within the database. Within a database, permitted users can create tables and populate them with data and query its contents.","title":"Database Administration"},{"location":"nz-04-Database-Admin/#objectives","text":"This lab will guide you through the typical steps to create and manage new IBM Netezza Performance Server users and groups after an IBM Netezza Performance Server has been delivered and configured. This will include creating a new database and assigning the appropriate privileges. The users and the database that you create in this lab will be used as a basis for the remaining labs in this bootcamp. After this lab you will have a basic understanding on how to plan and create an IBM Netezza Performance Server database environment. The first part of this lab will examine creating IBM Netezza Performance Server users and groups The second part of this lab will explore creating and using a database and tables. The table schema to be used within this bootcamp will be explained in the Data Distribution lab.","title":"Objectives"},{"location":"nz-04-Database-Admin/#lab-setup","text":"Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/databaseAdministration/setupLab ./setupLab.sh Output drop database labdb ; DROP DATABASE drop user labadmin ; DROP USER drop user labuser ; nzsql:drop/drop_users:2: ERROR: DROP USER: object LABUSER does not exist. drop user dbuser ; nzsql:drop/drop_users:3: ERROR: DROP USER: object DBUSER does not exist. drop group lagrp ; nzsql:drop/drop_groups:1: ERROR: DROP GROUP: object LAGRP does not exist. drop group lugrp ; nzsql:drop/drop_groups:2: ERROR: DROP GROUP: object LUGRP does not exist. Errors are expected if this is the first time running this lab on this virtual machine. Connect to the system database as the IBM Netezza Performance Server database super-user, ADMIN, using the nzsql interface: Input nzsql or nzsql -d system -u admin -pw password There are different options you can use with the nzsql interface. Here we present two options, where the first option uses information set in the NZ environment variables, NZ_DATABASE, NZ_USER, and NZ_PASSWORD. You can add these the environment variables (set in .bashrc) as follows: export NZ_DATATASE=system export NZ_USER=admin export NZ_PASSWORD=password Now you do not need to specify the database name or the user. In the second option the information is explicitly stated using the -d, -u, and -pw options, which specifies the database name, the user, and the user's password, respectively. This option is useful when you want to connect to a different database or use a different user than specified in the NZ environment variables. Output Welcome to nzsql, the Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM ( ADMIN )= >","title":"Lab Setup"},{"location":"nz-04-Database-Admin/#creating-ibm-performance-server-users-and-groups","text":"The initial task after an IBM Netezza Performance Server has been set up is to create the database environment. This typically begins by creating a new set of database users and user groups before creating the database. You will use the ADMIN user to start creating additional database users and user groups. Then you will assign the appropriate authorities after the database has been created in the next section. The ADMIN user should only be used to perform administrative tasks within the IBM Netezza Performance Server and is not recommended for regular use. Also, it is highly advisable to develop a security access model to control user access against the database and the database objects in an IBM Netezza Performance Server. This will involve creating separate users to perform certain tasks. The security access model for this bootcamp environment will use three IBM Netezza Performance Server database users and two groups: Users: LABADMIN LABUSER DBUSER Groups: LAGRP LUGRP Connect to the Netezza image using PuTTy or another terminal program. Login to as user nz with password nz. (192.168.9.2)","title":"Creating IBM Performance server Users and Groups"},{"location":"nz-04-Database-Admin/#creating-new-ibm-netezza-performance-server-users-and-groups","text":"The three new IBM Netezza Performance Server database users will be initially created using the ADMIN user. The LABADMIN user will be the full owner of the bootcamp database. The LABUSER user will be allowed to perform data manipulation language (DML) operations (INSERT, UPDATE, DELETE) against all of the tables in the database, but they will not be allowed to create new objects like tables in the database. And lastly, the DBUSER user will only be allowed to read tables in the database, that is, they will only have LIST and SELECT privilege against tables in the database. The basic syntax to create a user is: CREATE USER username WITH PASSWORD 'string'; As the IBM Netezza Performance Server database super-user, ADMIN, you can now start to create the first user, LABADMIN, which will be the administrator of the database: (Note user and group names are not case sensitive) unless surrounded by quotes. Note: The default case is Upper case, meaning all objects created will be created as upper case unless quoted, in which case the case will be maintained. The IPS system could be reinit with Lower case as the default. Creating New IBM Netezza Performance Server Users: Input create user labadmin with password 'password' ; Output CREATE USER Later in this lab you will assign administrative ownership of the lab database to this user. Now you will create two additional IBM Netezza Performance Server database users that will have restricted access to the database. The first user, LABUSER, will have full DML access to the data in the tables, but will not be able to create or alter tables. For now, you will just create the user. We will set the privileges after the database is created: Input create user labuser with password 'password' ; Output CREATE USER Finally, we create the user DBUSER. This user will have even more limited access to the database since it will only be allowed to select data from the tables within the database. Again, you will set the privileges after the database is created: Input create user dbuser with password 'password' ; Output CREATE USER To list the existing database users in the environment, use the \\du internal slash option: Input \\d u Output List of Users USERNAME | VALIDUNTIL | ROWLIMIT | SESSIONTIMEOUT | QUERYTIMEOUT | DEF_PRIORITY | MAX_PRIORITY | USERESOURCEGRPID | USERESOURCEGRPNAME | CROSS_JOINS_ALLOWED ----------+------------+----------+----------------+-------------- +--------------+--------------+------------------+-------------------- +--------------------- ADMIN | | | 0 | 0 | NONE | NONE | | _ADMIN_ | NULL DBUSER | | 0 | 0 | 0 | NONE | NONE | 4901 | PUBLIC | NULL LABADMIN | | 0 | 0 | 0 | NONE | NONE | 4901 | PUBLIC | NULL LABUSER | | 0 | 0 | 0 | NONE | NONE | 4901 | PUBLIC | NULL ( 4 rows ) The other columns are explained in the WLM presentation.","title":"Creating New IBM Netezza Performance Server Users and Groups"},{"location":"nz-04-Database-Admin/#creating-new-ibm-netezza-performance-server-groups","text":"IBM Netezza Performance Server database user groups are useful for organizing and managing database users. By default, IBM Netezza Performance Server contains one group with the name PUBLIC. All users are members in the PUBLIC group when they are created. Users can be members of other groups as well. In this section we will create two new IBM Netezza Performance Server database user groups. They will be initially created by the ADMIN user. We will create an administrative group LAGRP which is short for Lab Admin Group. This group will contain the LABADMIN user. The second group we create will be the LUGRP or Lab User Group. This group will contain the users LABUSER and DBUSER. Two different methods will be used to add the existing users to the newly created groups. Alternatively, the groups could be created first and then the users. The basic command to create a group is: CREATE GROUP groupname; As the IBM Netezza Performance Server database super-user, ADMIN, you will now create the first group, LAGRP, which will be the administrative group for the LABADMIN user: Input create group lagrp ; Output CREATE GROUP After the LAGRP group is created you will now add the LABADMIN user to this group. This is accomplished by using the ALTER statement. You can either ALTER the user or the group, for this task you will ALTER the group to add the LABADMIN user to the LAGRP group: Input alter group lagrp with add user labadmin ; Output ALTER GROUP To ALTER the user, you would use the following command: alter user LABADMIN with in group LAGRP; Now you will create the second group, LUGRP, which will be the user group for both the LABUSER and DBUSER users. You can specify the users to be included in the group when creating the group: Input create group lugrp with add user labuser dbuser ; Output CREATE GROUP To list the existing IBM Netezza Performance Server groups in the environment, use the \\dg internal slash option: Input \\d g Output List of Groups GROUPNAME | ROWLIMIT | SESSIONTIMEOUT | QUERYTIMEOUT | DEF_PRIORITY | MAX_PRIORITY | GRORSGPERCENT | RSGMAXPERCENT | JOBMAX | CROSS_JOINS_ALLOWED -----------+----------+----------------+--------------+-------------- +--------------+---------------+---------------+--------+--------------------- LAGRP | 0 | 0 | 0 | NONE | NONE | 0 | 100 | 0 | NULL LUGRP | 0 | 0 | 0 | NONE | NONE | 0 | 100 | 0 | NULL PUBLIC | 0 | 0 | 0 | NONE | NONE | 20 | 100 | 0 | NULL ( 3 rows ) The default database group is PUBLIC. The other columns are explained in the WLM presentation. To list the users in a group you can use one of the two internal slash options, \\dG, or \\dU. The internal slash option \\dG will list the groups with the associated users: Input \\d G Output List of Users in a Group GROUPNAME | USERNAME -----------+---------- LAGRP | LABADMIN LUGRP | DBUSER LUGRP | LABUSER PUBLIC | DBUSER PUBLIC | LABADMIN PUBLIC | LABUSER ( 6 rows ) The internal slash option \\dU will list the users with the associated group: Input \\d U Output List of Groups a User is a member USERNAME | GROUPNAME ----------+----------- DBUSER | LUGRP DBUSER | PUBLIC LABADMIN | LAGRP LABADMIN | PUBLIC LABUSER | LUGRP LABUSER | PUBLIC ( 6 rows )","title":"Creating New IBM Netezza Performance Server Groups"},{"location":"nz-04-Database-Admin/#creating-an-ibm-netezza-performance-server-database","text":"The next step after the Netezza Performance Server database users and user groups have been created is to create the lab database. You will continue to use the ADMIN user to create the lab database then assign the appropriate authorities and privileges to the users created in the previous sections. The ADMIN user can also be used to create tables within the new database. However, the ADMIN user should only be used to perform administrative tasks. After the appropriate privileges have been assigned by the ADMIN user, the database can be handed over to the end-users to start creating and populating the tables in the database.","title":"Creating an IBM Netezza Performance Server Database"},{"location":"nz-04-Database-Admin/#creating-a-database-and-transferring-ownership","text":"The lab database that will be created will be named LABDB. It will be initially created by the ADMIN user and then ownership of the database will be transferred to the LABADMIN user. The LABADMIN user will have full administrative privileges against the LABDB database. The basic syntax to create a database is: CREATE DATABASE database_name; As the Netezza Performance Server database super-user, ADMIN, you will create the first database, LABDB, using the CREATE DATABASE command: Input create database labdb ; Output CREATE DATABASE The database LABDB has been created. To view the existing databases use the internal slash option \\l : Input \\l Output List of databases DATABASE | OWNER ----------+------- LABDB | ADMIN SYSTEM | ADMIN ( 2 rows ) The owner of the newly created LABDB database is the ADMIN user. The other databases are the default database SYSTEM and the template database MASTER_DB. At this point you could continue by creating new tables as the ADMIN user. However, the ADMIN user should only be used to create users, groups, and databases, and to assign authorities and privileges. Therefore, we will transfer ownership of the LABDB database from the ADMIN user to the LABADMIN user we created previously. The ALTER DATABASE command is used to transfer ownership of an existing database: Input alter database labdb owner to labadmin ; Output ALTER DATABASE This is the only method to transfer ownership of a database to an existing user. The CREATE DATABASE command does not include this option. Check that the owner of the LABDB database is now the LABADMIN user: Input \\l Output List of databases DATABASE | OWNER ----------+---------- LABDB | LABADMIN SYSTEM | ADMIN ( 2 rows ) The owner of the LABDB database is now the LABADMIN user. The LABDB database is now created and the LABADMIN user has full privileges on the LABDB database. The user can create and alter objects within the database. You could now continue and start creating tables as the LABADMIN user. However, we will first finish assigning privileges to the two remaining database users that were created in the previous section.","title":"Creating a Database and Transferring Ownership"},{"location":"nz-04-Database-Admin/#assigning-authorities-and-privileges","text":"One last task for the ADMIN user is to assign the privileges to the two users we created earlier, LABUSER and DBUSER. LABUSER user will have full DML rights against all tables in the LABDB database. It will not be allowed to create or alter tables within the database. User DBUSER will have more restricted access in the database and will only be allowed to read data from the tables in the database. The privileges will be controlled by a combination of setting the privileges at the group and user level. The LUGRP user group will be granted LIST and SELECT privileges against the database and tables within the database. So any member of the LUGRP will have these privileges. The full data manipulation privileges will be granted individually to the LABUSER user. The GRANT command that is used to assign object privileges has the following syntax: GRANT object_privilege [, ...] ON object [, ...] TO { PUBLIC | GROUP group | username } [ WITH GRANT OPTION ] GRANT admin_privilege [, ...] [ IN { [database_name.]schema_name | database_name.ALL | ALL.ALL } ] TO { PUBLIC | GROUP group | username } [ WITH GRANT OPTION ] object_privilege ALL, ABORT, ALTER, DELETE, DROP, EXECUTE, EXECUTE AS, GENSTATS, GROOM, INSERT, LABEL ACCESS, LABEL RESTRICT, LABEL EXPAND, LIST, SELECT, TRUNCATE, UPDATE admin_privilege ALL ADMIN, BACKUP, [ CREATE ] DATABASE, [ CREATE ] GROUP, [ CREATE ] INDEX, [ CREATE ] SCHEMA, [ CREATE ] SEQUENCE, [ CREATE ] SYNONYM, [ CREATE ] TABLE, [ CREATE ] EXTERNAL TABLE, [ CREATE ] TEMP TABLE, [ CREATE ] FUNCTION, [ CREATE ] AGGREGATE, [ CREATE ] USER, [ CREATE ] VIEW, [ CREATE ] MATERIALIZED VIEW, [CREATE] PROCEDURE, [ CREATE ] LIBRARY, [ CREATE ] SCHEDULER RULE, [ MANAGE ] HARDWARE, [ MANAGE ] SYSTEM, [ MANAGE ] SECURITY, RESTORE, UNFENCE, VACUUM As the Netezza Performance Server database super-user, ADMIN, connect to the LABDB database using the internal slash option \\c Input \\c labdb admin password Output You are now connected to database labdb as user admin. You will notice that the database name in command prompt has changed from SYSTEM to LABDB. First you will grant LIST privilege on the LABDB database to the group LUGRP. This will allow members of the LUGRP to view and connect to the LABDB database : Input grant list on labdb to lugrp ; Output GRANT To list the object permissions for a group use the following internal slash option, \\dpg : Input \\d pg lugrp Output Group object permissions for group 'LUGRP' Database Name | Schema Name | Object Name | L S I U D T L A D B L G O E C R X A | D G U S T E X Q Y V M I B R C S H F A L P N S R ---------------+-------------+-------------+-------------------------------------+------------------------------------------------- GLOBAL | GLOBAL | LABDB | X | ( 1 rows ) Object Privileges ( L ) ist ( S ) elect ( I ) nsert ( U ) pdate ( D ) elete ( T ) runcate ( L ) ock ( A ) lter ( D ) rop a ( B ) ort ( L ) oad ( G ) enstats Gr ( O ) om ( E ) xecute Label-A ( C ) cess Label- ( R ) estrict Label-E ( X ) pand Execute- ( A ) s Administration Privilege ( D ) atabase ( G ) roup ( U ) ser ( S ) chema ( T ) able T ( E ) mp E ( X ) ternal Se ( Q ) uence S ( Y ) nonym ( V ) iew ( M ) aterialized View ( I ) ndex ( B ) ackup ( R ) estore va ( C ) uum ( S ) ystem ( H ) ardware ( F ) unction ( A ) ggregate ( L ) ibrary ( P ) rocedure U ( N ) fence ( S ) ecurity Scheduler ( R ) ule The X in the L column of the list denotes that the LUGRP group has LIST object privileges on the LABDB global object. With the current privileges set for the LABUSER and DBUSER, they can now view and connect to the LABDB database as members of the LUGRP group. But these two users have no privileges to access any of the objects within the database. So you will grant LIST and SELECT privilege to the tables within the LABDB database to the members of the LUGRP : Input grant list, select on table to lugrp ; Output GRANT View the object permissions for the LUGRP group : Input \\d pg lugrp Output Group object permissions for group 'LUGRP' Database Name | Schema Name | Object Name | L S I U D T L A D B L G O E C R X A | D G U S T E X Q Y V M I B R C S H F A L P N S R ---------------+-------------+-------------+-------------------------------------+------------------------------------------------- GLOBAL | GLOBAL | LABDB | X | LABDB | ADMIN | TABLE | X X | ( 2 rows ) Object Privileges ( L ) ist ( S ) elect ( I ) nsert ( U ) pdate ( D ) elete ( T ) runcate ( L ) ock ( A ) lter ( D ) rop a ( B ) ort ( L ) oad ( G ) enstats Gr ( O ) om ( E ) xecute Label-A ( C ) cess Label- ( R ) estrict Label-E ( X ) pand Execute- ( A ) s Administration Privilege ( D ) atabase ( G ) roup ( U ) ser ( S ) chema ( T ) able T ( E ) mp E ( X ) ternal Se ( Q ) uence S ( Y ) nonym ( V ) iew ( M ) aterialized View ( I ) ndex ( B ) ackup ( R ) estore va ( C ) uum ( S ) ystem ( H ) ardware ( F ) unction ( A ) ggregate ( L ) ibrary ( P ) rocedure U ( N ) fence ( S ) ecurity Scheduler ( R ) ule The X in the L and S column denotes that the LUGRP group has both LIST and SELECT privileges on all of the tables in the LABDB database. (The LIST privilege is used to allow users to view the tables using the internal slash opton \\d.) The current privileges satisfy the DBUSER user requirements, which is to allow access to the LABDB database and SELECT access to all the tables in the database. But these privileges do not satisfy the requirements for the LABUSER user. The LABUSER user is to have full DML access to all the tables in the database. So you will grant SELECT, INSERT, UPDATE, DELETE, LIST, and TRUNCATE privileges on tables in the LABDB database to the LABUSER user: Input grant select , insert, update, delete, list, truncate on table to labuser ; Output GRANT To list the object permissions for a user use the \\dpu internal slash option,: Input \\d pu labuser Output bash User object permissions for user 'LABUSER' Database Name | Schema Name | Object Name | L S I U D T L A D B L G O E C R X A | D G U S T E X Q Y V M I B R C S H F A L P N S R ---------------+-------------+-------------+-------------------------------------+------------------------------------------------- LABDB | ADMIN | TABLE | X X X X X X | (1 rows) Object Privileges (L)ist (S)elect (I)nsert (U)pdate (D)elete (T)runcate (L)ock (A)lter (D)rop a(B)ort (L)oad (G)enstats Gr(O)om (E)xecute Label-A(C)cess Label-(R)estrict Label-E(X)pand Execute-(A)s Administration Privilege (D)atabase (G)roup (U)ser (S)chema (T)able T(E)mp E(X)ternal Se(Q)uence S(Y)nonym (V)iew (M)aterialized View (I)ndex (B)ackup (R)estore va(C)uum (S)ystem (H)ardware (F)unction (A)ggregate (L)ibrary (P)rocedure U(N)fence (S)ecurity Scheduler (R)ule The X under the L, S, I, U, D, T columns indicates that the LABUSER user has LIST, SELECT, INSERT, UPDATE, DELETE, and TRUNCATE privileges on all of the tables in the LABDB database. Now that all of the privileges have been set by the ADMIN user the LABDB database can be handed over to the end-users. The end-users can use the LABADMIN user to create objects, which include tables, in the database and also maintain the database.","title":"Assigning Authorities and Privileges"},{"location":"nz-04-Database-Admin/#creating-tables","text":"The LABADMIN user will be used to create tables in the LABDB database instead of the ADMIN user. Two tables will be created in this lab. The remaining tables for the LABDB database schema will be created in the Data Distribution lab. Data Distribution is an important aspect that should be considered when creating tables. This concept is not covered in this lab since it is discussed separately in the Data Distribution presentation. The two tables that will be created are the REGION and NATION tables. These two tables will be populated with data in the next section using LABUSER user. Two methods will be utilized to create these tables. The basic syntax to create a table is: General syntax: CREATE [ TEMPORARY | TEMP ] TABLE [IF NOT EXISTS] <table> ( <col> <type> [<col_constraint>][,<col> <type> [<col_constraint>]...] [<table_constraint>[,<table_constraint>... ] ) [ DISTRIBUTE ON { RANDOM | [HASH] (<col>[,<col>...]) } ] [ ORGANIZE ON { (<col>) | NONE } ] [ ROW SECURITY ] Where represents: [ CONSTRAINT <constraint_name> ] {NOT NULL | NULL | UNIQUE | PRIMARY KEY | DEFAULT <value> | <ref>} [ [ [ NOT ] DEFERRABLE ] { INITIALLY DEFERRED | INITIALLY IMMEDIATE } | [ INITIALLY DEFERRED | INITIALLY IMMEDIATE ] [ NOT ] DEFERRABLE ] Where represents: [ CONSTRAINT <constraint_name> ] {UNIQUE (<col>[,<col>...] ) | PRIMARY KEY (<pkcol_name>[,<pkcol_name>...] ) | FOREIGN KEY (<fkcol_name>[,<fkcol_name>...] ) <ref>} [ [ [ NOT ] DEFERRABLE ] { INITIALLY DEFERRED | INITIALLY IMMEDIATE } | [ INITIALLY DEFERRED | INITIALLY IMMEDIATE ] [ NOT ] DEFERRABLE ] Where represents: REFERENCES <reftable> [ (<refcol_name>[,<refcol_name>...] ) ] [ MATCH FULL ] [ ON UPDATE {CASCADE | RESTRICT | SET NULL | SET DEFAULT | NO ACTION} ] [ ON DELETE {CASCADE | RESTRICT | SET NULL | SET DEFAULT | NO ACTION} ] Connect to the LABDB database as the LABADMIN user using the internal slash option \\c: Input \\c labdb labadmin password Output You are now connected to database labdb as user labadmin. You will notice that the username in the command prompt has changed from ADMIN to LABADMIN. Since you already had an opened session, you could use the internal slash option \\c to connect to the database. However, if you had handed over this environment to the end user they would need to initiate a new connection using the nzsql interface. To use the nzsql interface to connect to the LABDB database as the LABADMIN user you could use the following options (no need to run these command, informational): nzsql -d labdb -u labadmin -pw password or with the short form, omitting the options: nzsql labdb labadmin password or you could set the environment variables to the following values and issue nzsql without options. export NZ_DATABASE=LABDB export NZ_USER=LABADMIN export NZ_PASSWORD=password In further labs we will often leave out the password parameter since it has been set to the same value \"password\" for all users. Now you can create the first table in the LABDB database. The first table you will create is the REGION table with the following columns and datatypes : Column Name Data Type R_REGIONKEY INTEGER R_NAME CHAR(25) R_COMMENT VARCHAR(152) To create the above table execute the following command: Input create table region ( r_regionkey integer, r_name char ( 25 ) , r_comment varchar ( 152 ) ) ; Output CREATE TABLE To list the tables in the LABDB database use the \\dt internal slash option: Input \\d t Output List of relations Schema | Name | Type | Owner --------+--------+-------+---------- ADMIN | REGION | TABLE | LABADMIN ( 1 row ) To describe a table you can use the internal slash option \\d <table name> : Input \\d region Output Table \"REGION\" Attribute | Type | Modifier | Default Value -------------+------------------------+----------+--------------- R_REGIONKEY | INTEGER | | R_NAME | CHARACTER ( 25 ) | | R_COMMENT | CHARACTER VARYING ( 152 ) | | Distributed on hash: \"R_REGIONKEY\" The Distributed on hash clause is the distribution method used by the table. If you do not explicitly specify a distribution method, a default distribution is used. In our system this is hash distributed on the first column R_REGIONKEY. This concept is discussed in the Data Distribution presentation and lab. Note: the default distribution can be change to RANDOM. This might be a preferred default distribution when the Netezza environment is supporting ad-hoc user table creation. In the event a user forgets to add a distribution key, RANDOM would at a minimum guarantee even distribution. However, you table if joined to other tables would require data movement to perform the join. Instead of typing out the entire create table statement at the nzsql command line you can read and execute commands from a file. You'll use this method to create the NATION table in the LABDB database with the following columns and data types: Column Name Data Type Constraint N_NATIONKEY INTEGER NOT NULL N_NAME CHAR(25) NOT NULL N_REGIONKEY INTEGER NOT NULL N_COMMENT VARCHAR(152) --- The full create table statement for the NATION table: create table nation ( n_nationkey integer not null, n_name char(25) not null, n_regionkey integer not null, n_comment varchar(152) ) distribute on random; The statement can be found in the nation.ddl file under the /home/nz/labs/databaseAdministration directory. To read and execute commands from a file use the \\i < file > internal slash option: Input \\i /home/nz/labs/databaseAdministration/nation.ddl Output CREATE TABLE You can run Linux commands while in the nzsql console. For example: list the nation.ddl file Input ! ls -l /home/nz/labs/databaseAdministration/nation.ddl Output -rwxr-xr-x. 1 nz nz 176 May 13 2020 /home/nz/labs/databaseAdministration/nation.ddl List all the tables in the LABDB database: Input \\d t Output List of relations Schema | Name | Type | Owner --------+--------+-------+---------- ADMIN | NATION | TABLE | LABADMIN ADMIN | REGION | TABLE | LABADMIN ( 2 rows ) Describe the NATION table : Input \\d nation Output Table \"NATION\" Attribute | Type | Modifier | Default Value -------------+------------------------+----------+--------------- N_NATIONKEY | INTEGER | NOT NULL | N_NAME | CHARACTER ( 25 ) | NOT NULL | N_REGIONKEY | INTEGER | NOT NULL | N_COMMENT | CHARACTER VARYING ( 152 ) | | Distributed on random: ( round-robin ) The distributed on random is the distribution method used, in this case the rows in the NATION table are distributed in round-robin fashion. This concept will be discussed separately in the Data Distribution presentation and lab. It is possible to continue to use LABADMIN user to perform DML queries since it is the owner of the database and holds all privileges on all of the objects in the databases. However, the LABUSER and DBUSER users will be used to perform DML queries against the tables in the database.","title":"Creating Tables"},{"location":"nz-04-Database-Admin/#using-dml-queries","text":"We will now use the LABUSER user to populate data into both the REGION and NATION tables. This user has full data manipulation language (DML) privileges in the database, but no data definition language (DDL) privileges. Only the LABADMIN has full DDL privileges in the database. Later in this course more efficient methods to populate tables with data are discussed. The DBUSER will also be used to read data from the tables, but it can not insert data into the tables since it has limited DML privileges in the database. Connect to the LABDB database as the LABUSER user using the internal slash option, \\c: Input \\c labdb labuser password Output You are now connected to database LABDB as user labuser. LABDB.ADMIN ( LABUSER )= > You will notice that the user name in the command prompt has changed from LABADMIN to LABUSER. First check which tables exist in the LABDB database using the \\dt internal slash option: Input \\d t Output List of relations Schema | Name | Type | Owner --------+--------+-------+---------- ADMIN | NATION | TABLE | LABADMIN ADMIN | REGION | TABLE | LABADMIN ( 2 rows ) Remember that the LABUSER user is a member of the LUGRP group which was granted LIST privileges on the tables in the LABDB database. This is the reason why it can list and view the tables in the LABDB database. If it did not have this privilege, it would not be able to see any of the tables in the LABDB database. The LABUSER user was created to perform DML operations against the tables in the LABDB database. However, it was restricted on performing DDL operations against the database. Let's see what happens when you try create a new table, t1, with one column, C1, using the INTEGER data type: Input create table t1 ( c1 integer ) ; Output ERROR: CREATE TABLE: permission denied. As expected, the create table statement is not allowed since LABUSER user does not have the privilege to create tables in the LABDB database. Let's continue by performing DML operations that the LABUSER user is allowed to perform against the tables in the LABDB database. Insert a new row into the REGION table: Input ```bash insert into region values (1, 'NA', 'north america'); Output INSERT 0 1 As expected, this operation is successful. The output of the INSERT gives feedback about the number of successfully inserted rows. Issue the SELECT statement against the REGION table to check the new row you just added to the table: Input select * from region ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+--------------- 1 | NA | north america ( 1 row ) Instead of typing DML statements at the nzsql command line, you can read and execute statements from a file. You will use this method to add the following three rows to the REGION table: R_REGIONKEY R_NAME R_COMMENT 2 SA South America 3 EMEA Europe, Middle East, Africa 4 AP Asia Pacific This is done with a SQL script containing the following commands: insert into region values (2, 'sa', 'south america'); insert into region values (3, 'emea', 'europe, middle east, africa'); insert into region values (4, 'ap', 'asia pacific'); It can be found in the region.dml file under the /home/nz/labs/databaseAdministration directory. To read and execute commands from a file use the \\i < file > internal slash option: Input \\i /home/nz/labs/databaseAdministration/region.dml Output INSERT 0 1 INSERT 0 1 INSERT 0 1 You can see from the output that the SQL script contained three INSERT statements. You will load data into the NATION table using an external table with the following command: Input insert into nation select * from external '/home/nz/labs/databaseAdministration/nation.del' ; Output INSERT 0 14 You loaded 14 rows into the table. Loading data into a table is covered in the Loading and Unloading Data presentation and lab. Now you will switch over to the DBUSER user, who only has SELECT privilege on the tables in the LABDB database. This privilege is granted to this user since he is a member of the LUGRP group. Use the internal slash option, \\c to connect to the LABDB database as the DBUSER user: Input \\c labdb dbuser password Output You are now connected to database labdb as user dbuser. You will notice that the username in the command prompt changes from LABUSER to DBUSER. Before trying to view rows from tables in the LABDB database, try to add a new row to the REGION table: Input insert into region values ( 5 , 'NP' , 'north pole' ) ; Output ERROR: Permission denied on \"REGION\" . As expected, the INSERT statement is not allowed since the DBUSER does not have the privilege to add rows to any tables in the LABDB database. Now select all rows from the REGION table: Input select * from region ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | NA | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific ( 4 rows ) Finally let's run a slightly more complex query. We want to return all nation names in Asia Pacific, together with their region name. To do this you need to execute a simple join using the NATION and REGION tables. The join key will be the region key, and to restrict results on the AP region you need to add a WHERE condition: Input select n_name, r_name from nation, region where n_regionkey = r_regionkey and r_name = 'ap' ; Output N_NAME | R_NAME ---------------------------+--------------------------- australia | ap japan | ap macau | ap hong kong | ap new zealand | ap ( 5 rows ) This should return the following results, containing all countries from the ap region.","title":"Using DML Queries"},{"location":"nz-04-Database-Admin/#roles","text":"IBM Netezza Performance Server introduced a new object called role. A role is a potential grantee or grantor of privileges and of other roles. Similar to a user, a role can own schemas and other objects. Roles can own database objects such as tables or functions. By using roles, you can also assign privileges. You can create roles that are available across all databases on the IBM Netezza Performance Server system. Similar to group objects, roles are also [global objects] . A user or group can be grant permission to a role. A session can then be set to run under a ROLE's authorization. Once a session is set to a ROLE the permissions used are from that ROLE's authorization and/or privileges, not a user or group permissions. This allows a user/application to isolate a database session to execute with a ROLE's permission. In this section of the lab we will create roles to demonstrate how roles can be used.","title":"Roles"},{"location":"nz-04-Database-Admin/#creating-role","text":"Roles are create using the following syntax: CREATE ROLE role name [ WITH ADMIN owner name ] [ AS ADMIN ] 1. Login to the NPS command line as the nz user. See the section: Login to the NPS Command Line for details on how to access the command line for your given lab system. As the NPS database super-user, ADMIN, use nzsql to create the first role, LAROLE, which will be the administrator of the system: (Note: roles are not case sensitive) Input create role DBAROLE AS ADMIN ; Output CREATE ROLE This role will act as the super-user ADMIN and have all permissions on the database system. This is a simple way to give ADMIN rights to a user through the use of a role.","title":"Creating role"},{"location":"nz-04-Database-Admin/#display-roles","text":"Display the newly created role with the following command: Input show role ; Output ROLENAME | ROLEGRANTOR | ASADMIN ----------+-------------+--------- DBAROLE | ADMIN | t ( 1 row )","title":"Display roles"},{"location":"nz-04-Database-Admin/#create-a-new-user-without-any-permissions","text":"Create a new DBA user without any permissions. Input create user DBAUSER password 'password' ; Output CREATE USER Connect to the system database as DBAUSER and CREATE a database: Input \\c system dbauser password Output You are now connected to database system as user dbauser. Input create database testdb ; Output ERROR: CREATE DATABASE: permission denied. Notice that DBUSER cannot create a database as is expected since we didn't grant DATABASE permission to the DBUSER. Connect to the system database as ADMIN: Input \\c system Output You are now connected to database system.","title":"Create a new user without any permissions"},{"location":"nz-04-Database-Admin/#grant-permission-to-use-a-role","text":"Grant LIST on DBAROLE to DBAUSER: Input grant list on dbarole to DBAUSER ; Output GRANT This gives the DBAUSER permission to use the role DBAROLE which has ADMIN rights. Connect to the system database as DBAUSER: Input \\c system dbauser password Output You are now connected to database system as user dbauser. List the roles as DBUSER: Input show role ; Output ROLENAME | ROLEGRANTOR | ASADMIN ----------+-------------+--------- DBAROLE | ADMIN | t ( 1 row ) The LIST permission gives the DBAUSER the ability to show and set roles. As the DBUSER set the session to the role DBAROLE: Input set role dbarole ; Output SET ROLE All commands will run with ADMIN rights, thus, full control over the database system. Create a database with the DBAROLE set: Input create database testdb ; Output CREATE DATABASE List the database with the slash: option \\l: Input \\l Output List of databases DATABASE | OWNER ----------+------- LABDB | ADMIN SYSTEM | ADMIN TESTDB | ADMIN ( 3 rows ) Notice the owner of the TESTDB database is ADMIN. This is because the role DBAROLE (current session set to DBAROLE) was created with the AS ADMIN option. Drop the database TESTDB: Input drop database testdb ; Output DROP DATABASE","title":"Grant permission to use a role"},{"location":"nz-04-Database-Admin/#reset-session-from-a-role","text":"Return your nzsql session to the current user with the following command: set role none; Input set role none ; Output SET ROLE You are now back to using the permission of DBAUSER. Try creating the TESTDB database again and see what happens: Input create database testdb ; Output ERROR: CREATE DATABASE: permission denied. The command was run with the DBAUSER privileges and the DBAUSER doesn't have permissions to create a database.","title":"Reset session from a role"},{"location":"nz-04-Database-Admin/#create-a-second-user","text":"Connect to the system database as ADMIN: Input \\c system Output You are now connected to database system. Create a second user called DBOPS: Input create user dbops with password 'password' ; Output CREATE USER","title":"Create a second user"},{"location":"nz-04-Database-Admin/#create-a-new-role-without-permissions","text":"Create a new role called DBOPSROLE: Input create role dbopsrole with admin dbops ; Output CREATE ROLE Connect to the system database as the LABADMIN user: Input \\c system dbops password Output You are now connected to database system as user dbops. Show the role: Input show role ; Output ROLENAME | ROLEGRANTOR | ASADMIN -----------+-------------+--------- DBOPSROLE | DBOPS | f ( 1 row ) DBOPS can see the DBOPSROLE because the role was created with DBOPS user as the admin. DBOPS user is the GRANTOR for the role. Set the session role to DBOPSROLE as the DBOPS user: Input set role dbopsrole ; Output SET ROLE Create a database called TESTDB: Input create database testdb ; Output ERROR: CREATE DATABASE: permission denied. The CREATE DATABASE failed because the DBOPS user does not have any permissions on the database system other than the ADMIN of the role DBOPSROLE (i.e.: drop). After creating a role that is not defined with the AS ADMIN option you will need to GRANT the appropriate permissions similar to how you grant permissions to users and groups. You completed the section on Users, Groups and Roles. You should now understand how Users, Groups and Roles are all related and used. Congratulations you have completed the lab. You have successfully created the lab database, 2 tables, and database users, groups and roles with various privileges. You also ran a couple of simple queries.","title":"Create a new role without permissions"},{"location":"nz-05-Loading-and-Unloading-Data/","text":"1 Load and Unloading Data In every data warehouse environment, there is a need to load new data into the database. The task to load data into the database is not just a one-time operation but rather a continuous operation that can occur hourly, daily, weekly, or even monthly. Loading data into the database is a vital operation that needs to be supported by the data warehouse system. Netezza Performance Server (NPS) provides a framework to support not only the loading of data into the Netezza Performance Server database environment, but also the unloading of data from the database environment. This framework contains more than one component, some of these components are: External Tables -- These are tables stored as flat files on the host or client systems and registered like tables in the Netezza Performance Server catalog. They can be used to load data into the Netezza Performance Server or unload data to the file system. nzload -- This is a wrapper command line tool around external tables that provides an easy method loading data into the Netezza Performance Server. Format Options -- These are options for formatting the data load to and from external tables. 1.1 Objectives This lab will help you explore the Netezza Performance Server framework components for loading data into the database and unloading data from the database. You will use the various commands to create external tables to unload and load data. You will also get a basic understanding of the nzload utility. In this lab the REGION and NATION tables in the LABDB database are used to illustrate the use of external tables and the nzload utility. After this lab you will have a good understanding on how to load and unload data from a Netezza Performance Server database environment The first part of this lab will explore using External Tables to unload and load data. The second part of this lab will discuss using the nzload utility to load records into tables. 2 Lab Environment The lab system will be a virtual machine running on Virtual Box. Please see the document on how to install the NPS Virtual Machine for your workstation (Windows or Mac OS). 3 Connect to the Netezza Performance Server Use the following information to connect to the virtual NPS system. There are two options to access the command line: Login to the VM directly and use the terminal application available inside the VM. Use the local terminal application on your workstation. the lab will use the command line as the nz user. 4 Lab Setup This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using a terminal application (Windows PowerShell, PuTTY, Mac OSX Terminal) If you are continuing from the previous lab and are already connected to nzsql quit the nzsql console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/movingData/setupLab ./setupLab.sh Output DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully The error message at the beginning is expected since the script tries to clean up existing LINEITEM tables. 5 External Tables An external table allows Netezza Performance Server to treat an external file as a database table. An external table has a definition, a table schema, in the Netezza Performance Server system catalog, but the actual data exists outside of the Netezza Performance Server database. This is referred to as a data source file. External tables can be used to access files which are stored on a file system. After you have created the external table definition, you can use INSERT INTO statements to load data from the external file into a database table or SELECT FROM statements to query the external table. Different methods are described to create and use external tables using the nzsql interface. The external data source files for the external tables will also be examined, so a second session will be used to view these files. Connect to your Netezza Performance Server image using a Terminal application to ssh into <your-nps-vm-ip-address> (as user nz with password nz ). Alternatively, you can use a terminal application on the virtual machine desktop. <your-nps-vm-ip-address> is the default IP address for a local VM, the IP may be different for your session. Change to the lab working directory /home/nz/labs/movingData using the following command: Input cd /home/nz/labs/movingData Connect to the LABDB database as the database owner, LABADMIN, using the nzsql interface: Input nzsql -d LABDB -u labadmin -pw password Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit In this lab we will need to alternatively execute SQL commands and operating system commands. To make this task easier for you, we will open a second Terminal session for executing operating system commands like nzload, view generated external files etc. It will be referred to as session 2 throughout the lab. The picture above shows the two Terminal windows that you will need. Terminal 1, on the left, will be used for SQL commands and Terminal 2, on the right, will be used for operating system prompt commands. Open another session [Terminal 2] using PuTTY or a Terminal Application. Login to <your-nps-vm-ip-address> as user nz with password nz. Change to the /home/nz/labs/movingData directory: Input [Terminal 2] cd /home/nz/labs/movingData 5.1 Unloading Data using External Tables External tables will be used to unload rows from the LABDB database as records into an external datasource file. Various methods to create and use external tables will be explored unloading rows from either REGION or NATION tables. Five different basic use cases are presented for you to follow so you can gain a better understanding of how to use external tables to unload data from a database. 5.1.1 Unloading data with an External Table created with the SAMEAS clause The first external table will be used to unload data from the REGION table into an ASCII delimited text file. This external table will be named ET1_REGION using the same column definition as the REGION table. After the ET1_REGION external table is created you will then use it to unload all the rows from the REGION table. The records for the ET1_REGION external table will be in the external datasource file, et1_region_flat_file. The basic syntax to create this type of external table is: Sample Syntax CREATE EXTERNAL TABLE table_name SAMEAS table_name USING external_table_options The SAMEAS clause allows the external table to be created with the same column definition of the referenced table. This is referred to as implicit schema definition. As the LABDB database owner, LABADMIN, you will create the first basic external table using the same column definitions as the REGION table: Input [Terminal 1] create external table et1_region sameas region using (dataobject ('/home/nz/labs/movingData/et1_region_flat_file')); Output CREATE EXTERNAL TABLE Use the internal slash option \\dx to list the external tables in the LABDB database. Input [Terminal 1] \\dx Output List of relations Schema | Name | Type | Owner --------+------------+----------------+---------- ADMIN | ET1_REGION | EXTERNAL TABLE | LABADMIN (1 row) List the properties of the external table et1_region using the following internal slash option to describe the table, \\d <external table name> . Input [Terminal 1] \\d et1_region Output External Table \"ET1_REGION\" Attribute | Type | Modifier -------------+------------------------+---------- R_REGIONKEY | INTEGER | NOT NULL R_NAME | CHARACTER(25) | NOT NULL R_COMMENT | CHARACTER VARYING(152) | DataObject - '/home/nz/labs/movingData/et1_region_flat_file' adjustdistzeroint - bool style - 1_0 code set - compress - FALSE cr in string - ctrl chars - date delim - - date style - YMD delim - | encoding - INTERNAL escape - fill record - format - TEXT ignore zero - log dir - /tmp max errors - 1 max rows - null value - NULL quoted value - NO remote source - require quotes - skip rows - socket buf size - 8388608 timedelim - : time round nanos - time style - 24HOUR trunc string - y2base - includezeroseconds - record length - record delimiter - nullindicator bytes - layout - decimaldelim - disablenfc - includeheader - datetime delim - meridian delim - lfinstring - This output includes the columns and associated data types in the external table. Notice that this is similar to the REGION table since the external table was created using the SAMEAS clause in the CREATE EXTERNAL TABLE command. The output also includes the properties of the external table. The most notable property is the DataObject property that shows the location and the name of the external datasource file used for the external table. We will examine some of the others. Now that the external table is created, use it to unload data from the REGION table using an INSERT statement. Input [Terminal 1] insert into et1_region select * from region; Output INSERT 0 4 Use the external table like a regular table by issuing SQL statements. Try issuing a simple SELECT FROM statement to return all rows in external table ET1_REGION : Input [Terminal 1] select * from et1_region order by 1; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific (4 rows) You will notice that this is the same data that is in the REGION table. But the data retrieved for this SELECT statement was from the datasource of this external table and not from the data within the database. The main reason for creating an external table is to unload data from a table to a file. Using the second Putty (or Terminal) session, review the file et1_region_flat_file. This is the file that was created in the /home/nz/labs/movingData directory. Input [Terminal 2] more et1_region_flat_file Output 3|emea|europe, middle east, africa 1|na|north america 2|sa|south america 4|ap|asia pacific This is an ASCII delimited flat file containing the data from the REGION table. The column delimiter used in this file was the default character '|'. 5.1.2 Unloading data with an External Table using the AS SELECT clause The second external table will be used to unload data from the REGION table into an ASCII delimited text file using a different method. The external table will be created and the data will be unloaded in the same create statement. A separate step is not required to unload the data. The external table will be named ET2_REGION and the external datasource file will be named et2_region_flat_file. The basic syntax to create this type of external table is: Sample Syntax CREATE EXTERNAL TABLE table_name 'filename' AS select_statement; The AS clause allows the external table to be created with the same columns returned in the SELECT FROM statement, which is referred to as implicit table schema definition. This also unloads the rows at the same time the external table is created. The first method used to create an external table required the data to be unloaded in a second step using an INSERT statement. Using the first Putty (or Terminal) session, create an external table and unload the data in a single step. Input [Terminal 1] create external table et2_region '/home/nz/labs/movingData/et2_region_flat_file' as select * from region; Output INSERT 0 4 This command created the external table ET2_REGION using the same definition as the REGION table and also unloaded the data to the et2_region_flat_file. Again, use /dx to list the external tables in the LABDB database. Input [Terminal 1] \\dx Output List of relations Schema | Name | Type | Owner --------+------------+----------------+---------- ADMIN | ET1_REGION | EXTERNAL TABLE | LABADMIN ADMIN | ET2_REGION | EXTERNAL TABLE | LABADMIN (2 rows) You will notice that there are now two external tables. You can also list the properties of the external table. The output will be similar to the output in the last section, except for the filename. Using the second session, review the file that was created, et2_region_flat_file, in the /home/nz/labs/movingData directory. Input [Terminal 2] more et2_region_flat_file Output 3|emea|europe, middle east, africa 1|na|north america 2|sa|south america 4|ap|asia pacific This file is exactly the same as the file you reviewed in the last chapter. The only difference in this example is we didn't need to unload it explicitly. 5.1.3 Unloading data with an external table using defined columns The first two external tables that you created used the exact same columns from the REGION table using an implicit table schema. You can also create an external table by explicitly specifying the columns. This is referred to as an explicit table schema. The third external table that you create will still be used to unload data from the REGION table but only from the R_NAME and R_COMMENT columns. The ET3_REGION external table will be created in one step and then the data will be unloaded into the et3_region_flat_file ASCII delimited text file using a different delimiter string. The basic syntax to create this type of external table is: Sample Syntax CREATE EXTERNAL TABLE table_name ({column_name type} [, ... ]) [USING external_table_options}] Create a new external table to only include the R_NAME and R_COMMENT columns and exclude the R_REGIONKEY column from the REGION table. Also change the delimiter string from the default '|' to '=': Input [Terminal 1] create external table et3_region (r_name char(25), r_comment varchar(152)) USING (dataobject ('/home/nz/labs/movingData/et3_region_flat_file') DELIMITER '='); Output CREATE EXTERNAL TABLE List the properties of the ET3_REGION external table: Input [Terminal 1] \\d et3_region Output External Table \"ET3_REGION\" Attribute | Type | Modifier -----------+------------------------+---------- R_NAME | CHARACTER(25) | R_COMMENT | CHARACTER VARYING(152) | DataObject - '/home/nz/labs/movingData/et3_region_flat_file' adjustdistzeroint - bool style - 1_0 code set - compress - FALSE cr in string - ctrl chars - date delim - - date style - YMD delim - = encoding - INTERNAL escape - fill record - format - TEXT ignore zero - log dir - /tmp max errors - 1 max rows - null value - NULL quoted value - NO remote source - require quotes - skip rows - socket buf size - 8388608 timedelim - : time round nanos - time style - 24HOUR trunc string - y2base - includezeroseconds - record length - record delimiter - nullindicator bytes - layout - decimaldelim - disablenfc - includeheader - datetime delim - meridian delim - lfinstring - Notice that there are only two columns for this external table since only two columns were specified when creating the external table. The rest of the output is very similar to the properties of the other two external tables that were created, with two main exceptions. The first difference is the Dataobjects field, since the filename is different. The other difference is the string used for the delimiter, since it is now '=' instead of the default, '|'. Unload the data from the REGION table but only the data from columns R_NAME and R_COMMENT. Input [Terminal 1] insert into et3_region select r_name, r_comment from region; Output INSERT 0 4 Alternatively, you could have created the external table and unloaded the data in one step using the following command: Sample Syntax create external table et4_test '/home/nz/labs/movingData/et4_region_flat_file' using (delimiter '=') as select r_name, r_comment from region; Using the second session review the file that was created, et3_region_flat_file, in the /home/nz/labs/movingData directory. Input [Terminal 2] more et3_region_flat_file Output emea=europe, middle east, africa na=north america sa=south america ap=asia pacific Notice that only two columns are present in the flat file using the '=' string as a delimiter. 5.1.4 (Optional) Unloading data with an External Table from two tables The first three external table exercises unloaded data from one table. In this exercise, the external table will be created based on a table join between the REGION and NATION tables. The two tables will be joined on the REGIONKEY and only the N_NAME and R_NAME columns will be defined for the external table. This exercise will illustrate how data can be unloaded using SQL statements other than a simple SELECT FROM statement. The external table will be named ET_NATION_REGION using another ASCII delimited text file named et_nation_file_flat_file. Unload data from both the REGION and NATION tables joined on the REGIONKEY column to list all of the countries and their associated regions. Instead of specifying the columns in the create external table statement you will use the AS SELECT option: Input [Terminal 2] create external table et_nation_region '/home/nz/labs/movingData/et_nation_region_flat_file' as select n_name, r_name from nation, region where n_regionkey=r_regionkey; Output INSERT 0 14 List the properties of the ET_NATION_REGION external table. Input [Terminal 1] \\d et_nation_region Output External Table \"ET_NATION_REGION\" Attribute | Type | Modifier -----------+---------------+---------- N_NAME | CHARACTER(25) | NOT NULL R_NAME | CHARACTER(25) | NOT NULL DataObject - '/home/nz/labs/movingData/et_nation_region_flat_file' adjustdistzeroint - bool style - 1_0 code set - compress - FALSE cr in string - ctrl chars - date delim - - date style - YMD delim - | encoding - INTERNAL escape - fill record - format - TEXT ignore zero - log dir - /tmp max errors - 1 max rows - null value - NULL quoted value - NO remote source - require quotes - skip rows - socket buf size - 8388608 timedelim - : time round nanos - time style - 24HOUR trunc string - y2base - includezeroseconds - record length - record delimiter - nullindicator bytes - layout - decimaldelim - disablenfc - includeheader - datetime delim - meridian delim - lfinstring - You will notice that the external table was created using the two columns specified in the SELECT clause: N_NAME and R_NAME. View the data of the ET_NATION_REGION external table. Input [Terminal 1] select * from et_nation_region; Output N_NAME | R_NAME ---------------------------+--------------------------- canada | na united states | na brazil | sa guyana | sa venezuela | sa united kingdom | emea portugal | emea united arab emirates | emea south africa | emea australia | ap japan | ap macau | ap hong kong | ap new zealand | ap (14 rows) This is the result of the joining the NATION and REGION table on the REGIONKEY column to return just the N_NAME and R_NAME columns. Using the second session, review the file that was created, et_nation_region_flat_file, in the /home/nz/labs/movingData directory: Input [Terminal 2] more et_nation_region_flat_file Output canada|na united states|na brazil|sa guyana|sa venezuela|sa united kingdom|emea portugal|emea united arab emirates|emea south africa|emea australia|ap japan|ap macau|ap hong kong|ap new zealand|ap We created a flat delimited flat file from a complex SQL statement. External tables are a very flexible and powerful way to load, unload and transfer data. 5.1.5 (Optional) Unloading data with an External Table using the compress format In the previous exercises, the external tables were created used the default ASCII delimited text format. In this exercise, the external table will be similar to the second external table that was created, but instead of the using an ASCII delimited text format we will use the compressed binary format. The name of the external table will be ET4_REGION and the datasource file name will be et4_region_compress. Sample Syntax CREATE EXTERNAL TABLE table_name 'filename' USING (COMPRESS true FORMAT 'internal') AS select_statement; Create an external table using a similar method that you used to create the second external table, in section 2.1.2. But instead of using an ASCII delimited-text format, the datasource will use the compressed binary format. This is achieved by using the COMPRESS and FORMAT external table options: Input [Terminal 1] create external table et4_region '/home/nz/labs/movingData/et4_region_compress' using (compress true format 'internal') as select * from region; Output INSERT 0 4 As a reminder, the external table is created, and the data is unloaded in the same operation using the AS SELECT clause. List the properties of the ET4_REGION external table Input [Terminal 1] \\d et4_region Output External Table \"ET4_REGION\" Attribute | Type | Modifier -------------+------------------------+---------- R_REGIONKEY | INTEGER | NOT NULL R_NAME | CHARACTER(25) | NOT NULL R_COMMENT | CHARACTER VARYING(152) | DataObject - '/home/nz/labs/movingData/et4_region_compress' adjustdistzeroint - bool style - code set - compress - TRUE cr in string - ctrl chars - date delim - date style - delim - encoding - escape - fill record - format - INTERNAL ignore zero - log dir - max errors - 1 max rows - null value - quoted value - remote source - require quotes - skip rows - socket buf size - 8388608 timedelim - time round nanos - time style - trunc string - y2base - includezeroseconds - record length - record delimiter - nullindicator bytes - layout - decimaldelim - disablenfc - includeheader - datetime delim - meridian delim - lfinstring - Notice that the option for COMPRESS has changed from FALSE to TRUE indicating that the datasource file is compressed, and the FORMAT has changed from TEXT to INTERNAL which is required for compressed files. 5.2 Dropping External Tables Dropping external tables is similar to dropping a regular Netezza Performance Server table. The column definition for the external table is removed from the Netezza Performance Server catalog. Keep in mind that dropping the table doesn't delete the external datasource file so they also have to be maintained, but the external datasource file can still be used for loading data into a different table. In this exercise you will drop the ET1_REGION table, but you will not delete the associated external datasource file, et1_region_flat_file. This datasource file will be used later in this lab to load data into the REGION table. Drop the first external table that you created, ET1_REGION, using the DROP TABLE command: Input [Terminal 1] drop table et1_region; Output DROP TABLE The same drop command for tables is used for external tables. There is not a separate DROP EXTERNAL TABLE command. Verify the external table has been dropped using the internal slash option, \\dx, to list all of the external tables. Input [Terminal 1] \\dx Output List of relations Schema | Name | Type | Owner --------+------------------+----------------+---------- ADMIN | ET2_REGION | EXTERNAL TABLE | LABADMIN ADMIN | ET3_REGION | EXTERNAL TABLE | LABADMIN ADMIN | ET4_REGION | EXTERNAL TABLE | LABADMIN ADMIN | ET4_TEST | EXTERNAL TABLE | LABADMIN ADMIN | ET_NATION_REGION | EXTERNAL TABLE | LABADMIN (5 rows) Notice the remaining external tables that you created still exist. Even though the external table definition no longer exists within the LABDB database, the flat file named et1_region_flat_file still exists in the /home/nz/labs/movingData directory. Verify this by using the second putty session: Input [Terminal 2] ls Output et1_region_flat_file et3_region_flat_file et4_region_flat_file et2_region_flat_file et4_region_compress et_nation_region_flat_file Notice that the file et1_REGION_flat_file still exists. This file can still be used to load data into another similar table. 5.3 Loading Data using External Tables External tables can also be used to load data into tables in the database. In this exercise, data will be loaded into the REGION table, after first removing the existing rows. The method to load data from external tables into a table is similar to using the DML INSERT INTO and SELECT FROM statements. We will use two different methods to load data into the REGION table, one using an external table and the other using the external datasource file. Loading data into a table from any external table will generate an associated log file with a default name of <table_name>.<database_name>.log Before loading the data into the REGION table, delete the rows from the data using the [TRUNCATE TABLE command: Input [Terminal 1] truncate table region; Output TRUNCATE TABLE Verify the table is empty with the SELECT * command: Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+--------+----------- (0 rows) Load data into the REGION table from the ET2_REGION external table using an INSERT statement: Input [Terminal 1] insert into region select * from et2_region; Output INSERT 0 4 Check to ensure that the table contains the four rows using the SELECT * statement. Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific (4 rows) Again, delete the rows in the REGION table: Input [Terminal 1] truncate table region; Output TRUNCATE TABLE Check to ensure that the table is empty using the SELECT * statement. Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+--------+----------- (0 rows) Load data into the REGION table using the ASCII delimited file that was created for external table ET1_REGION. Recall that the definition of the external table was removed from that database, but the external data source file, et1_region_flat_file, still exists: Input [Terminal 1] insert into region select * from external '/labs/movingData/et1_region_flat_file'; Output INSERT 0 4 Verify the table contains the four rows using the SELECT * statement. Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific (4 rows) Since this is a load operation, there is always an associated log file <table>.<database>.nzlog created for each load performed. By default this log file is created in the /tmp directory. In the second Putty session review this file: Input [Terminal 2] more /tmp/REGION.ADMIN.LABDB.nzlog Output Load started at:08-May-13 07:10:13 EDT Database: LABDB Tablename: REGION Datafile: /labs/movingData/etl_region_flat_file ... Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' Time Style: 24HOUR Time Delim: ':' Time extra zeros: No Statistics number of records read: 4 number of bad records: 0 number of records loaded: 4 Elapsed Time (sec): 0.0 Load completed at: 08-May-13 07:10:13 EDT Notice that the log file contains the load options and statistics of the load, along with environment information to identify the table. 6 Loading Data using the nzload Utility The nzload command is a SQL CLI client application that allows you to load data from the local host or a remote client, on all the supported client platforms. The nzload command processes command-line load options to send queries to the host to create an external table definition, runs the insert/select query to load data, and when the load completes, drops the external table. The nzload command is a command-line program that accepts options from multiple sources, where some of the sources can be from: Command line Control file NZ Environment Variables Without a control file, you can only do one load at a time. Using a control file allows multiple loads. The nzload command connects to a database with a username and password, just like any other Netezza Performance Server client application. The username specifies an account with a particular set of privileges, and the system uses this account to verify access. For this section of the lab you will continue to use the LABADMIN user to load data into the LABDB database. The nzload utility will be used to load records from an external datasource file into the REGION table. The nzload log files will be reviewed to examine the nzload options. Since you will be loading data into a populated REGION table, you will use the TRUNCATE TABLE command to remove the rows from the table. We will continue to use the two putty sessions from the external table lab. Session One, which is connected to the NZSQL console to execute SQL commands, for example to review tables after load operations Session Two, which will be used for operating system commands such as execute nzload. 6.1 Using the nzload Utility with Command Line Options The first method for using the nzload utility to load data in the REGION table will specify options at the command line. We will only need to specify the datasource file and we will use default options for the rest. The datasource file will be the et1_region_flat_file that you created in the External Tables section. Sample Syntax nzload --db <database> -u <username> --pw <password> -df <datasource filename> As the LABDB database owner, LABADMIN first remove the rows in the REGION table: Input [Terminal 1] truncate table region; Output TRUNCATE TABLE Verify the rows have been removed from the table using the SELECT * statement: Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+--------+----------- (0 rows) Using the second session at the OS command line, use the nzload utility to load data from the et1_region_flat file into the REGION table using the following command line options: Sample Syntax -db <database name>, -u <user>, -pw <password>, -t <table name>, -df <data file>, and --delimiter <string>: Input [Terminal 2] nzload -db labdb -u labadmin -pw password -t region -df et1_region_flat_file -delimiter '|' Output Load session of table 'REGION' completed successfully Verify the rows have been load into the table by using the SELECT * statement: Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific (4 rows) These rows were loaded into the REGION table from the records in the et1_region_flat_file file. For every load task performed there is always an associated log file created with the format <table>.<db>.nzlog . By default, this log file is created in the current working directory, which is the /home/nz/labs/movingData directory. In the second session review this file: Input [Terminal 2] [more REGION.ADMIN.LABDB.nzlog Output Load started at:03-Apr-20 03:05:12 PDT Database: LABDB Schema: ADMIN Tablename: REGION Datafile: /home/nz/labs/movingData/et1_region_flat_file Host: localhost.localdomain Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Statistics number of records read: 4 number of bytes read: 91 number of bad records: 0 ------------------------------------------------- number of records loaded: 4 Elapsed Time (sec): 0.0 ----------------------------------------------------------------------------- Load completed at: 03-Apr-20 03:05:12 PDT ============================================================================= Notice the log file contains the load options and statistics of the load, along with environment information to identify the database and table. The --db, -u, and --pw, options specify the database name, the user, and the password respectively. Alternatively, you could omit these options if the NZ environment variables are set to the appropriate database, username and password values. Since the NZ environment variables, NZ_DATABASE, NZ_USER, and NZ_PASSWORD are set to system, admin, and password, we will need to use these options so the load will use the LABDB database and the LABADMIN user. There are other options that you can use with the nzload utility. These options were not specified here since the default values were sufficient for this load task. -t specifies the target table name in the database -df specifies the datasource file to be loaded -delimiter specifies the string to use as the delimiter in an ASCII delimited text file. The following nzload command syntax is equivalent to the nzload command we used above. It is intended to demonstrate some of the options that can be used with the nzload command, or can be omitted when default values are acceptable. Sample Syntax nzload --db labdb --u labadmin --pw password --t region --df et1_region_flat_file --delimiter '|' --outputDir '<current directory>' --lf <table>.<database>.nzlog --bf<table>.<database>.nzlog --compress false --format text --maxErrors 1 The --lf, -bf, and --maxErrors options are explained in the next exercise. The --compress and --format options indicate that the datasource file is an ASCII delimited text file. For a compressed binary datasource file the following options would be used: -compress true --format internal. 6.2 Using the nzload Utility with a Control File. As demonstrated in section 3.1 you can run the nzload command by specifying the command line options or you can use another method by specifying the options in a file, referred to as a control file. This is useful because the file can be modified over time, since loading data into a database for a data warehouse environment is a continuous operation. A nzload control file has the following basic structure: Sample Syntax DATAFILE <filename> { [<option name> <option value>] } The --cf option is used at the nzload command line to indicate the use of a control file: Sample Syntax nzload --u <username> -pw <password> -cf <control file> The --u and --pw options are optional if the NZ_USER and NZ_PASSWORD environment variables are set to the appropriate user and password. Using the --u and --pw options overrides the values in the NZ environment variables. In this session you will load rows into an empty REGION table using the nzload utility with a control file. The control file will set the following options: delimiter, logDir, logFile, and badFile, along with the database and table name. The datasource file to be used in this session is the region.del file. As the LABDB database owner, LABADMIN first remove the rows in the REGION table: Input [Terminal 1] truncate table region; Output TRUNCATE TABLE Verify the rows have been removed from the table using the SELECT * statement. The table should contain no rows. Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+--------+----------- (0 rows) The control file will be used by the nzload utility to load data into the REGION table using the region.del data file. The control file has already been created in the lab directory. A control file can include the following options: Parameter Value Database Database name Tablename Table name Delimiter Delimiter string LogDir Log directory LogFile Log file name BadFile Bad record log file name 1) Review the control file in the second putty session with the following command: Input [Terminal 2] more control_file Output DATAFILE /home/nz/labs/movingData/region.del { Database labdb Tablename region Delimiter '|' LogDir '/home/nz/labs/movingData' LogFile region.log BadFile region.bad } 2) Load the data using the nzload utility and the control file you just reviewed. Input [Terminal 2] nzload -u labadmin -pw password -cf control_file Output Load session of table 'REGION' completed successfully 3) The nzload log file was renamed using the information in the control file. The log file name was changed from the default to region.log and the location was changed from the /tmp directory to /labs/. Check the nzload log. Input [Terminal 2] more region.log Output Load started at:03-Apr-20 03:38:32 PDT Database: LABDB Schema: ADMIN Tablename: REGION Datafile: /home/nz/labs/movingData/region.del Host: localhost.localdomain Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Statistics number of records read: 4 number of bytes read: 91 number of bad records: 0 ------------------------------------------------- number of records loaded: 4 Elapsed Time (sec): 0.0 ----------------------------------------------------------------------------- Load completed at: 03-Apr-20 03:38:32 PDT 4) Verify the rows in the REGION table in the first putty session with the nzsql console: Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific (4 rows) 6.3 (Optional) Using nzload with Bad Records The first two load methods illustrated how to use the nzload utility to load data into an empty table using command line options or a control file. In a data warehousing environment, most of the time data is incrementally added to a table already containing some rows. There will be instances where records from a datasource might not match the datatypes in the table. When this occurs, the load will abort when the first bad record is encountered. This is the default behavior and is controlled by the maxErrors option, which is set to a default value of 1. For this exercise we will add additional rows to the NATION table. Since we will be adding rows to the NATION table, there will be no need to truncate the table. The datasource file we will be using is the nation.del file, which unfortunately has a bad record. First check the NATION table by listing all of the rows in the table using the SELECT * statement in the first putty session: Input [Terminal 1] select * from nation; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand (14 rows) Using the second session at the OS command line you will use the nzload utility to load data from the nation.del file into the NATION: Input [Terminal 1] nzload -db LABDB -u labadmin -pw password -t nation -df nation.del -delimiter '|' Output Error: Operation canceled Error: External Table : count of bad input rows reached maxerrors limit See /home/nz/labs/movingData/NATION.ADMIN.LABDB.nzlog file Error: Load Failed, records not inserted. This is an indication that the load has failed due to a bad record in the datasource file. Since the load has failed no rows were loaded into the NATION table, which you can confirm by using the SELECT * statement (in the first session): Input [Terminal 1] select * from nation; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand (14 rows) In the second session, check the log file to determine the problem: Input [Terminal 2] more NATION.ADMIN.LABDB.nzlog Output Load started at:03-Apr-20 04:31:19 PDT Database: LABDB Schema: ADMIN Tablename: NATION Datafile: <stdin> Host: localhost.localdomain Load Options Field delimiter: '\\t' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Load started at:03-Apr-20 04:36:02 PDT Database: LABDB Schema: ADMIN Tablename: NATION Datafile: /home/nz/labs/movingData/nation.del Host: localhost.localdomain Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Found bad records bad #: input row #(byte offset to last char examined) [field #, declaration] diagnostic, \"text consumed\"[last char examined] ---------------------------------------------------------------------------------------------------------------------------- 1: 10(1) [1, INT4] expected field delimiter or end of record, \"2\"[t] Statistics number of records read: 10 number of bytes read: 226 number of bad records: 1 ------------------------------------------------- number of records loaded: 0 Elapsed Time (sec): 0.0 ----------------------------------------------------------------------------- Load completed at: 03-Apr-20 04:36:02 PDT ============================================================================= The Statistics section indicates that 10 records were read before the bad record was encountered during the load process. As expected, no rows were inserted into the table since the default is to abort the load when one bad record is encountered. The log file also provides information about the bad record: Sample Output Found bad records bad #: input row #(byte offset to last char examined) [field #, declaration] diagnostic, \"text consumed\"[last char examined] ---------------------------------------------------------------------------------------------------------------------------- 1: 10(1) [1, INT4] expected field delimiter or end of record, \"2\"[t] Using the log file, we are able to determine the problem is that the value '2t' is in a field for an INT(4) column. Since '2t' is not a valid integer, the load marked this as a bad record 10(1) indicates the input record number within the file and the offset within the row where a problem was encountered. For this example, the input record is 10 and offset is 1. [1, INT(4)] indicates the column number within the row and the data for the column. For this example, the column number is 1 and the data type is INT(4). \"2\"[t] indicates the character that caused the problem. For this example, the character is 2t. We can verify our problem determination for the load failure is correct by examining the nation.del datasource file that was used for the load. In the second session execute the following command: Input [Terminal 2] more nation.del Output 15|andorra|2|andorra 16|ascension islan|3|ascension 17|austria|3|osterreich 18|bahamas|2|bahamas 19|barbados|2|barbados 20|belgium|3|belqique 21|chile|2|chile 22|cuba|2|cuba 23|cook islands|4|cook islands 2t|denmark|3|denmark 25|ecuador|2|ecuador 26|falkland islands|3|islas malinas 27|fiji|4|fiji 28|finland|3|suomen tasavalta 29|greenland|1|kalaallit nunaat 30|great britain|3|great britian 31|gibraltar|3|gibraltar 32|hungary|3|magyarorszag 33|iceland|3|lyoveldio island 34|ireland|3|eire 35|isle of man|3|isle of man 36|jamaica|2|jamaica 37|korea|4|han-guk 38|luxembourg|3|Luxembourg Notice on the 10th line the output. There is indeed an invalid 2t in the first column of the input file. Therefore, we made the correct assumption that the '2t' is causing the problem. From this list you can assume that the correct value should be 24. Alternatively, we could have examined the nzload bad log file NATION.LABDB.nzbad, which will contain all bad records that are encountered during a load. In the second session execute the following command: Input [Terminal 2] more NATION.ADMIN.LABDB.nzbad Output 2t|denmark|3|Denmark This is the same data as identified in the nation.del file and using the log file information to locate the record. Since the default is to stop the load after the first bad record is processed there is only one row in the bad log file. If we were to change the default behavior to allow more bad records to be processed, this file could potentially contain more records. It provides a comfortable overview of all the records that created exceptions during load. We have the option of changing the NATION.del file to change '2t' to '24' and then rerun the same nzload command as in step 7. Instead you will rerun a similar load but you will allow 10 bad records to be encountered during the load process. To change the default behavior, you need to use the command option -maxErrors. You will also change the name of the nzbad file using the --bf command option and the log filename using the --lf command option: Input [Terminal 2] nzload -db labdb -u labadmin -pw password -t nation -df nation.del -delimiter '|' -maxerrors 10 -bf nation.bad -lf nation.log Output Load session of table 'NATION' completed successfully Now the load is successful. 6) Verify the newly loaded rows are in the NATION using the SELECT * command: Input [Terminal 1] select * from nation order by n_nationkey; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand 15 | andorra | 2 | andorra 16 | ascension islan | 3 | ascension 17 | austria | 3 | osterreich 18 | bahamas | 2 | bahamas 19 | barbados | 2 | barbados 20 | belgium | 3 | belqique 21 | chile | 2 | chile 22 | cuba | 2 | cuba 23 | cook islands | 4 | cook islands 25 | ecuador | 2 | ecuador 26 | falkland islands | 3 | islas malinas 27 | fiji | 4 | fiji 28 | finland | 3 | suomen tasavalta 29 | greenland | 1 | kalaallit nunaat 30 | great britain | 3 | great britian 31 | gibraltar | 3 | gibraltar 32 | hungary | 3 | magyarorszag 33 | iceland | 3 | lyoveldio island 34 | ireland | 3 | eire 35 | isle of man | 3 | isle of man 36 | jamaica | 2 | jamaica 37 | korea | 4 | han-guk 38 | luxembourg | 3 | luxembourg 39 | monaco | 3 | monaco (38 rows) Now all of the new records were loaded except for the one bad row with nation key 24. 7) Even though the nzload command received a successful message it is good practice to review the nzload log file for any problems, for example bad rows that are under the maxErrors threshold. In the second putty session execute the following command: Note xxxx is the ID associated to your nzload. Input [Terminal 2] more NATION.ADMIN.LABDB.xxxx.nzbad The log file should be similar to the following. Output Load started at:03-Apr-20 06:12:43 PDT Database: LABDB Schema: ADMIN Tablename: NATION Datafile: /home/nz/labs/movingData/nation.del Host: localhost.localdomain Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 10 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Found bad records bad #: input row #(byte offset to last char examined) [field #, declaration] diagnostic, \"text consumed\"[last char examined] ---------------------------------------------------------------------------------------------------------------------------- 1: 10(1) [1, INT4] expected field delimiter or end of record, \"2\"[t] Statistics number of records read: 25 number of bytes read: 607 number of bad records: 1 ------------------------------------------------- number of records loaded: 24 Elapsed Time (sec): 0.0 ----------------------------------------------------------------------------- Load completed at: 03-Apr-20 06:12:43 PDT ============================================================================= The main difference as compared with the example before, is that all 25 of the data records in the data source file were processed, but only 24 records were loaded because there was one bad record in the data source file. Correct the bad row and load it into the NATION table. There are couple options you could use. One option is to extract the bad row from the original data source file and create a new data source file with the correct record. However, this task could be tedious when dealing with large data source files and potentially many bad records. The other option, which is more appropriate, is to use the bad log file. All of the bad records that cannot be loaded into the table are placed in the bad log file. In the second session use vi to open and edit the nation.bad file and change the '2t' to '24' in the first field. The vi editor has two modes, a command mode used to save files, quit the editor etc. and an insert mode. Initially you will be in the command mode. To change the file, you need to switch into the insert mode by pressing \"i\". The editor will show an -- INSERT -- at the bottom of the screen. Note: you can use gedit from the VM desktop (gedit nation.bad) Input [Terminal 2] vi nation.bad Output before changes 2t|denmark|3|Denmark 8) You can now use the cursor keys to navigate. Change the first two chars of the bad row from 2t to 24. Your screen should look like the following: Output after changes 24|denmark|3|Denmark -- INSERT -- 9) To save the changes, press \"Esc\" to switch back into command mode. You should see that the ---INSERT--- string at the bottom of the screen vanishes. Enter :wq! and press enter to write the file, and quit the editor. 10) After the nation.bad file has modified to correct the record, issue a nzload to load the modified nation.bad file: Input [Terminal 2] nzload -db labdb -u labadmin -pw password -t nation -df nation.bad -delimiter '|' Output Load session of table 'NATION' completed successfully 11) Verify the new row has been loaded into the table: Input [Terminal 1] select * from nation order by n_nationkey; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand 15 | andorra | 2 | andorra 16 | ascension islan | 3 | ascension 17 | austria | 3 | osterreich 18 | bahamas | 2 | bahamas 19 | barbados | 2 | barbados 20 | belgium | 3 | belqique 21 | chile | 2 | chile 22 | cuba | 2 | cuba 23 | cook islands | 4 | cook islands 24 | denmark | 3 | denmark 25 | ecuador | 2 | ecuador 26 | falkland islands | 3 | islas malinas 27 | fiji | 4 | fiji 28 | finland | 3 | suomen tasavalta 29 | greenland | 1 | kalaallit nunaat 30 | great britain | 3 | great britian 31 | gibraltar | 3 | gibraltar 32 | hungary | 3 | magyarorszag 33 | iceland | 3 | lyoveldio island 34 | ireland | 3 | eire 35 | isle of man | 3 | isle of man 36 | jamaica | 2 | jamaica 37 | korea | 4 | han-guk 38 | luxembourg | 3 | luxembourg 39 | monaco | 3 | monaco (39 rows) The row in bold denotes the new row that was added to the table, which was the bad record you corrected. Congratulations you have completed the lab.","title":"Loading and Unloading Data"},{"location":"nz-05-Loading-and-Unloading-Data/#1-load-and-unloading-data","text":"In every data warehouse environment, there is a need to load new data into the database. The task to load data into the database is not just a one-time operation but rather a continuous operation that can occur hourly, daily, weekly, or even monthly. Loading data into the database is a vital operation that needs to be supported by the data warehouse system. Netezza Performance Server (NPS) provides a framework to support not only the loading of data into the Netezza Performance Server database environment, but also the unloading of data from the database environment. This framework contains more than one component, some of these components are: External Tables -- These are tables stored as flat files on the host or client systems and registered like tables in the Netezza Performance Server catalog. They can be used to load data into the Netezza Performance Server or unload data to the file system. nzload -- This is a wrapper command line tool around external tables that provides an easy method loading data into the Netezza Performance Server. Format Options -- These are options for formatting the data load to and from external tables.","title":"1 Load and Unloading Data"},{"location":"nz-05-Loading-and-Unloading-Data/#11-objectives","text":"This lab will help you explore the Netezza Performance Server framework components for loading data into the database and unloading data from the database. You will use the various commands to create external tables to unload and load data. You will also get a basic understanding of the nzload utility. In this lab the REGION and NATION tables in the LABDB database are used to illustrate the use of external tables and the nzload utility. After this lab you will have a good understanding on how to load and unload data from a Netezza Performance Server database environment The first part of this lab will explore using External Tables to unload and load data. The second part of this lab will discuss using the nzload utility to load records into tables.","title":"1.1 Objectives"},{"location":"nz-05-Loading-and-Unloading-Data/#2-lab-environment","text":"The lab system will be a virtual machine running on Virtual Box. Please see the document on how to install the NPS Virtual Machine for your workstation (Windows or Mac OS).","title":"2 Lab Environment"},{"location":"nz-05-Loading-and-Unloading-Data/#3-connect-to-the-netezza-performance-server","text":"Use the following information to connect to the virtual NPS system. There are two options to access the command line: Login to the VM directly and use the terminal application available inside the VM. Use the local terminal application on your workstation. the lab will use the command line as the nz user.","title":"3 Connect to the Netezza Performance Server"},{"location":"nz-05-Loading-and-Unloading-Data/#4-lab-setup","text":"This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using a terminal application (Windows PowerShell, PuTTY, Mac OSX Terminal) If you are continuing from the previous lab and are already connected to nzsql quit the nzsql console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/movingData/setupLab ./setupLab.sh Output DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully The error message at the beginning is expected since the script tries to clean up existing LINEITEM tables.","title":"4 Lab Setup"},{"location":"nz-05-Loading-and-Unloading-Data/#5-external-tables","text":"An external table allows Netezza Performance Server to treat an external file as a database table. An external table has a definition, a table schema, in the Netezza Performance Server system catalog, but the actual data exists outside of the Netezza Performance Server database. This is referred to as a data source file. External tables can be used to access files which are stored on a file system. After you have created the external table definition, you can use INSERT INTO statements to load data from the external file into a database table or SELECT FROM statements to query the external table. Different methods are described to create and use external tables using the nzsql interface. The external data source files for the external tables will also be examined, so a second session will be used to view these files. Connect to your Netezza Performance Server image using a Terminal application to ssh into <your-nps-vm-ip-address> (as user nz with password nz ). Alternatively, you can use a terminal application on the virtual machine desktop. <your-nps-vm-ip-address> is the default IP address for a local VM, the IP may be different for your session. Change to the lab working directory /home/nz/labs/movingData using the following command: Input cd /home/nz/labs/movingData Connect to the LABDB database as the database owner, LABADMIN, using the nzsql interface: Input nzsql -d LABDB -u labadmin -pw password Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit In this lab we will need to alternatively execute SQL commands and operating system commands. To make this task easier for you, we will open a second Terminal session for executing operating system commands like nzload, view generated external files etc. It will be referred to as session 2 throughout the lab. The picture above shows the two Terminal windows that you will need. Terminal 1, on the left, will be used for SQL commands and Terminal 2, on the right, will be used for operating system prompt commands. Open another session [Terminal 2] using PuTTY or a Terminal Application. Login to <your-nps-vm-ip-address> as user nz with password nz. Change to the /home/nz/labs/movingData directory: Input [Terminal 2] cd /home/nz/labs/movingData","title":"5 External Tables"},{"location":"nz-05-Loading-and-Unloading-Data/#51-unloading-data-using-external-tables","text":"External tables will be used to unload rows from the LABDB database as records into an external datasource file. Various methods to create and use external tables will be explored unloading rows from either REGION or NATION tables. Five different basic use cases are presented for you to follow so you can gain a better understanding of how to use external tables to unload data from a database.","title":"5.1 Unloading Data using External Tables"},{"location":"nz-05-Loading-and-Unloading-Data/#511-unloading-data-with-an-external-table-created-with-the-sameas-clause","text":"The first external table will be used to unload data from the REGION table into an ASCII delimited text file. This external table will be named ET1_REGION using the same column definition as the REGION table. After the ET1_REGION external table is created you will then use it to unload all the rows from the REGION table. The records for the ET1_REGION external table will be in the external datasource file, et1_region_flat_file. The basic syntax to create this type of external table is: Sample Syntax CREATE EXTERNAL TABLE table_name SAMEAS table_name USING external_table_options The SAMEAS clause allows the external table to be created with the same column definition of the referenced table. This is referred to as implicit schema definition. As the LABDB database owner, LABADMIN, you will create the first basic external table using the same column definitions as the REGION table: Input [Terminal 1] create external table et1_region sameas region using (dataobject ('/home/nz/labs/movingData/et1_region_flat_file')); Output CREATE EXTERNAL TABLE Use the internal slash option \\dx to list the external tables in the LABDB database. Input [Terminal 1] \\dx Output List of relations Schema | Name | Type | Owner --------+------------+----------------+---------- ADMIN | ET1_REGION | EXTERNAL TABLE | LABADMIN (1 row) List the properties of the external table et1_region using the following internal slash option to describe the table, \\d <external table name> . Input [Terminal 1] \\d et1_region Output External Table \"ET1_REGION\" Attribute | Type | Modifier -------------+------------------------+---------- R_REGIONKEY | INTEGER | NOT NULL R_NAME | CHARACTER(25) | NOT NULL R_COMMENT | CHARACTER VARYING(152) | DataObject - '/home/nz/labs/movingData/et1_region_flat_file' adjustdistzeroint - bool style - 1_0 code set - compress - FALSE cr in string - ctrl chars - date delim - - date style - YMD delim - | encoding - INTERNAL escape - fill record - format - TEXT ignore zero - log dir - /tmp max errors - 1 max rows - null value - NULL quoted value - NO remote source - require quotes - skip rows - socket buf size - 8388608 timedelim - : time round nanos - time style - 24HOUR trunc string - y2base - includezeroseconds - record length - record delimiter - nullindicator bytes - layout - decimaldelim - disablenfc - includeheader - datetime delim - meridian delim - lfinstring - This output includes the columns and associated data types in the external table. Notice that this is similar to the REGION table since the external table was created using the SAMEAS clause in the CREATE EXTERNAL TABLE command. The output also includes the properties of the external table. The most notable property is the DataObject property that shows the location and the name of the external datasource file used for the external table. We will examine some of the others. Now that the external table is created, use it to unload data from the REGION table using an INSERT statement. Input [Terminal 1] insert into et1_region select * from region; Output INSERT 0 4 Use the external table like a regular table by issuing SQL statements. Try issuing a simple SELECT FROM statement to return all rows in external table ET1_REGION : Input [Terminal 1] select * from et1_region order by 1; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific (4 rows) You will notice that this is the same data that is in the REGION table. But the data retrieved for this SELECT statement was from the datasource of this external table and not from the data within the database. The main reason for creating an external table is to unload data from a table to a file. Using the second Putty (or Terminal) session, review the file et1_region_flat_file. This is the file that was created in the /home/nz/labs/movingData directory. Input [Terminal 2] more et1_region_flat_file Output 3|emea|europe, middle east, africa 1|na|north america 2|sa|south america 4|ap|asia pacific This is an ASCII delimited flat file containing the data from the REGION table. The column delimiter used in this file was the default character '|'.","title":"5.1.1 Unloading data with an External Table created with the SAMEAS clause"},{"location":"nz-05-Loading-and-Unloading-Data/#512-unloading-data-with-an-external-table-using-the-as-select-clause","text":"The second external table will be used to unload data from the REGION table into an ASCII delimited text file using a different method. The external table will be created and the data will be unloaded in the same create statement. A separate step is not required to unload the data. The external table will be named ET2_REGION and the external datasource file will be named et2_region_flat_file. The basic syntax to create this type of external table is: Sample Syntax CREATE EXTERNAL TABLE table_name 'filename' AS select_statement; The AS clause allows the external table to be created with the same columns returned in the SELECT FROM statement, which is referred to as implicit table schema definition. This also unloads the rows at the same time the external table is created. The first method used to create an external table required the data to be unloaded in a second step using an INSERT statement. Using the first Putty (or Terminal) session, create an external table and unload the data in a single step. Input [Terminal 1] create external table et2_region '/home/nz/labs/movingData/et2_region_flat_file' as select * from region; Output INSERT 0 4 This command created the external table ET2_REGION using the same definition as the REGION table and also unloaded the data to the et2_region_flat_file. Again, use /dx to list the external tables in the LABDB database. Input [Terminal 1] \\dx Output List of relations Schema | Name | Type | Owner --------+------------+----------------+---------- ADMIN | ET1_REGION | EXTERNAL TABLE | LABADMIN ADMIN | ET2_REGION | EXTERNAL TABLE | LABADMIN (2 rows) You will notice that there are now two external tables. You can also list the properties of the external table. The output will be similar to the output in the last section, except for the filename. Using the second session, review the file that was created, et2_region_flat_file, in the /home/nz/labs/movingData directory. Input [Terminal 2] more et2_region_flat_file Output 3|emea|europe, middle east, africa 1|na|north america 2|sa|south america 4|ap|asia pacific This file is exactly the same as the file you reviewed in the last chapter. The only difference in this example is we didn't need to unload it explicitly.","title":"5.1.2 Unloading data with an External Table using the AS SELECT clause"},{"location":"nz-05-Loading-and-Unloading-Data/#513-unloading-data-with-an-external-table-using-defined-columns","text":"The first two external tables that you created used the exact same columns from the REGION table using an implicit table schema. You can also create an external table by explicitly specifying the columns. This is referred to as an explicit table schema. The third external table that you create will still be used to unload data from the REGION table but only from the R_NAME and R_COMMENT columns. The ET3_REGION external table will be created in one step and then the data will be unloaded into the et3_region_flat_file ASCII delimited text file using a different delimiter string. The basic syntax to create this type of external table is: Sample Syntax CREATE EXTERNAL TABLE table_name ({column_name type} [, ... ]) [USING external_table_options}] Create a new external table to only include the R_NAME and R_COMMENT columns and exclude the R_REGIONKEY column from the REGION table. Also change the delimiter string from the default '|' to '=': Input [Terminal 1] create external table et3_region (r_name char(25), r_comment varchar(152)) USING (dataobject ('/home/nz/labs/movingData/et3_region_flat_file') DELIMITER '='); Output CREATE EXTERNAL TABLE List the properties of the ET3_REGION external table: Input [Terminal 1] \\d et3_region Output External Table \"ET3_REGION\" Attribute | Type | Modifier -----------+------------------------+---------- R_NAME | CHARACTER(25) | R_COMMENT | CHARACTER VARYING(152) | DataObject - '/home/nz/labs/movingData/et3_region_flat_file' adjustdistzeroint - bool style - 1_0 code set - compress - FALSE cr in string - ctrl chars - date delim - - date style - YMD delim - = encoding - INTERNAL escape - fill record - format - TEXT ignore zero - log dir - /tmp max errors - 1 max rows - null value - NULL quoted value - NO remote source - require quotes - skip rows - socket buf size - 8388608 timedelim - : time round nanos - time style - 24HOUR trunc string - y2base - includezeroseconds - record length - record delimiter - nullindicator bytes - layout - decimaldelim - disablenfc - includeheader - datetime delim - meridian delim - lfinstring - Notice that there are only two columns for this external table since only two columns were specified when creating the external table. The rest of the output is very similar to the properties of the other two external tables that were created, with two main exceptions. The first difference is the Dataobjects field, since the filename is different. The other difference is the string used for the delimiter, since it is now '=' instead of the default, '|'. Unload the data from the REGION table but only the data from columns R_NAME and R_COMMENT. Input [Terminal 1] insert into et3_region select r_name, r_comment from region; Output INSERT 0 4 Alternatively, you could have created the external table and unloaded the data in one step using the following command: Sample Syntax create external table et4_test '/home/nz/labs/movingData/et4_region_flat_file' using (delimiter '=') as select r_name, r_comment from region; Using the second session review the file that was created, et3_region_flat_file, in the /home/nz/labs/movingData directory. Input [Terminal 2] more et3_region_flat_file Output emea=europe, middle east, africa na=north america sa=south america ap=asia pacific Notice that only two columns are present in the flat file using the '=' string as a delimiter.","title":"5.1.3 Unloading data with an external table using defined columns"},{"location":"nz-05-Loading-and-Unloading-Data/#514-optional-unloading-data-with-an-external-table-from-two-tables","text":"The first three external table exercises unloaded data from one table. In this exercise, the external table will be created based on a table join between the REGION and NATION tables. The two tables will be joined on the REGIONKEY and only the N_NAME and R_NAME columns will be defined for the external table. This exercise will illustrate how data can be unloaded using SQL statements other than a simple SELECT FROM statement. The external table will be named ET_NATION_REGION using another ASCII delimited text file named et_nation_file_flat_file. Unload data from both the REGION and NATION tables joined on the REGIONKEY column to list all of the countries and their associated regions. Instead of specifying the columns in the create external table statement you will use the AS SELECT option: Input [Terminal 2] create external table et_nation_region '/home/nz/labs/movingData/et_nation_region_flat_file' as select n_name, r_name from nation, region where n_regionkey=r_regionkey; Output INSERT 0 14 List the properties of the ET_NATION_REGION external table. Input [Terminal 1] \\d et_nation_region Output External Table \"ET_NATION_REGION\" Attribute | Type | Modifier -----------+---------------+---------- N_NAME | CHARACTER(25) | NOT NULL R_NAME | CHARACTER(25) | NOT NULL DataObject - '/home/nz/labs/movingData/et_nation_region_flat_file' adjustdistzeroint - bool style - 1_0 code set - compress - FALSE cr in string - ctrl chars - date delim - - date style - YMD delim - | encoding - INTERNAL escape - fill record - format - TEXT ignore zero - log dir - /tmp max errors - 1 max rows - null value - NULL quoted value - NO remote source - require quotes - skip rows - socket buf size - 8388608 timedelim - : time round nanos - time style - 24HOUR trunc string - y2base - includezeroseconds - record length - record delimiter - nullindicator bytes - layout - decimaldelim - disablenfc - includeheader - datetime delim - meridian delim - lfinstring - You will notice that the external table was created using the two columns specified in the SELECT clause: N_NAME and R_NAME. View the data of the ET_NATION_REGION external table. Input [Terminal 1] select * from et_nation_region; Output N_NAME | R_NAME ---------------------------+--------------------------- canada | na united states | na brazil | sa guyana | sa venezuela | sa united kingdom | emea portugal | emea united arab emirates | emea south africa | emea australia | ap japan | ap macau | ap hong kong | ap new zealand | ap (14 rows) This is the result of the joining the NATION and REGION table on the REGIONKEY column to return just the N_NAME and R_NAME columns. Using the second session, review the file that was created, et_nation_region_flat_file, in the /home/nz/labs/movingData directory: Input [Terminal 2] more et_nation_region_flat_file Output canada|na united states|na brazil|sa guyana|sa venezuela|sa united kingdom|emea portugal|emea united arab emirates|emea south africa|emea australia|ap japan|ap macau|ap hong kong|ap new zealand|ap We created a flat delimited flat file from a complex SQL statement. External tables are a very flexible and powerful way to load, unload and transfer data.","title":"5.1.4 (Optional) Unloading data with an External Table from two tables"},{"location":"nz-05-Loading-and-Unloading-Data/#515-optional-unloading-data-with-an-external-table-using-the-compress-format","text":"In the previous exercises, the external tables were created used the default ASCII delimited text format. In this exercise, the external table will be similar to the second external table that was created, but instead of the using an ASCII delimited text format we will use the compressed binary format. The name of the external table will be ET4_REGION and the datasource file name will be et4_region_compress. Sample Syntax CREATE EXTERNAL TABLE table_name 'filename' USING (COMPRESS true FORMAT 'internal') AS select_statement; Create an external table using a similar method that you used to create the second external table, in section 2.1.2. But instead of using an ASCII delimited-text format, the datasource will use the compressed binary format. This is achieved by using the COMPRESS and FORMAT external table options: Input [Terminal 1] create external table et4_region '/home/nz/labs/movingData/et4_region_compress' using (compress true format 'internal') as select * from region; Output INSERT 0 4 As a reminder, the external table is created, and the data is unloaded in the same operation using the AS SELECT clause. List the properties of the ET4_REGION external table Input [Terminal 1] \\d et4_region Output External Table \"ET4_REGION\" Attribute | Type | Modifier -------------+------------------------+---------- R_REGIONKEY | INTEGER | NOT NULL R_NAME | CHARACTER(25) | NOT NULL R_COMMENT | CHARACTER VARYING(152) | DataObject - '/home/nz/labs/movingData/et4_region_compress' adjustdistzeroint - bool style - code set - compress - TRUE cr in string - ctrl chars - date delim - date style - delim - encoding - escape - fill record - format - INTERNAL ignore zero - log dir - max errors - 1 max rows - null value - quoted value - remote source - require quotes - skip rows - socket buf size - 8388608 timedelim - time round nanos - time style - trunc string - y2base - includezeroseconds - record length - record delimiter - nullindicator bytes - layout - decimaldelim - disablenfc - includeheader - datetime delim - meridian delim - lfinstring - Notice that the option for COMPRESS has changed from FALSE to TRUE indicating that the datasource file is compressed, and the FORMAT has changed from TEXT to INTERNAL which is required for compressed files.","title":"5.1.5 (Optional) Unloading data with an External Table using the compress format"},{"location":"nz-05-Loading-and-Unloading-Data/#52-dropping-external-tables","text":"Dropping external tables is similar to dropping a regular Netezza Performance Server table. The column definition for the external table is removed from the Netezza Performance Server catalog. Keep in mind that dropping the table doesn't delete the external datasource file so they also have to be maintained, but the external datasource file can still be used for loading data into a different table. In this exercise you will drop the ET1_REGION table, but you will not delete the associated external datasource file, et1_region_flat_file. This datasource file will be used later in this lab to load data into the REGION table. Drop the first external table that you created, ET1_REGION, using the DROP TABLE command: Input [Terminal 1] drop table et1_region; Output DROP TABLE The same drop command for tables is used for external tables. There is not a separate DROP EXTERNAL TABLE command. Verify the external table has been dropped using the internal slash option, \\dx, to list all of the external tables. Input [Terminal 1] \\dx Output List of relations Schema | Name | Type | Owner --------+------------------+----------------+---------- ADMIN | ET2_REGION | EXTERNAL TABLE | LABADMIN ADMIN | ET3_REGION | EXTERNAL TABLE | LABADMIN ADMIN | ET4_REGION | EXTERNAL TABLE | LABADMIN ADMIN | ET4_TEST | EXTERNAL TABLE | LABADMIN ADMIN | ET_NATION_REGION | EXTERNAL TABLE | LABADMIN (5 rows) Notice the remaining external tables that you created still exist. Even though the external table definition no longer exists within the LABDB database, the flat file named et1_region_flat_file still exists in the /home/nz/labs/movingData directory. Verify this by using the second putty session: Input [Terminal 2] ls Output et1_region_flat_file et3_region_flat_file et4_region_flat_file et2_region_flat_file et4_region_compress et_nation_region_flat_file Notice that the file et1_REGION_flat_file still exists. This file can still be used to load data into another similar table.","title":"5.2 Dropping External Tables"},{"location":"nz-05-Loading-and-Unloading-Data/#53-loading-data-using-external-tables","text":"External tables can also be used to load data into tables in the database. In this exercise, data will be loaded into the REGION table, after first removing the existing rows. The method to load data from external tables into a table is similar to using the DML INSERT INTO and SELECT FROM statements. We will use two different methods to load data into the REGION table, one using an external table and the other using the external datasource file. Loading data into a table from any external table will generate an associated log file with a default name of <table_name>.<database_name>.log Before loading the data into the REGION table, delete the rows from the data using the [TRUNCATE TABLE command: Input [Terminal 1] truncate table region; Output TRUNCATE TABLE Verify the table is empty with the SELECT * command: Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+--------+----------- (0 rows) Load data into the REGION table from the ET2_REGION external table using an INSERT statement: Input [Terminal 1] insert into region select * from et2_region; Output INSERT 0 4 Check to ensure that the table contains the four rows using the SELECT * statement. Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific (4 rows) Again, delete the rows in the REGION table: Input [Terminal 1] truncate table region; Output TRUNCATE TABLE Check to ensure that the table is empty using the SELECT * statement. Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+--------+----------- (0 rows) Load data into the REGION table using the ASCII delimited file that was created for external table ET1_REGION. Recall that the definition of the external table was removed from that database, but the external data source file, et1_region_flat_file, still exists: Input [Terminal 1] insert into region select * from external '/labs/movingData/et1_region_flat_file'; Output INSERT 0 4 Verify the table contains the four rows using the SELECT * statement. Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific (4 rows) Since this is a load operation, there is always an associated log file <table>.<database>.nzlog created for each load performed. By default this log file is created in the /tmp directory. In the second Putty session review this file: Input [Terminal 2] more /tmp/REGION.ADMIN.LABDB.nzlog Output Load started at:08-May-13 07:10:13 EDT Database: LABDB Tablename: REGION Datafile: /labs/movingData/etl_region_flat_file ... Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' Time Style: 24HOUR Time Delim: ':' Time extra zeros: No Statistics number of records read: 4 number of bad records: 0 number of records loaded: 4 Elapsed Time (sec): 0.0 Load completed at: 08-May-13 07:10:13 EDT Notice that the log file contains the load options and statistics of the load, along with environment information to identify the table.","title":"5.3 Loading Data using External Tables"},{"location":"nz-05-Loading-and-Unloading-Data/#6-loading-data-using-the-nzload-utility","text":"The nzload command is a SQL CLI client application that allows you to load data from the local host or a remote client, on all the supported client platforms. The nzload command processes command-line load options to send queries to the host to create an external table definition, runs the insert/select query to load data, and when the load completes, drops the external table. The nzload command is a command-line program that accepts options from multiple sources, where some of the sources can be from: Command line Control file NZ Environment Variables Without a control file, you can only do one load at a time. Using a control file allows multiple loads. The nzload command connects to a database with a username and password, just like any other Netezza Performance Server client application. The username specifies an account with a particular set of privileges, and the system uses this account to verify access. For this section of the lab you will continue to use the LABADMIN user to load data into the LABDB database. The nzload utility will be used to load records from an external datasource file into the REGION table. The nzload log files will be reviewed to examine the nzload options. Since you will be loading data into a populated REGION table, you will use the TRUNCATE TABLE command to remove the rows from the table. We will continue to use the two putty sessions from the external table lab. Session One, which is connected to the NZSQL console to execute SQL commands, for example to review tables after load operations Session Two, which will be used for operating system commands such as execute nzload.","title":"6 Loading Data using the nzload Utility"},{"location":"nz-05-Loading-and-Unloading-Data/#61-using-the-nzload-utility-with-command-line-options","text":"The first method for using the nzload utility to load data in the REGION table will specify options at the command line. We will only need to specify the datasource file and we will use default options for the rest. The datasource file will be the et1_region_flat_file that you created in the External Tables section. Sample Syntax nzload --db <database> -u <username> --pw <password> -df <datasource filename> As the LABDB database owner, LABADMIN first remove the rows in the REGION table: Input [Terminal 1] truncate table region; Output TRUNCATE TABLE Verify the rows have been removed from the table using the SELECT * statement: Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+--------+----------- (0 rows) Using the second session at the OS command line, use the nzload utility to load data from the et1_region_flat file into the REGION table using the following command line options: Sample Syntax -db <database name>, -u <user>, -pw <password>, -t <table name>, -df <data file>, and --delimiter <string>: Input [Terminal 2] nzload -db labdb -u labadmin -pw password -t region -df et1_region_flat_file -delimiter '|' Output Load session of table 'REGION' completed successfully Verify the rows have been load into the table by using the SELECT * statement: Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific (4 rows) These rows were loaded into the REGION table from the records in the et1_region_flat_file file. For every load task performed there is always an associated log file created with the format <table>.<db>.nzlog . By default, this log file is created in the current working directory, which is the /home/nz/labs/movingData directory. In the second session review this file: Input [Terminal 2] [more REGION.ADMIN.LABDB.nzlog Output Load started at:03-Apr-20 03:05:12 PDT Database: LABDB Schema: ADMIN Tablename: REGION Datafile: /home/nz/labs/movingData/et1_region_flat_file Host: localhost.localdomain Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Statistics number of records read: 4 number of bytes read: 91 number of bad records: 0 ------------------------------------------------- number of records loaded: 4 Elapsed Time (sec): 0.0 ----------------------------------------------------------------------------- Load completed at: 03-Apr-20 03:05:12 PDT ============================================================================= Notice the log file contains the load options and statistics of the load, along with environment information to identify the database and table. The --db, -u, and --pw, options specify the database name, the user, and the password respectively. Alternatively, you could omit these options if the NZ environment variables are set to the appropriate database, username and password values. Since the NZ environment variables, NZ_DATABASE, NZ_USER, and NZ_PASSWORD are set to system, admin, and password, we will need to use these options so the load will use the LABDB database and the LABADMIN user. There are other options that you can use with the nzload utility. These options were not specified here since the default values were sufficient for this load task. -t specifies the target table name in the database -df specifies the datasource file to be loaded -delimiter specifies the string to use as the delimiter in an ASCII delimited text file. The following nzload command syntax is equivalent to the nzload command we used above. It is intended to demonstrate some of the options that can be used with the nzload command, or can be omitted when default values are acceptable. Sample Syntax nzload --db labdb --u labadmin --pw password --t region --df et1_region_flat_file --delimiter '|' --outputDir '<current directory>' --lf <table>.<database>.nzlog --bf<table>.<database>.nzlog --compress false --format text --maxErrors 1 The --lf, -bf, and --maxErrors options are explained in the next exercise. The --compress and --format options indicate that the datasource file is an ASCII delimited text file. For a compressed binary datasource file the following options would be used: -compress true --format internal.","title":"6.1 Using the nzload Utility with Command Line Options"},{"location":"nz-05-Loading-and-Unloading-Data/#62-using-the-nzload-utility-with-a-control-file","text":"As demonstrated in section 3.1 you can run the nzload command by specifying the command line options or you can use another method by specifying the options in a file, referred to as a control file. This is useful because the file can be modified over time, since loading data into a database for a data warehouse environment is a continuous operation. A nzload control file has the following basic structure: Sample Syntax DATAFILE <filename> { [<option name> <option value>] } The --cf option is used at the nzload command line to indicate the use of a control file: Sample Syntax nzload --u <username> -pw <password> -cf <control file> The --u and --pw options are optional if the NZ_USER and NZ_PASSWORD environment variables are set to the appropriate user and password. Using the --u and --pw options overrides the values in the NZ environment variables. In this session you will load rows into an empty REGION table using the nzload utility with a control file. The control file will set the following options: delimiter, logDir, logFile, and badFile, along with the database and table name. The datasource file to be used in this session is the region.del file. As the LABDB database owner, LABADMIN first remove the rows in the REGION table: Input [Terminal 1] truncate table region; Output TRUNCATE TABLE Verify the rows have been removed from the table using the SELECT * statement. The table should contain no rows. Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+--------+----------- (0 rows) The control file will be used by the nzload utility to load data into the REGION table using the region.del data file. The control file has already been created in the lab directory. A control file can include the following options: Parameter Value Database Database name Tablename Table name Delimiter Delimiter string LogDir Log directory LogFile Log file name BadFile Bad record log file name 1) Review the control file in the second putty session with the following command: Input [Terminal 2] more control_file Output DATAFILE /home/nz/labs/movingData/region.del { Database labdb Tablename region Delimiter '|' LogDir '/home/nz/labs/movingData' LogFile region.log BadFile region.bad } 2) Load the data using the nzload utility and the control file you just reviewed. Input [Terminal 2] nzload -u labadmin -pw password -cf control_file Output Load session of table 'REGION' completed successfully 3) The nzload log file was renamed using the information in the control file. The log file name was changed from the default to region.log and the location was changed from the /tmp directory to /labs/. Check the nzload log. Input [Terminal 2] more region.log Output Load started at:03-Apr-20 03:38:32 PDT Database: LABDB Schema: ADMIN Tablename: REGION Datafile: /home/nz/labs/movingData/region.del Host: localhost.localdomain Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Statistics number of records read: 4 number of bytes read: 91 number of bad records: 0 ------------------------------------------------- number of records loaded: 4 Elapsed Time (sec): 0.0 ----------------------------------------------------------------------------- Load completed at: 03-Apr-20 03:38:32 PDT 4) Verify the rows in the REGION table in the first putty session with the nzsql console: Input [Terminal 1] select * from region; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific (4 rows)","title":"6.2 Using the nzload Utility with a Control File."},{"location":"nz-05-Loading-and-Unloading-Data/#63-optional-using-nzload-with-bad-records","text":"The first two load methods illustrated how to use the nzload utility to load data into an empty table using command line options or a control file. In a data warehousing environment, most of the time data is incrementally added to a table already containing some rows. There will be instances where records from a datasource might not match the datatypes in the table. When this occurs, the load will abort when the first bad record is encountered. This is the default behavior and is controlled by the maxErrors option, which is set to a default value of 1. For this exercise we will add additional rows to the NATION table. Since we will be adding rows to the NATION table, there will be no need to truncate the table. The datasource file we will be using is the nation.del file, which unfortunately has a bad record. First check the NATION table by listing all of the rows in the table using the SELECT * statement in the first putty session: Input [Terminal 1] select * from nation; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand (14 rows) Using the second session at the OS command line you will use the nzload utility to load data from the nation.del file into the NATION: Input [Terminal 1] nzload -db LABDB -u labadmin -pw password -t nation -df nation.del -delimiter '|' Output Error: Operation canceled Error: External Table : count of bad input rows reached maxerrors limit See /home/nz/labs/movingData/NATION.ADMIN.LABDB.nzlog file Error: Load Failed, records not inserted. This is an indication that the load has failed due to a bad record in the datasource file. Since the load has failed no rows were loaded into the NATION table, which you can confirm by using the SELECT * statement (in the first session): Input [Terminal 1] select * from nation; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand (14 rows) In the second session, check the log file to determine the problem: Input [Terminal 2] more NATION.ADMIN.LABDB.nzlog Output Load started at:03-Apr-20 04:31:19 PDT Database: LABDB Schema: ADMIN Tablename: NATION Datafile: <stdin> Host: localhost.localdomain Load Options Field delimiter: '\\t' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Load started at:03-Apr-20 04:36:02 PDT Database: LABDB Schema: ADMIN Tablename: NATION Datafile: /home/nz/labs/movingData/nation.del Host: localhost.localdomain Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 1 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Found bad records bad #: input row #(byte offset to last char examined) [field #, declaration] diagnostic, \"text consumed\"[last char examined] ---------------------------------------------------------------------------------------------------------------------------- 1: 10(1) [1, INT4] expected field delimiter or end of record, \"2\"[t] Statistics number of records read: 10 number of bytes read: 226 number of bad records: 1 ------------------------------------------------- number of records loaded: 0 Elapsed Time (sec): 0.0 ----------------------------------------------------------------------------- Load completed at: 03-Apr-20 04:36:02 PDT ============================================================================= The Statistics section indicates that 10 records were read before the bad record was encountered during the load process. As expected, no rows were inserted into the table since the default is to abort the load when one bad record is encountered. The log file also provides information about the bad record: Sample Output Found bad records bad #: input row #(byte offset to last char examined) [field #, declaration] diagnostic, \"text consumed\"[last char examined] ---------------------------------------------------------------------------------------------------------------------------- 1: 10(1) [1, INT4] expected field delimiter or end of record, \"2\"[t] Using the log file, we are able to determine the problem is that the value '2t' is in a field for an INT(4) column. Since '2t' is not a valid integer, the load marked this as a bad record 10(1) indicates the input record number within the file and the offset within the row where a problem was encountered. For this example, the input record is 10 and offset is 1. [1, INT(4)] indicates the column number within the row and the data for the column. For this example, the column number is 1 and the data type is INT(4). \"2\"[t] indicates the character that caused the problem. For this example, the character is 2t. We can verify our problem determination for the load failure is correct by examining the nation.del datasource file that was used for the load. In the second session execute the following command: Input [Terminal 2] more nation.del Output 15|andorra|2|andorra 16|ascension islan|3|ascension 17|austria|3|osterreich 18|bahamas|2|bahamas 19|barbados|2|barbados 20|belgium|3|belqique 21|chile|2|chile 22|cuba|2|cuba 23|cook islands|4|cook islands 2t|denmark|3|denmark 25|ecuador|2|ecuador 26|falkland islands|3|islas malinas 27|fiji|4|fiji 28|finland|3|suomen tasavalta 29|greenland|1|kalaallit nunaat 30|great britain|3|great britian 31|gibraltar|3|gibraltar 32|hungary|3|magyarorszag 33|iceland|3|lyoveldio island 34|ireland|3|eire 35|isle of man|3|isle of man 36|jamaica|2|jamaica 37|korea|4|han-guk 38|luxembourg|3|Luxembourg Notice on the 10th line the output. There is indeed an invalid 2t in the first column of the input file. Therefore, we made the correct assumption that the '2t' is causing the problem. From this list you can assume that the correct value should be 24. Alternatively, we could have examined the nzload bad log file NATION.LABDB.nzbad, which will contain all bad records that are encountered during a load. In the second session execute the following command: Input [Terminal 2] more NATION.ADMIN.LABDB.nzbad Output 2t|denmark|3|Denmark This is the same data as identified in the nation.del file and using the log file information to locate the record. Since the default is to stop the load after the first bad record is processed there is only one row in the bad log file. If we were to change the default behavior to allow more bad records to be processed, this file could potentially contain more records. It provides a comfortable overview of all the records that created exceptions during load. We have the option of changing the NATION.del file to change '2t' to '24' and then rerun the same nzload command as in step 7. Instead you will rerun a similar load but you will allow 10 bad records to be encountered during the load process. To change the default behavior, you need to use the command option -maxErrors. You will also change the name of the nzbad file using the --bf command option and the log filename using the --lf command option: Input [Terminal 2] nzload -db labdb -u labadmin -pw password -t nation -df nation.del -delimiter '|' -maxerrors 10 -bf nation.bad -lf nation.log Output Load session of table 'NATION' completed successfully Now the load is successful. 6) Verify the newly loaded rows are in the NATION using the SELECT * command: Input [Terminal 1] select * from nation order by n_nationkey; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand 15 | andorra | 2 | andorra 16 | ascension islan | 3 | ascension 17 | austria | 3 | osterreich 18 | bahamas | 2 | bahamas 19 | barbados | 2 | barbados 20 | belgium | 3 | belqique 21 | chile | 2 | chile 22 | cuba | 2 | cuba 23 | cook islands | 4 | cook islands 25 | ecuador | 2 | ecuador 26 | falkland islands | 3 | islas malinas 27 | fiji | 4 | fiji 28 | finland | 3 | suomen tasavalta 29 | greenland | 1 | kalaallit nunaat 30 | great britain | 3 | great britian 31 | gibraltar | 3 | gibraltar 32 | hungary | 3 | magyarorszag 33 | iceland | 3 | lyoveldio island 34 | ireland | 3 | eire 35 | isle of man | 3 | isle of man 36 | jamaica | 2 | jamaica 37 | korea | 4 | han-guk 38 | luxembourg | 3 | luxembourg 39 | monaco | 3 | monaco (38 rows) Now all of the new records were loaded except for the one bad row with nation key 24. 7) Even though the nzload command received a successful message it is good practice to review the nzload log file for any problems, for example bad rows that are under the maxErrors threshold. In the second putty session execute the following command: Note xxxx is the ID associated to your nzload. Input [Terminal 2] more NATION.ADMIN.LABDB.xxxx.nzbad The log file should be similar to the following. Output Load started at:03-Apr-20 06:12:43 PDT Database: LABDB Schema: ADMIN Tablename: NATION Datafile: /home/nz/labs/movingData/nation.del Host: localhost.localdomain Load Options Field delimiter: '|' NULL value: NULL File Buffer Size (MB): 8 Load Replay Region (MB): 0 Encoding: INTERNAL Max errors: 10 Skip records: 0 Max rows: 0 FillRecord: No Truncate String: No Escape Char: None Accept Control Chars: No Allow CR in string: No Ignore Zero: No Quoted data: NO Require Quotes: No BoolStyle: 1_0 Decimal Delimiter: '.' Disable NFC: No Date Style: YMD Date Delim: '-' DateTime Delim: ' ' Time Style: 24HOUR Time Delim: ':' Record Delim: '\\n' Meridian Delim: ' ' Time extra zeros: No LfInString: False Found bad records bad #: input row #(byte offset to last char examined) [field #, declaration] diagnostic, \"text consumed\"[last char examined] ---------------------------------------------------------------------------------------------------------------------------- 1: 10(1) [1, INT4] expected field delimiter or end of record, \"2\"[t] Statistics number of records read: 25 number of bytes read: 607 number of bad records: 1 ------------------------------------------------- number of records loaded: 24 Elapsed Time (sec): 0.0 ----------------------------------------------------------------------------- Load completed at: 03-Apr-20 06:12:43 PDT ============================================================================= The main difference as compared with the example before, is that all 25 of the data records in the data source file were processed, but only 24 records were loaded because there was one bad record in the data source file. Correct the bad row and load it into the NATION table. There are couple options you could use. One option is to extract the bad row from the original data source file and create a new data source file with the correct record. However, this task could be tedious when dealing with large data source files and potentially many bad records. The other option, which is more appropriate, is to use the bad log file. All of the bad records that cannot be loaded into the table are placed in the bad log file. In the second session use vi to open and edit the nation.bad file and change the '2t' to '24' in the first field. The vi editor has two modes, a command mode used to save files, quit the editor etc. and an insert mode. Initially you will be in the command mode. To change the file, you need to switch into the insert mode by pressing \"i\". The editor will show an -- INSERT -- at the bottom of the screen. Note: you can use gedit from the VM desktop (gedit nation.bad) Input [Terminal 2] vi nation.bad Output before changes 2t|denmark|3|Denmark 8) You can now use the cursor keys to navigate. Change the first two chars of the bad row from 2t to 24. Your screen should look like the following: Output after changes 24|denmark|3|Denmark -- INSERT -- 9) To save the changes, press \"Esc\" to switch back into command mode. You should see that the ---INSERT--- string at the bottom of the screen vanishes. Enter :wq! and press enter to write the file, and quit the editor. 10) After the nation.bad file has modified to correct the record, issue a nzload to load the modified nation.bad file: Input [Terminal 2] nzload -db labdb -u labadmin -pw password -t nation -df nation.bad -delimiter '|' Output Load session of table 'NATION' completed successfully 11) Verify the new row has been loaded into the table: Input [Terminal 1] select * from nation order by n_nationkey; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand 15 | andorra | 2 | andorra 16 | ascension islan | 3 | ascension 17 | austria | 3 | osterreich 18 | bahamas | 2 | bahamas 19 | barbados | 2 | barbados 20 | belgium | 3 | belqique 21 | chile | 2 | chile 22 | cuba | 2 | cuba 23 | cook islands | 4 | cook islands 24 | denmark | 3 | denmark 25 | ecuador | 2 | ecuador 26 | falkland islands | 3 | islas malinas 27 | fiji | 4 | fiji 28 | finland | 3 | suomen tasavalta 29 | greenland | 1 | kalaallit nunaat 30 | great britain | 3 | great britian 31 | gibraltar | 3 | gibraltar 32 | hungary | 3 | magyarorszag 33 | iceland | 3 | lyoveldio island 34 | ireland | 3 | eire 35 | isle of man | 3 | isle of man 36 | jamaica | 2 | jamaica 37 | korea | 4 | han-guk 38 | luxembourg | 3 | luxembourg 39 | monaco | 3 | monaco (39 rows) The row in bold denotes the new row that was added to the table, which was the bad record you corrected. Congratulations you have completed the lab.","title":"6.3 (Optional) Using nzload with Bad Records"},{"location":"nz-06-BNR/","text":"1 Backup and Restore Regular backups of your user databases and system catalogs should be taken as part of any data warehouse continuity strategy. One reason to take backups is for disaster recovery, for example in case of a fire in the data center. Another reason is to undo changes such as accidental deletes. For disaster recovery, backups should be stored in a different physical location than the data center that hosts the data warehouse. IBM Netezza Performance Server provides several backup and restore methods to cover your various requirements. The Netezza Performance Server backup and restore operations can use network file system locations and several third-party solutions such as IBM Spectrum Protect (formerly Tivoli\u00ae Storage Manager), Veritas NetBackup, and EMC NetWorker as destinations. 1.1 Objectives In the previous labs we created our LABDB database and loaded the data into it. In this lab we will set up a QA database that contains a subset of the tables and data of the full database. To create the tables, we will use Cross-Database-Access from our QA database to the LABDB production database. Next we will use the schema-only function of nzbackup to create a test database that contains the same tables and data objects as the QA database, but no data. Test data will later be added specifically for testing needs. After that we will do a multistep backup of our QA database and test the restore functionality. Testing backups by restoring them is generally a good idea and should be done during the development phase and also at regular intervals. After all, you are never fully sure what a backup contains until you restore it. Finally, we will backup the system user data and the host data. While a database backup saves all users and groups that are involved in that database, a full user backup may be needed to get the full picture, for example to archive users and groups that are no longer used in any database. Host data should be backed up regularly so you can restore the Performance Server data directory from the host backup without the additional time to restore all of the databases. 2 Lab Virtual Machine This lab system will be a virtual machine running on Virtual Box. Please see the document on how to install the IPS Virtual Machine for your workstation (Windows or Mac OS). 3 Creating a QA Database In this lab we will create a QA database called LABDBQA, which contains a subset of the tables. It will contain all of the data from the NATION and REGION tables, along with a subset of the data from the CUSTOMER table. We will create our QA database, connect to it and use CTAS (Create Table As) tables to create the table copies. We will use cross-database access to create our CTAS tables from the LABDB database. This is possible since Netezza Performance Server allows read-only cross database access if fully qualified names are used. In this lab we will regularly switch between the operating system prompt and the NZSQL console. The operating system prompt will be used to execute the backup and restore commands and review the created files. The NZSQL console will be used to create the tables and further review the changes made to the user data using the restore commands. To make this easier you should open two Terminal sessions (Terminal 1 and Terminal 2), the first one will be used to execute the operating system commands and it will be referred to as the OS session, in the second session we will start the NZSQL console. It will be referred to as the NZSQL session. You can also see which session to use from the command prompt in the screenshots. Figure 2 The two terminal sessions for this lab, OS session 1 (Terminal 1) on the left, nzsql session 2 (Terminal 2) on the right Open the first Terminal session. Login to as user nz with password nz. ( is the default IP address for a local VM, the IP may be different for your Bootcamp) Access the lab directory for this lab with the following command: Input [Terminal 1]: cd ~/labs/backupRestore/setupLab ./setupLab.sh Output DROP DATABASE ERROR: DROP DATABASE: object LABDBQA does not exist. ERROR: DROP DATABASE: object LABDBTEST does not exist. CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully Open the second Terminal session. Login to <your-nps-vm-ip-address> as user nz with password nz. <your-nps-vm-ip-address> is the default IP address for a local VM, the IP may be different for your Bootcamp) Access the lab directory for this lab with the same change directory command: Input [Terminal 2]: cd ~/labs/backupRestore/ Output [ nz@localhost backupRestore ] $ Start the nzsql console using the nzsql command: Input [Terminal 2]: nzsql Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM.ADMIN ( ADMIN )= > Info This will connect you to the SYSTEM database with the ADMIN user. These are the default settings stored in the environment variables of the NZ user. Create the empty QA database using the CREATE DATABASE command: Input [Terminal 2]: create database LABDBQA ; Output CREATE DATABASE Connect to the QA database using the \\c command. Input [Terminal 2]: \\c LABDBQA Output You are now connected to database LABDBQA. Create a full copy of the REGION table from the LABDB database: Input [Terminal 2]: create table region as select * from labdb.admin.region ; Output INSERT 0 4 With the CTAS statement, we created a local REGION table in the currently connected LABDBQA database with the same definition and content as the REGION table from the LABDB database. The [CREATE TABLE AS statement is one of the most flexible administrative tools for an IBM Netezza Performance Server administrator. We can easily access tables of databases we are currently not connected to, but only for read operations. We cannot insert data into a database we are not connected to. Verify that the content has been copied over correctly. View the original data in the LABDB database: Input [Terminal 2]: select * from labdb.admin.region order by 1 ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific ( 4 rows ) You should see four rows in the result set. To access a table from a foreign database, we need to have the fully qualified name. Notice that we include the schema name between the two dots. Schemas are fully supported in Performance Server and since each table name needs to be unique in a given database it should be included. Now let's compare that to our local REGION table using the SELECT statement: Input [Terminal 2]: select * from labdbqa.admin.region order by 1 ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific ( 4 rows ) You should see the same rows as before. Now copy over the NATION table: Input [Terminal 2]: create table nation as select * from labdb.admin.nation ; Output INSERT 0 14 Finally, we will copy over a subset of our CUSTOMER table, copying only the rows from the automobile market segment into the QA database: Input [Terminal 2]: create table customer as select * from labdb.admin.customer where c_mktsegment = 'AUTOMOBILE' ; Output INSERT 0 29752 Notice that this inserts 29,752 rows into the QA customer table, roughly a fifth of the original table. Next create a view NATIONSBYREGION which returns a list of nation names with their corresponding region names. This is used in a couple of applications: Input [Terminal 2]: create view nationsbyregions as select r_name, n_name from nation, region where r_regionkey = n_regionkey ; Output CREATE VIEW Let's have a look at what the view returns: Input [Terminal 2]: select * from nationsbyregions order by 1 ; Output R_NAME | N_NAME ---------------------------+--------------------------- ap | australia ap | macau ap | japan ap | hong kong ap | new zealand emea | united arab emirates emea | portugal emea | united kingdom emea | south africa na | united states na | canada sa | brazil sa | guyana sa | venezuela ( 14 rows ) You should get a list of all nations and their corresponding region name. Views are a very convenient way to hide SQL complexity. They can also be used to implement column level security by creating views of tables that only contain a subset of columns. They are fully supported by Netezza Performance Server. Verify the created tables with the display tables command: Input [Terminal 2]: \\d t Output List of relations Schema | Name | Type | Owner --------+----------+-------+------- ADMIN | CUSTOMER | TABLE | ADMIN ADMIN | NATION | TABLE | ADMIN ADMIN | REGION | TABLE | ADMIN ( 3 rows ) Notice that the QA database only contains the three tables we just created. Next create a QA user and make them owner of the database. Create a user qauser: Input [Terminal 2]: create user qauser ; Output CREATE USER Make the id qauser owner of the QA database called labdbqa: Input [Terminal 2]: alter database labdbqa owner to qauser ; Output ALTER DATABASE We have successfully created our QA database using cross access database CTAS statements. Our QA database contains three tables and a view, and we have a user that is the owner of this database. In the next section we will use backup and restore to create an empty copy of the QA database for the test database. Creating the Test Database In this section, we will use schema-only backup and restore to create an empty copy of the QA database as test database. This will not contain any data since the developers will fill it with test-specific data. Schema only backup is a convenient way to recreate databases without user data. We need to specify three parameters to the nzbackup command, the database we want to backup, the file system location where we want to save the backup files to and the --schema-only parameter to specify that user data shouldn't be backed up. Warning Normally backups should be saved on a remote network file server, not on the host hard disks. Not only is this essential for disaster recovery, but the host hard disks are small, optimized for speed and not intended to hold large amount of data. They are strictly intended for Netezza Performance Server software and operational data. Switch to the OS session and create the schema only backup of our QA database: Input [Terminal 1]: nzbackup -schema-only -db labdbqa -dir /tmp/bkschema Output Backup of schema for database labdbqa completed successfully. Later in this Lab we will have a deeper look at the files and the logs the backup command created. We can restore a database to a different database name. We simply need to specify the new name in the --db parameter and the old name in the --sourcedb parameter. Now we will restore the test database from this backup: Input [Terminal 1]: nzrestore -dir /tmp/bkschema -db labdbtest -sourcedb labdbqa -schema-only Output Restore of schema for database labdbtest completed successfully. In the nzsql session we will verify that we successfully created an empty copy of our database. See all available databases with the following command: \\l Input [Terminal 2]: \\l Output List of databases DATABASE | OWNER -----------+---------- LABDB | LABADMIN LABDBQA | QAUSER LABDBTEST | QAUSER SYSTEM | ADMIN ( 4 rows ) Notice that the LABDBTEST database was successfully created and the privilege information has been copied as well, the owner is QAUSER, which is the same as the owner of the LABDBQA database. We do not want the QA user being the owner of the test database, change the owner to ADMIN for now: Input [Terminal 2]: alter database labdbtest owner to admin ; Output ALTER DATABASE Now let's check the contents of the test database. First connect to the database with the \\c command, and then display the database objects with the \\d command Input [Terminal 2]: \\c labdbtest Output You are now connected to database labdbtest. Verify the test database contains all the objects of the QA database: \\d Input [Terminal 2]: \\d Output List of relations Schema | Name | Type | Owner --------+------------------+-------+------- ADMIN | CUSTOMER | TABLE | ADMIN ADMIN | NATION | TABLE | ADMIN ADMIN | NATIONSBYREGIONS | VIEW | ADMIN ADMIN | REGION | TABLE | ADMIN ( 4 rows ) You will see the three tables and the view we created. Performance Server Backup saves all database objects including views, stored procedures, and more. It also includes all users, groups and privileges that refer to the database included in the backup. Since we used the --schema-only option we have not copied any data. Verify this for the NATION table using the SELECT command: Input select * from nation ; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+--------+-------------+----------- ( 0 rows ) Notice the result set is empty as expected. The schema-only backup option is a convenient way to save your database schema and to create empty copies of your database. It will create a full 1:1 copy of the original database without the user data. You could also restore the database to a different Netezza Performance Server. This would only require that the backup server location is accessible from both Netezza Performance Servers. The target server can be another Netezza Performance Server, which can be a different model type or have a later software release. Backing up and Restoring a Database Netezza Performance Server's user data backup will create a backup of the complete database, including all database objects and user data. Even global objects like users and privileges that are used in the database are backed up. This makes backup and restore a very easy and straightforward process. Since Netezza Performance Server doesn't have a transaction log, point in time restore is not possible. Therefore, frequent backups are advisable. NPS supports full, differential and cumulative backups that allow easy and fast regular data backups. An example backup strategy would be monthly full backups, weekly cumulative backups and daily differentials. Netezza Performance Server is not intended to be used nor has been designed as an OLTP database, therefore this should provide enough backup flexibility for most situations. For example, run differential backups after the daily ETL processes that feed the warehouse. Figure 3 A typical backup strategy In this section we will create a backup of our QA database. We will then do a differential backup and then do a restore. Warning Our VMWare environment has some specific restrictions that only allow the restoration of up to 2 increments. The labs will work correctly but don't be surprised if you encounter errors during restore operations of more than 2 increments. Backing up the Database Netezza Performance Server's backup is organized in so called backup sets. Every new full backup creates a new backup set. Differential and cumulative backups are added to the last backup set by default. They can also be added to a different backup set as well. In this section we will switch between the two Terminal sessions. In the OS session execute the following command to create a full backup of the QA database: Input [Terminal 1]: nzbackup -db labdbqa -dir /tmp/bk1 /tmp/bk2 Output Backup of database labdbqa to backupset\ufeff20210331133512 completed successfully. This command will create a full user data backup of the LABDBQA database. Each backup set has a unique id that can be later used to access it. By default, the last active backup set is used for restore and differential backups. Info In this lab we split up the backup between two file system locations. You can specify up to 16 file system locations after the --dir parameter. Alternatively, you could use a directory list file as well with the --dirfile option. Splitting up the backup between different file servers will result in higher backup performance. In the NZSQL session we will now add a new row to the REGION table. First connect to the QA database using the \\c command: Input [Terminal 2]: \\c labdbqa Output You are now connected to database labdbqa. Now add a new entry for the north pole to the REGION table: Input [Terminal 2]: insert into region values ( 5 , 'np' , 'north pole' ) ; Output INSERT 0 1 Now create a differential backup with the --differential option. This will create a new entry to the backup set we created previously only containing the differences since the full backup. In the OS session create a differential backup: Input [Terminal 1]: nzbackup -db labdbqa -dir /tmp/bk1 /tmp/bk2 -differential Output Backup of database labdbqa to backupset \ufeff20210331133512 completed successfully. Notice that the backup set id hasn't changed. In the NZSQL session add the south pole to the REGION table: Input [Terminal 2]: insert into region values ( 6 , 'sp' , 'south pole' ) ; Output INSERT 0 1 You have now one full backup with the original 4 rows in the REGION table, a differential backup that has additionally the north pole entry and a current state that has in addition to that the south pole region. Verifying the Backups In this section we will have a closer look at the files and logs that are created during the Netezza Performance Server Backup process. In the OS session display the backup history of your Netezza Performance Server. Input [Terminal 1]: nzbackup -history Output Database Backupset Seq # OpType Status Date Log File -------- -------------- ----- ------- --------- ------------------- ------------------------------ LABDBQA 20210331133344 1 NO DATA COMPLETED 2021 -03-31 06 :33:44 backupsvr.30436.2021-03-31.log LABDBQA 20210331133512 1 FULL COMPLETED 2021 -03-31 06 :35:12 backupsvr.30648.2021-03-31.log LABDBQA 20210331133512 2 DIFF COMPLETED 2021 -03-31 06 :36:00 backupsvr.30859.2021-03-31.log Netezza Performance Server keeps track of all backups and saves them in the system catalog. This is used for differential backups and it is also integrated with the Groom process. Since Performance Server doesn't use transaction logs it needs logically deleted rows for differential backups. By default, Groom doesn't remove a logically deleted row that has not been backed up yet. Therefore, the Groom process is integrated with the backup history. We will explain this in more detail in the Transaction and Groom Labs. We have done three backups on our server. One backup set contains the schema only backup, two backups for the second backup set, one full backup and one differential. Let's have a closer look at the log that has been generated for the last differential backup. In the OS session, switch to the log directory of the backupsrv process, which is the process responsible for backing up data: Input [Terminal 1]: [ cd /nz/kit/log/backupsvr Output [ nz@localhost backupsvr ] $ Info The /nz/kit/log directory contains the log directories for all Performance Server processes. Display the end of the log for the last differential backup process. You will need to replace the XXX values with the actual values of your log. You can cut and paste the log name from the history output above. We are interested in the last differential backup process: Input [Terminal 1]: tail backupsvr.xxxxx.xxxx-xx-xx.log Output 2021 -03-31 06 :36:04.879861 PDT ( 30859 ) Info: [ 30882 ] Postgres client pid: 30884 , session: 16111 2021 -03-31 06 :36:04.883271 PDT ( 30859 ) Info: [ 30883 ] Postgres client pid: 30885 , session: 16112 2021 -03-31 06 :36:04.892053 PDT ( 30859 ) Info: Capturing deleted rows 2021 -03-31 06 :36:04.892145 PDT ( 30859 ) Info: Backing up table ADMIN.REGION 2021 -03-31 06 :36:06.115265 PDT ( 30859 ) Info: Wrote 5569 bytes of metadata and udx files in less than a second 2021 -03-31 06 :36:06.115362 PDT ( 30859 ) Info: Operation committed 2021 -03-31 06 :36:06.115381 PDT ( 30859 ) Info: Wrote 72 bytes in less than one second to location 1 2021 -03-31 06 :36:06.115384 PDT ( 30859 ) Info: Wrote 193 bytes in less than one second to location 2 2021 -03-31 06 :36:06.115434 PDT ( 30859 ) Info: Backup of database labdbqa to backupset 20210331133512 completed successfully. 2021 -03-31 06 :36:06.123928 PDT ( 30859 ) Info: NZ-00023: --- program 'backupsvr' ( 30859 ) exiting on host 'localhost.localdomain' ... You can see that the process backed up the three tables REGION, NATION and CUSTOMER and wrote the result to two different locations. You also see the amount of data written to these locations. Since we only added a single row the amount of data is tiny. If you look at the log of the full backup you will see a lot more data being written. Now let's have a look at the files that are created during the backup process, enter the first backup location: Input [Terminal 1]: cd /tmp/bk1 Output [ nz@localhost bk1 ] $ Display the contents of the directory using ls -l. Input [Terminal 1]: ls -l Output drwxrwxrwx. 3 nz nz 35 Mar 31 04 :43 Netezza The directory contains all backup sets for all Performance Servers that use this backup location. If you need to move the backup you always have to move the complete folder. Change directories into the Netezza folder, with cd Netezza, and display the contents with ls -l. Input [Terminal 1]: cd Netezza ls -l Output total 0 drwxrwxrwx. 3 nz nz 21 Mar 31 04 :43 localhost.localdomain Under the main Netezza folder you will find sub folders for each Netezza host that is backed up to this location. In our case we only have one Netezza host called \"netezza\". But if your company had multiple Netezza hosts you would find them here. Change directories to the localhost.localdomain folder with the cd command and display the contents with [ll: Input [Terminal 1]: cd localhost.localdomain ls -l Output drwxrwxrwx. 3 nz nz 28 Mar 31 04 :43 LABDBQA Change directories to the LABDBQA folder with cd and display the contents with ls -l: Input [Terminal 1]: cd LABDBQA/ ls -l Output total 0 drwxrwxrwx. 4 nz nz 24 Apr 5 11 :44 \ufeff20210331133512 You will find all the databases of the host that have been backed up to this location, in our case the QA database. In this folder you can see all the backup sets that have been saved for this database. Each backup set corresponds to one full backup plus an optional set of differential and cumulative backups. Note that we backed up the schema to a different location, so we only have one backup set in here. Change directories to the backup set folder with cd and display the contents with ll: Input [Terminal 1]: cd \ufeff20210331133512 / ls -l Output total 0 drwxrwxrwx. 3 nz nz 18 Mar 31 06 :35 1 drwxrwxrwx. 3 nz nz 18 Mar 31 04 :36 2 Under the backup set are folders for each backup that has been added to that backup set. \"1\" is always the full backup followed by additional differential or cumulative backups. We will later use these numbers to restore our database to a specific backup of the backup set. Change directories to the full backup with cd and display the contents with ll: Input [Terminal 1]: cd 1 ls -l Output total 0 drwxrwxrwx. 4 nz nz 28 Mar 31 06 :43 FULL As expected, it's a full backup. Change directories to the FULL folder with cd and display the contents with ls -l: Input [Terminal 1]: cd FULL ls -l Output total 0 drwxrwxrwx. 2 nz nz 94 Mar 31 06 :35 data drwxrwxrwx. 3 nz nz 92 Mar 31 06 :35 md The data folder contains the user data, the md folder contains metadata including the schema definition of the database. Change directories to the data folder with the cd command and display detailed information with ll: Input [Terminal 1]: cd data ls -l Output total 8 total 1128 -rw-------. 1 nz nz 337 Mar 31 04 :43 202287 .full.2.1 -rw-------. 1 nz nz 449 Mar 31 04 :43 202289 .full.2.1 -rw-------. 1 nz nz 1140681 Mar 31 04 :43 202291 .full.2.1 -rw-------. 1 nz nz 1 Mar 31 04 :43 data.marker Now switch to the md folder using cd ../md and display the contents with ll: Input [Terminal 1]: cd ../md ls -l Output total 1120 -rw-------. 1 nz nz 338 Mar 31 06 :35 200580 .full.2.1 -rw-------. 1 nz nz 451 Mar 31 06 :35 200582 .full.2.1 -rw-------. 1 nz nz 1132441 Mar 31 06 :35 200584 .full.1.1 -rw-------. 1 nz nz 1 Mar 31 06 :35 data.marker This folder contains information about the files that contribute to the backup and the schema definition of the database in the schema.xml Let's have a quick look inside the schema.xml file: Input [Terminal 1]: more schema.xml Output <ARCHIVE archive_major = \"4\" archive_minor = \"0\" archive_subminor = \"1\" product_ver = \"Release 11.0.3.1 [Build 0]\" catalog_ver = \"3.1792\" hostname = \"localhost.localdomain\" dataslices = \"1\" createtime = \"2020-04-05 18:42:23\" lowercase = \"f\" objidcycle = \"0\" hpfrel = \"3.2\" model = \"sim\" family = \"sim\" platform = \"sim\" > <OPERATION backupset = \"20200405184220\" increment = \"1\" predecessor = \"0\" optype = \"0\" dbname = \"LABDBQA\" /> <DATABASE name = \"LABDBQA\" schema = \"\" owner = \"QAUSER\" oidhi = \"0\" oid = \"200820\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" charset = \"LATIN9\" collation = \"BINARY\" collecth ist = \"t\" replcsn = \"0\" replsrcid = \"0\" repldbid = \"0\" replsetid = \"0\" defschema = \"ADMIN\" defschemadelim = \"f\" dbtrackchanges = \"1\" > <SCHEMA name = \"ADMIN\" schema = \"\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200819\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" path = \"\" /> <STATISTICS column_count = \"15\" /> <TABLE ver = \"2\" name = \"REGION\" schema = \"ADMIN\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200821\" classhi = \"0\" class = \"4905\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" rowsecurity = \"f\" origoidhi = \"0\" ori goid = \"200821\" excludebackup = \"0\" > <COLUMN name = \"R_REGIONKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203501\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"1\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" notnull = \"t\" /> <COLUMN name = \"R_NAME\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203502\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"2\" type = \"CHARACTER(25)\" typeno = \"1042\" typemod = \"4 1\" notnull = \"t\" /> <COLUMN name = \"R_COMMENT\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203503\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"3\" type = \"CHARACTER VARYING(152)\" typeno = \"1043 \" typemod = \"168\" notnull = \"f\" /> <DISTRIBUTION seq = \"1\" attnum = \"1\" /> </TABLE> <TABLE ver = \"2\" name = \"NATION\" schema = \"ADMIN\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200823\" classhi = \"0\" class = \"4905\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" rowsecurity = \"f\" origoidhi = \"0\" ori goid = \"200823\" excludebackup = \"0\" > <COLUMN name = \"N_NATIONKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203510\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"1\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" notnull = \"t\" /> <COLUMN name = \"N_NAME\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203511\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"2\" type = \"CHARACTER(25)\" typeno = \"1042\" typemod = \"4 1\" notnull = \"t\" /> <COLUMN name = \"N_REGIONKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203512\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"3\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" notnull = \"t\" /> <COLUMN name = \"N_COMMENT\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203513\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"4\" type = \"CHARACTER VARYING(152)\" typeno = \"1043 \" typemod = \"168\" notnull = \"f\" /> <DISTRIBUTION seq = \"1\" attnum = \"1\" /> </TABLE> <TABLE ver = \"2\" name = \"CUSTOMER\" schema = \"ADMIN\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200825\" classhi = \"0\" class = \"4905\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" rowsecurity = \"f\" origoidhi = \"0\" o rigoid = \"200825\" excludebackup = \"0\" > <COLUMN name = \"C_CUSTKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203520\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"1\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" no tnull = \"t\" /> <COLUMN name = \"C_NAME\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203521\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"2\" type = \"CHARACTER VARYING(25)\" typeno = \"1043\" ty pemod = \"41\" notnull = \"t\" /> <COLUMN name = \"C_ADDRESS\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203522\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"3\" type = \"CHARACTER VARYING(40)\" typeno = \"1043\" typemod = \"56\" notnull = \"t\" /> <COLUMN name = \"C_NATIONKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203523\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"4\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" notnull = \"t\" /> <COLUMN name = \"C_PHONE\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203524\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"5\" type = \"CHARACTER(15)\" typeno = \"1042\" typemod = \" 31\" notnull = \"t\" /> <COLUMN name = \"C_ACCTBAL\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203525\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"6\" type = \"NUMERIC(15,2)\" typeno = \"1700\" typemod = \"983058\" notnull = \"t\" /> <COLUMN name = \"C_MKTSEGMENT\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203526\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"7\" type = \"CHARACTER(10)\" typeno = \"1042\" type mod = \"26\" notnull = \"t\" /> <COLUMN name = \"C_COMMENT\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203527\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"8\" type = \"CHARACTER VARYING(117)\" typeno = \"1043 \" typemod = \"133\" notnull = \"t\" /> <DISTRIBUTION seq = \"1\" attnum = \"1\" /> </TABLE> <VIEW name = \"NATIONSBYREGIONS\" schema = \"ADMIN\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200827\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" definition = \"SELECT REGION.R_NAME, N ATION.N_NAME FROM ADMIN.NATION, ADMIN.REGION WHERE (REGION.R_REGIONKEY = NATION.N_REGIONKEY);\" /> <USER name = \"QAUSER\" schema = \"\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200829\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" pwd = \"\" rowsetlimit = \"0\" validuntil = \"\" sesstimeout = \" 0\" qrytimeout = \"0\" defp = \"NONE\" maxp = \"NONE\" pwdinv = \"f\" pwdlastchged = \"\" resrcgrpid = \"4901\" crossjoin = \"NULL\" collecthist = \"0\" accesstime = \"0\" concursess = \"0\" seclabel = \"PUBLIC::\" audit cat = \"NONE\" useauth = \"0\" > </USER> <RSG username = \"QAUSER\" userdelim = \"f\" groupname = \"PUBLIC\" groupdelim = \"f\" /> </DATABASE> </ARCHIVE> As you see this file contains a full XML description of your database, including table definition, views, users etc. Switch back to the lab folder: Input [Terminal 1]: cd ~/labs/backupRestore/ Output [ nz@localhost backupRestore ] $ You should now have a pretty good understanding of the Performance Server Backup process. In the next section we will demonstrate the restore functionality. Restoring the Database In this section, we will restore our database to the first increment and then we will upgrade our database to the next increment. Performance Server allows you to return a database to a specific increment in your backup set. If you want to do an incremental restore, the database must be locked. Tables can be queried but not changed until the database is in the desired state and unlocked. In the NZSQL session we will drop the QA database and the QA user. First connect to the SYSTEM database: Input [Terminal 2]: \\c SYSTEM Output You are now connected to database SYSTEM. SYSTEM.ADMIN ( ADMIN )= > Now drop the QA database: Input [Terminal 2]: DROP DATABASE LABDBQA ; Output DROP DATABASE Now drop the QA User: Input [Terminal 2]: DROP USER QAUSER ; Output DROP USER Verify that the QA database has been deleted by displaying the databases using the \\l command. You will see that the LABDBQA database has been removed: Input [Terminal 2]: \\l Output List of databases DATABASE | OWNER -----------+---------- LABDB | LABADMIN LABDBTEST | ADMIN SYSTEM | ADMIN ( 3 rows ) In the OS session, restore the database to the first increment: Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -increment 1 -lockdb true Output Restore of increment 1 from backupset \ufeff20210331133512 to database 'labdbqa' committed. Notice that we have specified the increment with the --increment option. In our case this is the first full backup in our backup set. We did not have to specify a backup set, by default the most recent one is used. Since we are not sure which increment we want to restore the database to, we have to lock the database with the -lockdb option. This allows only read-only access until the desired increment has been restored. In the NZSQL session, verify that the database has been recreated with \\l Input [Terminal 2]: \\l Output List of databases DATABASE | OWNER -----------+---------- LABDB | LABADMIN LABDBQA | QAUSER LABDBTEST | ADMIN SYSTEM | ADMIN ( 4 rows ) Notice the LABDBQA database is there. You can also see that the owner QAUSER has been recreated and is again the database owner. Connect to the LABDBQA database with the \\c command Input [Terminal 2]: \\c labdbqa Output NOTICE: Database 'LABDBQA' is available for read-only You are now connected to database labdbqa. Notice that LABDBQA database is currently in read-only mode. Verify the contents of the REGION table from the LABDBQA database: Input [Terminal 2]: select * from region order by 1 ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific ( 4 rows ) Notice that we have returned the database to the point in time after the first full backup. There is no north or south pole in the R_COMMENT column. Try to insert a row to verify the read only mode: Input [Terminal 2]: insert into region values ( 5 , 'np' , 'north pole' ) ; Output ERROR: Database 'LABDBQA' is available for read-only ( command ignored ) As expected, this is prohibited until we unlock the database. In the OS session, apply the next increment to the database Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -increment next -lockdb true Output Restore of increment 2 from backupset \ufeff20210331133512 to database 'labdbqa' committed. Notice that we now apply the second increment to the database. Since we do not need to load any more increments, we can now unlock the database: Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -unlockdb Output [ nz@localhost backupRestore ] $ After the database unlock, we cannot apply any further increments to this database. To jump to a different increment, we would need to start from the beginning. In the nzsql session we will look at the REGION table again: Input [Terminal 2]: select * from region order by 1 ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific 5 | np | north pole ( 5 rows ) Notice that we have added the north pole region, which was created before the first differential backup. Verify that the database is unlocked and ready for use again by adding a new set of customers to the CUSTOMER table. In addition to the Automobile users we want to add the machinery users from the main database: Input [Terminal 2]: insert into customer select * fromlabdb.admin.customer where c_mktsegment = 'MACHINERY' ; Output INSERT 0 29949 Notice that we now can use the database in a normal fashion again. We had around 30000 customers before, verify that the new user set has been added successfully: Input [Terminal 2]: select count ( * ) from customer ; Output COUNT ------- 59701 ( 1 row ) You will see that we now have around 60000 rows in the CUSTOMER table. You have now done a full restore cycle for the database and applied a full and incremental backup to your database. In the next section we will demonstrate single table restore and the ability to restore from any backup set. Single Table Restore In this chapter we will demonstrate the targeted restore of a subset of tables from a backup set. We will also demonstrate how to restore from a specific older backup set. First, we will create a second backup set with the new customer data. In the OS session execute the following command: Input [Terminal 1]: nzbackup -db labdbqa -dir /tmp/bk1 /tmp/bk2 Output Backup of database labdbqa to backupset \ufeff20210331134349 completed successfully. Notice that this is a full database backup because we took the defaults. In this case Netezza Performance Server automatically creates a new backup set. We want to return the CUSTOMER table to the previous condition. But we do not want to change the REGION or the NATION tables. To do this we need to know the backup set id of the previous backup set. To do this execute the history command again: Input [Terminal 1]: nzbackup -history Output Database Backupset Seq # OpType Status Date Log File --------- -------------- ----- ------- --------- ------------------- ------------------------------ ( LABDBQA ) 20210331133344 1 NO DATA COMPLETED 2021 -03-31 06 :33:44 backupsvr.30436.2021-03-31.log ( LABDBQA ) 20210331133512 1 FULL COMPLETED 2021 -03-31 06 :35:12 backupsvr.30648.2021-03-31.log ( LABDBQA ) 20210331133512 2 DIFF COMPLETED 2021 -03-31 06 :36:00 backupsvr.30859.2021-03-31.log LABDBQA 20210331134349 1 FULL COMPLETED 2021 -03-31 06 :43:49 backupsvr.31972.2021-03-31.log We now see three different backup sets, the schema only backup, the two step backupset and the new full backupset. Remember the backup set id of the two step backupset. To return only the CUSTOMER table to its condition of the second backup set, we can do a table level restore with the following command: Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -backupset \ufeff20210331133512 -tables CUSTOMER Output Error: Specify -droptables to force drop of tables in the -tables list. This command will only restore the tables in the --tables option. If you want to restore multiple tables, you can simply write them in a list after the option. We use the --backupset option to specify a specific backup set. Remember to replace the id with the value you retrieved with the history command. Info Notice that the table name needs to be case sensitive. This is in contrast to the database name. You will get the error \"Performance Server cannot restore a table that exists in the target database. You can either drop the table before restoring it or use the --droptables option.\" Repeat the previous command with the added --droptables option: Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -backupset 20210331133512 -tables CUSTOMER -droptables Output [ Restore Server ] : Dropping TABLE 'CUSTOMER' [ Restore Server ] : Dropping TABLE 'CUSTOMER' Restore of increment 1 from backupset 20210331133512 to database 'labdbqa' committed. Restore of increment 2 from backupset 20210331133512 to database 'labdbqa' committed. Notice that the target table was dropped before the restore happened and the specified backup set was used. Since we didn't stipulate a specific increment, the full backup set has been applied with all increments. Also, the table is automatically unlocked after the restore process finishes. Finally let's verify that the restore worked as expected, in the NZSQL console count the rows of the customer table again: Input [Terminal 2]: select count ( * ) from customer ; Output COUNT ------- 29752 ( 1 row ) You will see that we are back to approximately 30000 rows. This means that we have backed out the most recent changes. In this chapter you executed a single table restore and you did a restore from a specific backup set. Backing up User Data and Host Data In the previous chapters you have learned to backup Performance Server databases. This backs up all the database objects that are used in the database and the user data. These are the most critical components to back up in a Performance Server. They will allow you to recreate your databases even if you switch to a completely new Netezza Performance Server. There are two other things that should be backed up: The global user information. The host data In this chapter you will do a backup of these components, so you would be able to revert your IBM Netezza Performance Server to the exact condition it was in before the backup. User Data Backup Users, groups, and privileges that are not used in databases will not be backed up by the user data backup. To be able to revert a Performance Server completely to its original condition you need to have a backup of the global user information as well, to capture for example administrative users that are not part of any database. This is done with the --users option of the [nzbackup command: In the OS session execute the following command. Input [Terminal 2]: nzbackup -dir /tmp/bk1 /tmp/bk2 -users Output Backup of global objects completed successfully. This will create a backup of all Users, Groups and Privileges. Restoring it will not delete any users, instead it will only add missing Users, Groups and Privileges, so it doesn't need to be fully synchronized with the user data backup. You can even restore an older user backup without fear of destroying information. Host Data Backup Until now we have always backed up database content. This is essentially catalog and user data that can be applied to a new Performance Server. Performance Server also provides the functionality to backup and restore host data. This is essentially the data in the /nz/data and /export/nz directories of the host server. There are two reasons for regularly backing up host data. The first is a host crash. If the data on your system is intact but the host file system has been destroyed, you can recreate all databases from the user backup. In very large systems this might take a long time. It is much easier to just restore the host information and reconnect to the undamaged user tables. The second reason is that the host data contains configuration information, log and plan files etc. that are not saved by the user backup. If you changed the system configuration that information would be lost. Therefore, it is advisable to regularly backup host data. To backup the host data execute the following command in the OS session: Input [Terminal 2]: nzhostbackup /tmp/hostbackup Output Starting host backup. System state is 'online' . Pausing the system ... Checkpointing host catalog ... Archiving system catalog ... Resuming the system ... Host backup completed successfully. System state is 'online' . As you can see, the system was paused for the duration of the host backup but is automatically resumed after the backup is successful. Also notice that the host backup is done with the nzhostbackup command instead of the standard nzbackup command. Let's have a look at the created file: Input [Terminal 2]: ls -l /tmp Output total 389308 drwxrwxrwx. 3 nz nz 21 Apr 5 11 :42 bk1 drwxrwxrwx. 3 nz nz 21 Apr 5 11 :42 bk2 drwxrwxrwx. 3 nz nz 21 Apr 5 11 :37 bkschema -rw-r--r--. 1 root root 36730 Apr 6 03 :54 cyclops_run.log -rw-------. 1 nz nz 398447694 Apr 6 03 :52 hostbackup ... Notice that a backup file has been created. It's a compressed file containing the system catalog and Performance Server host information. If possible, host backups should be done regularly. If an old host backup is restored, there might be some orphaned tables. Orphan tables are tables that were created after the host backup and consume disk space but are not registered in the system catalog anymore. During host restore, Performance Server will create a script to clean up these orphaned tables. Congratulations you have finished the Backup & Restore lab and you have had a chance to see all components of a successful Performance Server backup strategy. In this lab, we used the file system backup. In a real Performance Server backup and restore operation, we would most likely use third-party solutions such as IBM Spectrum Protect (formerly Tivoli\u00ae Storage Manager), Veritas NetBackup, and EMC NetWorker as destinations. For further information regarding the setup steps please refer to the IBM Knowledge Center for Netezza Performance Server.","title":"Backup and Restore"},{"location":"nz-06-BNR/#1-backup-and-restore","text":"Regular backups of your user databases and system catalogs should be taken as part of any data warehouse continuity strategy. One reason to take backups is for disaster recovery, for example in case of a fire in the data center. Another reason is to undo changes such as accidental deletes. For disaster recovery, backups should be stored in a different physical location than the data center that hosts the data warehouse. IBM Netezza Performance Server provides several backup and restore methods to cover your various requirements. The Netezza Performance Server backup and restore operations can use network file system locations and several third-party solutions such as IBM Spectrum Protect (formerly Tivoli\u00ae Storage Manager), Veritas NetBackup, and EMC NetWorker as destinations.","title":"1 Backup and Restore"},{"location":"nz-06-BNR/#11-objectives","text":"In the previous labs we created our LABDB database and loaded the data into it. In this lab we will set up a QA database that contains a subset of the tables and data of the full database. To create the tables, we will use Cross-Database-Access from our QA database to the LABDB production database. Next we will use the schema-only function of nzbackup to create a test database that contains the same tables and data objects as the QA database, but no data. Test data will later be added specifically for testing needs. After that we will do a multistep backup of our QA database and test the restore functionality. Testing backups by restoring them is generally a good idea and should be done during the development phase and also at regular intervals. After all, you are never fully sure what a backup contains until you restore it. Finally, we will backup the system user data and the host data. While a database backup saves all users and groups that are involved in that database, a full user backup may be needed to get the full picture, for example to archive users and groups that are no longer used in any database. Host data should be backed up regularly so you can restore the Performance Server data directory from the host backup without the additional time to restore all of the databases.","title":"1.1 Objectives"},{"location":"nz-06-BNR/#2-lab-virtual-machine","text":"This lab system will be a virtual machine running on Virtual Box. Please see the document on how to install the IPS Virtual Machine for your workstation (Windows or Mac OS).","title":"2 Lab Virtual Machine"},{"location":"nz-06-BNR/#3-creating-a-qa-database","text":"In this lab we will create a QA database called LABDBQA, which contains a subset of the tables. It will contain all of the data from the NATION and REGION tables, along with a subset of the data from the CUSTOMER table. We will create our QA database, connect to it and use CTAS (Create Table As) tables to create the table copies. We will use cross-database access to create our CTAS tables from the LABDB database. This is possible since Netezza Performance Server allows read-only cross database access if fully qualified names are used. In this lab we will regularly switch between the operating system prompt and the NZSQL console. The operating system prompt will be used to execute the backup and restore commands and review the created files. The NZSQL console will be used to create the tables and further review the changes made to the user data using the restore commands. To make this easier you should open two Terminal sessions (Terminal 1 and Terminal 2), the first one will be used to execute the operating system commands and it will be referred to as the OS session, in the second session we will start the NZSQL console. It will be referred to as the NZSQL session. You can also see which session to use from the command prompt in the screenshots. Figure 2 The two terminal sessions for this lab, OS session 1 (Terminal 1) on the left, nzsql session 2 (Terminal 2) on the right Open the first Terminal session. Login to as user nz with password nz. ( is the default IP address for a local VM, the IP may be different for your Bootcamp) Access the lab directory for this lab with the following command: Input [Terminal 1]: cd ~/labs/backupRestore/setupLab ./setupLab.sh Output DROP DATABASE ERROR: DROP DATABASE: object LABDBQA does not exist. ERROR: DROP DATABASE: object LABDBTEST does not exist. CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully Open the second Terminal session. Login to <your-nps-vm-ip-address> as user nz with password nz. <your-nps-vm-ip-address> is the default IP address for a local VM, the IP may be different for your Bootcamp) Access the lab directory for this lab with the same change directory command: Input [Terminal 2]: cd ~/labs/backupRestore/ Output [ nz@localhost backupRestore ] $ Start the nzsql console using the nzsql command: Input [Terminal 2]: nzsql Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM.ADMIN ( ADMIN )= > Info This will connect you to the SYSTEM database with the ADMIN user. These are the default settings stored in the environment variables of the NZ user. Create the empty QA database using the CREATE DATABASE command: Input [Terminal 2]: create database LABDBQA ; Output CREATE DATABASE Connect to the QA database using the \\c command. Input [Terminal 2]: \\c LABDBQA Output You are now connected to database LABDBQA. Create a full copy of the REGION table from the LABDB database: Input [Terminal 2]: create table region as select * from labdb.admin.region ; Output INSERT 0 4 With the CTAS statement, we created a local REGION table in the currently connected LABDBQA database with the same definition and content as the REGION table from the LABDB database. The [CREATE TABLE AS statement is one of the most flexible administrative tools for an IBM Netezza Performance Server administrator. We can easily access tables of databases we are currently not connected to, but only for read operations. We cannot insert data into a database we are not connected to. Verify that the content has been copied over correctly. View the original data in the LABDB database: Input [Terminal 2]: select * from labdb.admin.region order by 1 ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific ( 4 rows ) You should see four rows in the result set. To access a table from a foreign database, we need to have the fully qualified name. Notice that we include the schema name between the two dots. Schemas are fully supported in Performance Server and since each table name needs to be unique in a given database it should be included. Now let's compare that to our local REGION table using the SELECT statement: Input [Terminal 2]: select * from labdbqa.admin.region order by 1 ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific ( 4 rows ) You should see the same rows as before. Now copy over the NATION table: Input [Terminal 2]: create table nation as select * from labdb.admin.nation ; Output INSERT 0 14 Finally, we will copy over a subset of our CUSTOMER table, copying only the rows from the automobile market segment into the QA database: Input [Terminal 2]: create table customer as select * from labdb.admin.customer where c_mktsegment = 'AUTOMOBILE' ; Output INSERT 0 29752 Notice that this inserts 29,752 rows into the QA customer table, roughly a fifth of the original table. Next create a view NATIONSBYREGION which returns a list of nation names with their corresponding region names. This is used in a couple of applications: Input [Terminal 2]: create view nationsbyregions as select r_name, n_name from nation, region where r_regionkey = n_regionkey ; Output CREATE VIEW Let's have a look at what the view returns: Input [Terminal 2]: select * from nationsbyregions order by 1 ; Output R_NAME | N_NAME ---------------------------+--------------------------- ap | australia ap | macau ap | japan ap | hong kong ap | new zealand emea | united arab emirates emea | portugal emea | united kingdom emea | south africa na | united states na | canada sa | brazil sa | guyana sa | venezuela ( 14 rows ) You should get a list of all nations and their corresponding region name. Views are a very convenient way to hide SQL complexity. They can also be used to implement column level security by creating views of tables that only contain a subset of columns. They are fully supported by Netezza Performance Server. Verify the created tables with the display tables command: Input [Terminal 2]: \\d t Output List of relations Schema | Name | Type | Owner --------+----------+-------+------- ADMIN | CUSTOMER | TABLE | ADMIN ADMIN | NATION | TABLE | ADMIN ADMIN | REGION | TABLE | ADMIN ( 3 rows ) Notice that the QA database only contains the three tables we just created. Next create a QA user and make them owner of the database. Create a user qauser: Input [Terminal 2]: create user qauser ; Output CREATE USER Make the id qauser owner of the QA database called labdbqa: Input [Terminal 2]: alter database labdbqa owner to qauser ; Output ALTER DATABASE We have successfully created our QA database using cross access database CTAS statements. Our QA database contains three tables and a view, and we have a user that is the owner of this database. In the next section we will use backup and restore to create an empty copy of the QA database for the test database.","title":"3 Creating a QA Database"},{"location":"nz-06-BNR/#creating-the-test-database","text":"In this section, we will use schema-only backup and restore to create an empty copy of the QA database as test database. This will not contain any data since the developers will fill it with test-specific data. Schema only backup is a convenient way to recreate databases without user data. We need to specify three parameters to the nzbackup command, the database we want to backup, the file system location where we want to save the backup files to and the --schema-only parameter to specify that user data shouldn't be backed up. Warning Normally backups should be saved on a remote network file server, not on the host hard disks. Not only is this essential for disaster recovery, but the host hard disks are small, optimized for speed and not intended to hold large amount of data. They are strictly intended for Netezza Performance Server software and operational data. Switch to the OS session and create the schema only backup of our QA database: Input [Terminal 1]: nzbackup -schema-only -db labdbqa -dir /tmp/bkschema Output Backup of schema for database labdbqa completed successfully. Later in this Lab we will have a deeper look at the files and the logs the backup command created. We can restore a database to a different database name. We simply need to specify the new name in the --db parameter and the old name in the --sourcedb parameter. Now we will restore the test database from this backup: Input [Terminal 1]: nzrestore -dir /tmp/bkschema -db labdbtest -sourcedb labdbqa -schema-only Output Restore of schema for database labdbtest completed successfully. In the nzsql session we will verify that we successfully created an empty copy of our database. See all available databases with the following command: \\l Input [Terminal 2]: \\l Output List of databases DATABASE | OWNER -----------+---------- LABDB | LABADMIN LABDBQA | QAUSER LABDBTEST | QAUSER SYSTEM | ADMIN ( 4 rows ) Notice that the LABDBTEST database was successfully created and the privilege information has been copied as well, the owner is QAUSER, which is the same as the owner of the LABDBQA database. We do not want the QA user being the owner of the test database, change the owner to ADMIN for now: Input [Terminal 2]: alter database labdbtest owner to admin ; Output ALTER DATABASE Now let's check the contents of the test database. First connect to the database with the \\c command, and then display the database objects with the \\d command Input [Terminal 2]: \\c labdbtest Output You are now connected to database labdbtest. Verify the test database contains all the objects of the QA database: \\d Input [Terminal 2]: \\d Output List of relations Schema | Name | Type | Owner --------+------------------+-------+------- ADMIN | CUSTOMER | TABLE | ADMIN ADMIN | NATION | TABLE | ADMIN ADMIN | NATIONSBYREGIONS | VIEW | ADMIN ADMIN | REGION | TABLE | ADMIN ( 4 rows ) You will see the three tables and the view we created. Performance Server Backup saves all database objects including views, stored procedures, and more. It also includes all users, groups and privileges that refer to the database included in the backup. Since we used the --schema-only option we have not copied any data. Verify this for the NATION table using the SELECT command: Input select * from nation ; Output N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+--------+-------------+----------- ( 0 rows ) Notice the result set is empty as expected. The schema-only backup option is a convenient way to save your database schema and to create empty copies of your database. It will create a full 1:1 copy of the original database without the user data. You could also restore the database to a different Netezza Performance Server. This would only require that the backup server location is accessible from both Netezza Performance Servers. The target server can be another Netezza Performance Server, which can be a different model type or have a later software release.","title":"Creating the Test Database"},{"location":"nz-06-BNR/#backing-up-and-restoring-a-database","text":"Netezza Performance Server's user data backup will create a backup of the complete database, including all database objects and user data. Even global objects like users and privileges that are used in the database are backed up. This makes backup and restore a very easy and straightforward process. Since Netezza Performance Server doesn't have a transaction log, point in time restore is not possible. Therefore, frequent backups are advisable. NPS supports full, differential and cumulative backups that allow easy and fast regular data backups. An example backup strategy would be monthly full backups, weekly cumulative backups and daily differentials. Netezza Performance Server is not intended to be used nor has been designed as an OLTP database, therefore this should provide enough backup flexibility for most situations. For example, run differential backups after the daily ETL processes that feed the warehouse. Figure 3 A typical backup strategy In this section we will create a backup of our QA database. We will then do a differential backup and then do a restore. Warning Our VMWare environment has some specific restrictions that only allow the restoration of up to 2 increments. The labs will work correctly but don't be surprised if you encounter errors during restore operations of more than 2 increments.","title":"Backing up and Restoring a Database"},{"location":"nz-06-BNR/#backing-up-the-database","text":"Netezza Performance Server's backup is organized in so called backup sets. Every new full backup creates a new backup set. Differential and cumulative backups are added to the last backup set by default. They can also be added to a different backup set as well. In this section we will switch between the two Terminal sessions. In the OS session execute the following command to create a full backup of the QA database: Input [Terminal 1]: nzbackup -db labdbqa -dir /tmp/bk1 /tmp/bk2 Output Backup of database labdbqa to backupset\ufeff20210331133512 completed successfully. This command will create a full user data backup of the LABDBQA database. Each backup set has a unique id that can be later used to access it. By default, the last active backup set is used for restore and differential backups. Info In this lab we split up the backup between two file system locations. You can specify up to 16 file system locations after the --dir parameter. Alternatively, you could use a directory list file as well with the --dirfile option. Splitting up the backup between different file servers will result in higher backup performance. In the NZSQL session we will now add a new row to the REGION table. First connect to the QA database using the \\c command: Input [Terminal 2]: \\c labdbqa Output You are now connected to database labdbqa. Now add a new entry for the north pole to the REGION table: Input [Terminal 2]: insert into region values ( 5 , 'np' , 'north pole' ) ; Output INSERT 0 1 Now create a differential backup with the --differential option. This will create a new entry to the backup set we created previously only containing the differences since the full backup. In the OS session create a differential backup: Input [Terminal 1]: nzbackup -db labdbqa -dir /tmp/bk1 /tmp/bk2 -differential Output Backup of database labdbqa to backupset \ufeff20210331133512 completed successfully. Notice that the backup set id hasn't changed. In the NZSQL session add the south pole to the REGION table: Input [Terminal 2]: insert into region values ( 6 , 'sp' , 'south pole' ) ; Output INSERT 0 1 You have now one full backup with the original 4 rows in the REGION table, a differential backup that has additionally the north pole entry and a current state that has in addition to that the south pole region.","title":"Backing up the Database"},{"location":"nz-06-BNR/#verifying-the-backups","text":"In this section we will have a closer look at the files and logs that are created during the Netezza Performance Server Backup process. In the OS session display the backup history of your Netezza Performance Server. Input [Terminal 1]: nzbackup -history Output Database Backupset Seq # OpType Status Date Log File -------- -------------- ----- ------- --------- ------------------- ------------------------------ LABDBQA 20210331133344 1 NO DATA COMPLETED 2021 -03-31 06 :33:44 backupsvr.30436.2021-03-31.log LABDBQA 20210331133512 1 FULL COMPLETED 2021 -03-31 06 :35:12 backupsvr.30648.2021-03-31.log LABDBQA 20210331133512 2 DIFF COMPLETED 2021 -03-31 06 :36:00 backupsvr.30859.2021-03-31.log Netezza Performance Server keeps track of all backups and saves them in the system catalog. This is used for differential backups and it is also integrated with the Groom process. Since Performance Server doesn't use transaction logs it needs logically deleted rows for differential backups. By default, Groom doesn't remove a logically deleted row that has not been backed up yet. Therefore, the Groom process is integrated with the backup history. We will explain this in more detail in the Transaction and Groom Labs. We have done three backups on our server. One backup set contains the schema only backup, two backups for the second backup set, one full backup and one differential. Let's have a closer look at the log that has been generated for the last differential backup. In the OS session, switch to the log directory of the backupsrv process, which is the process responsible for backing up data: Input [Terminal 1]: [ cd /nz/kit/log/backupsvr Output [ nz@localhost backupsvr ] $ Info The /nz/kit/log directory contains the log directories for all Performance Server processes. Display the end of the log for the last differential backup process. You will need to replace the XXX values with the actual values of your log. You can cut and paste the log name from the history output above. We are interested in the last differential backup process: Input [Terminal 1]: tail backupsvr.xxxxx.xxxx-xx-xx.log Output 2021 -03-31 06 :36:04.879861 PDT ( 30859 ) Info: [ 30882 ] Postgres client pid: 30884 , session: 16111 2021 -03-31 06 :36:04.883271 PDT ( 30859 ) Info: [ 30883 ] Postgres client pid: 30885 , session: 16112 2021 -03-31 06 :36:04.892053 PDT ( 30859 ) Info: Capturing deleted rows 2021 -03-31 06 :36:04.892145 PDT ( 30859 ) Info: Backing up table ADMIN.REGION 2021 -03-31 06 :36:06.115265 PDT ( 30859 ) Info: Wrote 5569 bytes of metadata and udx files in less than a second 2021 -03-31 06 :36:06.115362 PDT ( 30859 ) Info: Operation committed 2021 -03-31 06 :36:06.115381 PDT ( 30859 ) Info: Wrote 72 bytes in less than one second to location 1 2021 -03-31 06 :36:06.115384 PDT ( 30859 ) Info: Wrote 193 bytes in less than one second to location 2 2021 -03-31 06 :36:06.115434 PDT ( 30859 ) Info: Backup of database labdbqa to backupset 20210331133512 completed successfully. 2021 -03-31 06 :36:06.123928 PDT ( 30859 ) Info: NZ-00023: --- program 'backupsvr' ( 30859 ) exiting on host 'localhost.localdomain' ... You can see that the process backed up the three tables REGION, NATION and CUSTOMER and wrote the result to two different locations. You also see the amount of data written to these locations. Since we only added a single row the amount of data is tiny. If you look at the log of the full backup you will see a lot more data being written. Now let's have a look at the files that are created during the backup process, enter the first backup location: Input [Terminal 1]: cd /tmp/bk1 Output [ nz@localhost bk1 ] $ Display the contents of the directory using ls -l. Input [Terminal 1]: ls -l Output drwxrwxrwx. 3 nz nz 35 Mar 31 04 :43 Netezza The directory contains all backup sets for all Performance Servers that use this backup location. If you need to move the backup you always have to move the complete folder. Change directories into the Netezza folder, with cd Netezza, and display the contents with ls -l. Input [Terminal 1]: cd Netezza ls -l Output total 0 drwxrwxrwx. 3 nz nz 21 Mar 31 04 :43 localhost.localdomain Under the main Netezza folder you will find sub folders for each Netezza host that is backed up to this location. In our case we only have one Netezza host called \"netezza\". But if your company had multiple Netezza hosts you would find them here. Change directories to the localhost.localdomain folder with the cd command and display the contents with [ll: Input [Terminal 1]: cd localhost.localdomain ls -l Output drwxrwxrwx. 3 nz nz 28 Mar 31 04 :43 LABDBQA Change directories to the LABDBQA folder with cd and display the contents with ls -l: Input [Terminal 1]: cd LABDBQA/ ls -l Output total 0 drwxrwxrwx. 4 nz nz 24 Apr 5 11 :44 \ufeff20210331133512 You will find all the databases of the host that have been backed up to this location, in our case the QA database. In this folder you can see all the backup sets that have been saved for this database. Each backup set corresponds to one full backup plus an optional set of differential and cumulative backups. Note that we backed up the schema to a different location, so we only have one backup set in here. Change directories to the backup set folder with cd and display the contents with ll: Input [Terminal 1]: cd \ufeff20210331133512 / ls -l Output total 0 drwxrwxrwx. 3 nz nz 18 Mar 31 06 :35 1 drwxrwxrwx. 3 nz nz 18 Mar 31 04 :36 2 Under the backup set are folders for each backup that has been added to that backup set. \"1\" is always the full backup followed by additional differential or cumulative backups. We will later use these numbers to restore our database to a specific backup of the backup set. Change directories to the full backup with cd and display the contents with ll: Input [Terminal 1]: cd 1 ls -l Output total 0 drwxrwxrwx. 4 nz nz 28 Mar 31 06 :43 FULL As expected, it's a full backup. Change directories to the FULL folder with cd and display the contents with ls -l: Input [Terminal 1]: cd FULL ls -l Output total 0 drwxrwxrwx. 2 nz nz 94 Mar 31 06 :35 data drwxrwxrwx. 3 nz nz 92 Mar 31 06 :35 md The data folder contains the user data, the md folder contains metadata including the schema definition of the database. Change directories to the data folder with the cd command and display detailed information with ll: Input [Terminal 1]: cd data ls -l Output total 8 total 1128 -rw-------. 1 nz nz 337 Mar 31 04 :43 202287 .full.2.1 -rw-------. 1 nz nz 449 Mar 31 04 :43 202289 .full.2.1 -rw-------. 1 nz nz 1140681 Mar 31 04 :43 202291 .full.2.1 -rw-------. 1 nz nz 1 Mar 31 04 :43 data.marker Now switch to the md folder using cd ../md and display the contents with ll: Input [Terminal 1]: cd ../md ls -l Output total 1120 -rw-------. 1 nz nz 338 Mar 31 06 :35 200580 .full.2.1 -rw-------. 1 nz nz 451 Mar 31 06 :35 200582 .full.2.1 -rw-------. 1 nz nz 1132441 Mar 31 06 :35 200584 .full.1.1 -rw-------. 1 nz nz 1 Mar 31 06 :35 data.marker This folder contains information about the files that contribute to the backup and the schema definition of the database in the schema.xml Let's have a quick look inside the schema.xml file: Input [Terminal 1]: more schema.xml Output <ARCHIVE archive_major = \"4\" archive_minor = \"0\" archive_subminor = \"1\" product_ver = \"Release 11.0.3.1 [Build 0]\" catalog_ver = \"3.1792\" hostname = \"localhost.localdomain\" dataslices = \"1\" createtime = \"2020-04-05 18:42:23\" lowercase = \"f\" objidcycle = \"0\" hpfrel = \"3.2\" model = \"sim\" family = \"sim\" platform = \"sim\" > <OPERATION backupset = \"20200405184220\" increment = \"1\" predecessor = \"0\" optype = \"0\" dbname = \"LABDBQA\" /> <DATABASE name = \"LABDBQA\" schema = \"\" owner = \"QAUSER\" oidhi = \"0\" oid = \"200820\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" charset = \"LATIN9\" collation = \"BINARY\" collecth ist = \"t\" replcsn = \"0\" replsrcid = \"0\" repldbid = \"0\" replsetid = \"0\" defschema = \"ADMIN\" defschemadelim = \"f\" dbtrackchanges = \"1\" > <SCHEMA name = \"ADMIN\" schema = \"\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200819\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" path = \"\" /> <STATISTICS column_count = \"15\" /> <TABLE ver = \"2\" name = \"REGION\" schema = \"ADMIN\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200821\" classhi = \"0\" class = \"4905\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" rowsecurity = \"f\" origoidhi = \"0\" ori goid = \"200821\" excludebackup = \"0\" > <COLUMN name = \"R_REGIONKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203501\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"1\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" notnull = \"t\" /> <COLUMN name = \"R_NAME\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203502\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"2\" type = \"CHARACTER(25)\" typeno = \"1042\" typemod = \"4 1\" notnull = \"t\" /> <COLUMN name = \"R_COMMENT\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203503\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"3\" type = \"CHARACTER VARYING(152)\" typeno = \"1043 \" typemod = \"168\" notnull = \"f\" /> <DISTRIBUTION seq = \"1\" attnum = \"1\" /> </TABLE> <TABLE ver = \"2\" name = \"NATION\" schema = \"ADMIN\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200823\" classhi = \"0\" class = \"4905\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" rowsecurity = \"f\" origoidhi = \"0\" ori goid = \"200823\" excludebackup = \"0\" > <COLUMN name = \"N_NATIONKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203510\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"1\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" notnull = \"t\" /> <COLUMN name = \"N_NAME\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203511\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"2\" type = \"CHARACTER(25)\" typeno = \"1042\" typemod = \"4 1\" notnull = \"t\" /> <COLUMN name = \"N_REGIONKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203512\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"3\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" notnull = \"t\" /> <COLUMN name = \"N_COMMENT\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203513\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"4\" type = \"CHARACTER VARYING(152)\" typeno = \"1043 \" typemod = \"168\" notnull = \"f\" /> <DISTRIBUTION seq = \"1\" attnum = \"1\" /> </TABLE> <TABLE ver = \"2\" name = \"CUSTOMER\" schema = \"ADMIN\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200825\" classhi = \"0\" class = \"4905\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" rowsecurity = \"f\" origoidhi = \"0\" o rigoid = \"200825\" excludebackup = \"0\" > <COLUMN name = \"C_CUSTKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203520\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"1\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" no tnull = \"t\" /> <COLUMN name = \"C_NAME\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203521\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"2\" type = \"CHARACTER VARYING(25)\" typeno = \"1043\" ty pemod = \"41\" notnull = \"t\" /> <COLUMN name = \"C_ADDRESS\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203522\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"3\" type = \"CHARACTER VARYING(40)\" typeno = \"1043\" typemod = \"56\" notnull = \"t\" /> <COLUMN name = \"C_NATIONKEY\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203523\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"4\" type = \"INTEGER\" typeno = \"23\" typemod = \"-1\" notnull = \"t\" /> <COLUMN name = \"C_PHONE\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203524\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"5\" type = \"CHARACTER(15)\" typeno = \"1042\" typemod = \" 31\" notnull = \"t\" /> <COLUMN name = \"C_ACCTBAL\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203525\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"6\" type = \"NUMERIC(15,2)\" typeno = \"1700\" typemod = \"983058\" notnull = \"t\" /> <COLUMN name = \"C_MKTSEGMENT\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203526\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"7\" type = \"CHARACTER(10)\" typeno = \"1042\" type mod = \"26\" notnull = \"t\" /> <COLUMN name = \"C_COMMENT\" schema = \"\" owner = \"\" oidhi = \"0\" oid = \"203527\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"t\" odelim = \"t\" seq = \"8\" type = \"CHARACTER VARYING(117)\" typeno = \"1043 \" typemod = \"133\" notnull = \"t\" /> <DISTRIBUTION seq = \"1\" attnum = \"1\" /> </TABLE> <VIEW name = \"NATIONSBYREGIONS\" schema = \"ADMIN\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200827\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" definition = \"SELECT REGION.R_NAME, N ATION.N_NAME FROM ADMIN.NATION, ADMIN.REGION WHERE (REGION.R_REGIONKEY = NATION.N_REGIONKEY);\" /> <USER name = \"QAUSER\" schema = \"\" owner = \"ADMIN\" oidhi = \"0\" oid = \"200829\" classhi = \"0\" class = \"0\" delimited = \"f\" sdelim = \"f\" odelim = \"f\" pwd = \"\" rowsetlimit = \"0\" validuntil = \"\" sesstimeout = \" 0\" qrytimeout = \"0\" defp = \"NONE\" maxp = \"NONE\" pwdinv = \"f\" pwdlastchged = \"\" resrcgrpid = \"4901\" crossjoin = \"NULL\" collecthist = \"0\" accesstime = \"0\" concursess = \"0\" seclabel = \"PUBLIC::\" audit cat = \"NONE\" useauth = \"0\" > </USER> <RSG username = \"QAUSER\" userdelim = \"f\" groupname = \"PUBLIC\" groupdelim = \"f\" /> </DATABASE> </ARCHIVE> As you see this file contains a full XML description of your database, including table definition, views, users etc. Switch back to the lab folder: Input [Terminal 1]: cd ~/labs/backupRestore/ Output [ nz@localhost backupRestore ] $ You should now have a pretty good understanding of the Performance Server Backup process. In the next section we will demonstrate the restore functionality.","title":"Verifying the Backups"},{"location":"nz-06-BNR/#restoring-the-database","text":"In this section, we will restore our database to the first increment and then we will upgrade our database to the next increment. Performance Server allows you to return a database to a specific increment in your backup set. If you want to do an incremental restore, the database must be locked. Tables can be queried but not changed until the database is in the desired state and unlocked. In the NZSQL session we will drop the QA database and the QA user. First connect to the SYSTEM database: Input [Terminal 2]: \\c SYSTEM Output You are now connected to database SYSTEM. SYSTEM.ADMIN ( ADMIN )= > Now drop the QA database: Input [Terminal 2]: DROP DATABASE LABDBQA ; Output DROP DATABASE Now drop the QA User: Input [Terminal 2]: DROP USER QAUSER ; Output DROP USER Verify that the QA database has been deleted by displaying the databases using the \\l command. You will see that the LABDBQA database has been removed: Input [Terminal 2]: \\l Output List of databases DATABASE | OWNER -----------+---------- LABDB | LABADMIN LABDBTEST | ADMIN SYSTEM | ADMIN ( 3 rows ) In the OS session, restore the database to the first increment: Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -increment 1 -lockdb true Output Restore of increment 1 from backupset \ufeff20210331133512 to database 'labdbqa' committed. Notice that we have specified the increment with the --increment option. In our case this is the first full backup in our backup set. We did not have to specify a backup set, by default the most recent one is used. Since we are not sure which increment we want to restore the database to, we have to lock the database with the -lockdb option. This allows only read-only access until the desired increment has been restored. In the NZSQL session, verify that the database has been recreated with \\l Input [Terminal 2]: \\l Output List of databases DATABASE | OWNER -----------+---------- LABDB | LABADMIN LABDBQA | QAUSER LABDBTEST | ADMIN SYSTEM | ADMIN ( 4 rows ) Notice the LABDBQA database is there. You can also see that the owner QAUSER has been recreated and is again the database owner. Connect to the LABDBQA database with the \\c command Input [Terminal 2]: \\c labdbqa Output NOTICE: Database 'LABDBQA' is available for read-only You are now connected to database labdbqa. Notice that LABDBQA database is currently in read-only mode. Verify the contents of the REGION table from the LABDBQA database: Input [Terminal 2]: select * from region order by 1 ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific ( 4 rows ) Notice that we have returned the database to the point in time after the first full backup. There is no north or south pole in the R_COMMENT column. Try to insert a row to verify the read only mode: Input [Terminal 2]: insert into region values ( 5 , 'np' , 'north pole' ) ; Output ERROR: Database 'LABDBQA' is available for read-only ( command ignored ) As expected, this is prohibited until we unlock the database. In the OS session, apply the next increment to the database Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -increment next -lockdb true Output Restore of increment 2 from backupset \ufeff20210331133512 to database 'labdbqa' committed. Notice that we now apply the second increment to the database. Since we do not need to load any more increments, we can now unlock the database: Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -unlockdb Output [ nz@localhost backupRestore ] $ After the database unlock, we cannot apply any further increments to this database. To jump to a different increment, we would need to start from the beginning. In the nzsql session we will look at the REGION table again: Input [Terminal 2]: select * from region order by 1 ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 1 | na | north america 2 | sa | south america 3 | emea | europe, middle east, africa 4 | ap | asia pacific 5 | np | north pole ( 5 rows ) Notice that we have added the north pole region, which was created before the first differential backup. Verify that the database is unlocked and ready for use again by adding a new set of customers to the CUSTOMER table. In addition to the Automobile users we want to add the machinery users from the main database: Input [Terminal 2]: insert into customer select * fromlabdb.admin.customer where c_mktsegment = 'MACHINERY' ; Output INSERT 0 29949 Notice that we now can use the database in a normal fashion again. We had around 30000 customers before, verify that the new user set has been added successfully: Input [Terminal 2]: select count ( * ) from customer ; Output COUNT ------- 59701 ( 1 row ) You will see that we now have around 60000 rows in the CUSTOMER table. You have now done a full restore cycle for the database and applied a full and incremental backup to your database. In the next section we will demonstrate single table restore and the ability to restore from any backup set.","title":"Restoring the Database"},{"location":"nz-06-BNR/#single-table-restore","text":"In this chapter we will demonstrate the targeted restore of a subset of tables from a backup set. We will also demonstrate how to restore from a specific older backup set. First, we will create a second backup set with the new customer data. In the OS session execute the following command: Input [Terminal 1]: nzbackup -db labdbqa -dir /tmp/bk1 /tmp/bk2 Output Backup of database labdbqa to backupset \ufeff20210331134349 completed successfully. Notice that this is a full database backup because we took the defaults. In this case Netezza Performance Server automatically creates a new backup set. We want to return the CUSTOMER table to the previous condition. But we do not want to change the REGION or the NATION tables. To do this we need to know the backup set id of the previous backup set. To do this execute the history command again: Input [Terminal 1]: nzbackup -history Output Database Backupset Seq # OpType Status Date Log File --------- -------------- ----- ------- --------- ------------------- ------------------------------ ( LABDBQA ) 20210331133344 1 NO DATA COMPLETED 2021 -03-31 06 :33:44 backupsvr.30436.2021-03-31.log ( LABDBQA ) 20210331133512 1 FULL COMPLETED 2021 -03-31 06 :35:12 backupsvr.30648.2021-03-31.log ( LABDBQA ) 20210331133512 2 DIFF COMPLETED 2021 -03-31 06 :36:00 backupsvr.30859.2021-03-31.log LABDBQA 20210331134349 1 FULL COMPLETED 2021 -03-31 06 :43:49 backupsvr.31972.2021-03-31.log We now see three different backup sets, the schema only backup, the two step backupset and the new full backupset. Remember the backup set id of the two step backupset. To return only the CUSTOMER table to its condition of the second backup set, we can do a table level restore with the following command: Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -backupset \ufeff20210331133512 -tables CUSTOMER Output Error: Specify -droptables to force drop of tables in the -tables list. This command will only restore the tables in the --tables option. If you want to restore multiple tables, you can simply write them in a list after the option. We use the --backupset option to specify a specific backup set. Remember to replace the id with the value you retrieved with the history command. Info Notice that the table name needs to be case sensitive. This is in contrast to the database name. You will get the error \"Performance Server cannot restore a table that exists in the target database. You can either drop the table before restoring it or use the --droptables option.\" Repeat the previous command with the added --droptables option: Input [Terminal 1]: nzrestore -db labdbqa -dir /tmp/bk1 /tmp/bk2 -backupset 20210331133512 -tables CUSTOMER -droptables Output [ Restore Server ] : Dropping TABLE 'CUSTOMER' [ Restore Server ] : Dropping TABLE 'CUSTOMER' Restore of increment 1 from backupset 20210331133512 to database 'labdbqa' committed. Restore of increment 2 from backupset 20210331133512 to database 'labdbqa' committed. Notice that the target table was dropped before the restore happened and the specified backup set was used. Since we didn't stipulate a specific increment, the full backup set has been applied with all increments. Also, the table is automatically unlocked after the restore process finishes. Finally let's verify that the restore worked as expected, in the NZSQL console count the rows of the customer table again: Input [Terminal 2]: select count ( * ) from customer ; Output COUNT ------- 29752 ( 1 row ) You will see that we are back to approximately 30000 rows. This means that we have backed out the most recent changes. In this chapter you executed a single table restore and you did a restore from a specific backup set.","title":"Single Table Restore"},{"location":"nz-06-BNR/#backing-up-user-data-and-host-data","text":"In the previous chapters you have learned to backup Performance Server databases. This backs up all the database objects that are used in the database and the user data. These are the most critical components to back up in a Performance Server. They will allow you to recreate your databases even if you switch to a completely new Netezza Performance Server. There are two other things that should be backed up: The global user information. The host data In this chapter you will do a backup of these components, so you would be able to revert your IBM Netezza Performance Server to the exact condition it was in before the backup.","title":"Backing up User Data and Host Data"},{"location":"nz-06-BNR/#user-data-backup","text":"Users, groups, and privileges that are not used in databases will not be backed up by the user data backup. To be able to revert a Performance Server completely to its original condition you need to have a backup of the global user information as well, to capture for example administrative users that are not part of any database. This is done with the --users option of the [nzbackup command: In the OS session execute the following command. Input [Terminal 2]: nzbackup -dir /tmp/bk1 /tmp/bk2 -users Output Backup of global objects completed successfully. This will create a backup of all Users, Groups and Privileges. Restoring it will not delete any users, instead it will only add missing Users, Groups and Privileges, so it doesn't need to be fully synchronized with the user data backup. You can even restore an older user backup without fear of destroying information.","title":"User Data Backup"},{"location":"nz-06-BNR/#host-data-backup","text":"Until now we have always backed up database content. This is essentially catalog and user data that can be applied to a new Performance Server. Performance Server also provides the functionality to backup and restore host data. This is essentially the data in the /nz/data and /export/nz directories of the host server. There are two reasons for regularly backing up host data. The first is a host crash. If the data on your system is intact but the host file system has been destroyed, you can recreate all databases from the user backup. In very large systems this might take a long time. It is much easier to just restore the host information and reconnect to the undamaged user tables. The second reason is that the host data contains configuration information, log and plan files etc. that are not saved by the user backup. If you changed the system configuration that information would be lost. Therefore, it is advisable to regularly backup host data. To backup the host data execute the following command in the OS session: Input [Terminal 2]: nzhostbackup /tmp/hostbackup Output Starting host backup. System state is 'online' . Pausing the system ... Checkpointing host catalog ... Archiving system catalog ... Resuming the system ... Host backup completed successfully. System state is 'online' . As you can see, the system was paused for the duration of the host backup but is automatically resumed after the backup is successful. Also notice that the host backup is done with the nzhostbackup command instead of the standard nzbackup command. Let's have a look at the created file: Input [Terminal 2]: ls -l /tmp Output total 389308 drwxrwxrwx. 3 nz nz 21 Apr 5 11 :42 bk1 drwxrwxrwx. 3 nz nz 21 Apr 5 11 :42 bk2 drwxrwxrwx. 3 nz nz 21 Apr 5 11 :37 bkschema -rw-r--r--. 1 root root 36730 Apr 6 03 :54 cyclops_run.log -rw-------. 1 nz nz 398447694 Apr 6 03 :52 hostbackup ... Notice that a backup file has been created. It's a compressed file containing the system catalog and Performance Server host information. If possible, host backups should be done regularly. If an old host backup is restored, there might be some orphaned tables. Orphan tables are tables that were created after the host backup and consume disk space but are not registered in the system catalog anymore. During host restore, Performance Server will create a script to clean up these orphaned tables. Congratulations you have finished the Backup & Restore lab and you have had a chance to see all components of a successful Performance Server backup strategy. In this lab, we used the file system backup. In a real Performance Server backup and restore operation, we would most likely use third-party solutions such as IBM Spectrum Protect (formerly Tivoli\u00ae Storage Manager), Veritas NetBackup, and EMC NetWorker as destinations. For further information regarding the setup steps please refer to the IBM Knowledge Center for Netezza Performance Server.","title":"Host Data Backup"},{"location":"nz-07-Query-Optimization/","text":"Query Optimization Netezza Performance Server uses a cost-based optimizer to determine the best method for scan and join operations, join order, and data movement between SPUs (redistribute or broadcast operations if necessary). For example, the planner tries to avoid redistributing large tables because of the performance impact. The optimizer can also dynamically rewrite queries to improve query performance. The optimizer takes a SQL query as input and creates a detailed execution or query plan for the database system. For the optimizer to create the best execution plan that results in the best performance, it must have the most up-to-date statistics. You can use EXPLAIN, HTML (also known as bubble), and text plans to analyze how the Netezza Performance Server system executes a query. Explain is a very useful tool to spot and identify performance problems, bad distribution keys, badly written SQL queries and out-of-date statistics. Objectives During our POC we have identified a couple of very long running customer queries that have significantly worse performance than the number of rows involved would suggest. In this lab we will use Explain functionality to identify the concrete bottlenecks and if possible, fix them to improve query performance. Lab Setup This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using putty If you are continuing from the previous lab and are already connected to NZSQL quit the NZSQL console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/queryOptimization/setupLab ./setupLab.sh Output DROP DATABASE CREATE DATABASE DROP USER CREATE USER ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully There may be error message at the beginning of the output since the script tries to clean up existing databases and users. Switch to the lab directory ~/labs/queryOptimization. To do this use the following command: (Notice that you can use bash auto complete by using the Tab key to complete folder and files names) Input cd ~/labs/queryOptimization Output [ nz@localhost queryOptimization ] $ The command line prompt changes to reflect the directory you are in (queryOptimization). Generate Statistics Cost based optimizers depend on accurate information about the data in the tables in order to generate well optimized query plans. This part of the lab looks at the importance of having up to date statistics about the table data, such as number of rows, size of the rows, size of the table, number of columns, number of unique values in each column, etc. The generate statistics command is used to collect the statistical information about the tables and columns. Our first long running customer query returns the average order price by customer segment for a given year and order priority. It joins the customer table for the market segment and the orders table for the total price of the order. Due to restrictive join conditions it shouldn't require too much processing time. But on our test systems it runs a very long time. In this chapter we will use Netezza Performance Server's Explain functionality to find out why this is the case. The customer query in question: SELECT c.c_mktsegment, AVG(o.o_totalprice) FROM orders AS o, CUSTOMER as c WHERE EXTRACT(YEAR FROM o.o_orderdate) = 1996 AND o.o_orderpriority = '1-URGENT' GROUP BY c.c_mktsegment; First, we will make sure that the system doesn't run a different workload that could influence our tests. Use the following nzsession command to verify that the system is free: Input nzsession show Output ID Type User Start Time PID Database Schema State Priority Name Client IP Client PID Command ----- ---- ----- ----------------------- ----- -------- ------ ------ ------------ --------- ---------- ------------------------ 21920 sql ADMIN 03 -Apr-20, 16 :35:54 PDT 15500 SYSTEM ADMIN active normal 127 .0.0.1 15497 SELECT session_id, clien This result shows that there is currently only one session connected to the database, which is the nzsession command itself. Per default the database user in your VM image is ADMIN. Executing this command before doing any performance measurements ensures that other workloads are not influencing the performance of the system. You can use the nzsession command as well to abort bad or locked sessions. After we verified that the system is free, we can start analyzing the query. Connect to the lab database with the following command: Input nzsql labdb labadmin !! abstract \"Output\" Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > Let's first have a look at the two tables and the WHERE conditions to get an idea of the row numbers involved. Our query joins the CUSTOMER table without any where condition applied to it and the ORDERS table that has two where conditions restricting it on the date and order priority. From the data distribution lab, we know that the CUSTOMER table has 150000 rows. To get the rows that are involved from the ORDERS table Execute the following COUNT(*) command: Input SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; Output COUNT ------- 46014 ( 1 row ) So, the ORDERS table has 46014 rows that fit the WHERE condition. We will use EXPLAIN functionality to check if the available Statistics allow the Netezza Performance Server optimizer to estimate this correctly for its plan creation. The Netezza Performance Server optimizer uses statistics about the data in the system to estimate the number of rows that result from WHERE conditions, joins, etc. Doing wrong approximations can lead to bad execution plans. For example, a huge result set could be broadcast for a join instead of doing a double redistribution. To see its estimated rows for the WHERE conditions in our query run the following EXPLAIN command: Input EXPLAIN VERBOSE SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; Output EXPLAIN VERBOSE SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"ORDERS\" {( ORDERS.O_ORDERKEY )}] -- **Estimated Rows = 150 **, Width = 0 , Cost = 0 .0 .. 1653 .2, Conf = 64 .0 Restrictions: (( ORDERS.O_ORDERPRIORITY = '1-URGENT' ::BPCHAR ) AND ( DATE_PART ( 'YEAR' :: \"VARCHAR\" , ORDERS.O_ORDERDATE ) = 1996 )) Projections: Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 8 , Cost = 1653 .2 .. 1653 .2, Conf = 0 .0 Projections: 1 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..<Removed Plan Text>.. The execution plan of this query consists of two nodes or snippets. First the table is scanned and the WHERE conditions are applied, which can be seen in the Restrictions sub node. Since we use a COUNT(*) the Projections node is empty. Then an Aggregation node (Node 2) is applied to count the rows that are returned by node 1. When we look at the estimated number of rows, we can see that it is way off the mark. The Netezza Performance Server Optimizer estimates from its available statistics that only 150 rows are returned by the WHERE conditions. We have seen before that in reality its 46014 or roughly 300 times as many. One way to help the optimizer in its estimates is the collection of detailed statistics about the involved tables. Execute the following command to generate full, detailed statistics about the ORDERS table: !! abstract \"Input\" GENERATE STATISTICS ON orders ; Output GENERATE STATISTICS Since generating full statistics involves a table scan this command may take some time to execute, especially on a table with many rows and columns. We will now check if generating statistics has improved the estimates. Execute the EXPLAIN command again: Input EXPLAIN VERBOSE SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; Output EXPLAIN VERBOSE SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"ORDERS\" {( ORDERS.O_ORDERKEY )}] -- **Estimated Rows = 3000 **, Width = 0 , Cost = 0 .0 .. 1653 .2, Conf = 64 .0 Restrictions: (( ORDERS.O_ORDERPRIORITY = '1-URGENT' ::BPCHAR ) AND ( DATE_PART ( 'YEAR' :: \"VARCHAR\" , ORDERS.O_ORDERDATE ) = 1996 )) Projections: Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 8 , Cost = 1653 .4 .. 1653 .4, Conf = 0 .0 Projections: 1 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. As we can see the estimated rows of the SELECT query have improved drastically. The optimizer now assumes this WHERE condition will apply to 3000 rows of the order table. Still significantly off the true number of 46000 but by a factor of 20 better than the original estimate of 150. Estimations are very difficult to make. Obviously, the optimizer cannot do the actual computation during planning. It relies on current statistics about the involved columns. Statistics include min/max values, distinct values, numbers of null values etc. Some of these statistics are collected on the fly but the most detailed statistics can be generated manually with the Generate Statistics command (or the nz_genstats command with \"full\" option). Generating full statistics after loading a table or changing its content significantly is one of the most important administration tasks in Netezza Performance Server. The Netezza Performance Server appliance will automatically generate express statistics after many tasks like load operations and just-in-time statistics during planning. Nevertheless, full statistics should be generated on a regular basis. Identifying Join Problems In the last chapter we have taken a first look at the tables involved in our join query and have improved optimizer estimates by generating statistics on the involved tables. Now we will have a look at the complete execution plan, and we will have a specific look at the distribution and involved join. In our example we have a query that doesn't finish in a reasonable amount of time. It is taken much longer than you would expect from the involved data sizes. We will now analyze why this is the case. Let's analyze the execution plan for this query using the EXPLAIN VERBOSE command: Input EXPLAIN VERBOSE SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' GROUP BY c.c_mktsegment ; Output EXPLAIN VERBOSE SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' GROUP BY c.c_mktsegment ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"CUSTOMER\" as \"C\" {( C.C_CUSTKEY )}] -- Estimated Rows = 150000 , Width = 10 , Cost = 0 .0 .. 258 .4, Conf = 100 .0 Projections: 1 :C.C_MKTSEGMENT [ SPU Broadcast ] Node 2 . [ SPU Sequential Scan table \"ORDERS\" as \"O\" {( O.O_ORDERKEY )}] -- Estimated Rows = 3000 , Width = 8 , Cost = 0 .0 .. 1653 .2, Conf = 64 .0 Restrictions: (( O.O_ORDERPRIORITY = '1-URGENT' ::BPCHAR ) AND ( DATE_PART ( 'YEAR' :: \"VARCHAR\" , O.O_ORDERDATE ) = 1996 )) Projections: 1 :O.O_TOTALPRICE Node 3 . [ SPU Nested Loop Stream \"Node 2\" with Temp \"Node 1\" {( O.O_ORDERKEY )}] -- Estimated Rows = 450000007 , Width = 18 , Cost = 660086 .9 .. 7306828 .5, Conf = 64 .0 Restrictions: 't' ::BOOL Projections: 1 :C.C_MKTSEGMENT 2 :O.O_TOTALPRICE Node 4 . [ SPU Group ] -- Estimated Rows = 100 , Width = 18 , Cost = 660086 .9 .. 7341984 .7, Conf = 0 .0 Projections: 1 :C.C_MKTSEGMENT 2 :O.O_TOTALPRICE [ SPU Return ] [ HOST Merge Group ] Node 5 . [ Host Aggregate ] -- Estimated Rows = 100 , Width = 26 , Cost = 660086 .9 .. 7341984 .7, Conf = 0 .0 Projections: 1 :C.C_MKTSEGMENT 2 : ( SUM ( O.O_TOTALPRICE ) / \"NUMERIC\" ( COUNT ( O.O_TOTALPRICE ))) [ Host Return ] ..<Removed Plan Text>.. First try to answer the following questions through the execution plan yourself. Take your time. We will walk through the answers after that. Question Answer a. Which columns of Table CUSTOMER are used in further computations? b. Is Table CUSTOMER redistributed, broadcast or can it be joined locally? c. Is Table ORDERS redistributed, broadcast or can it be joined locally? d. In which node are the WHERE conditions applied and how many rows does Netezza Performance Server expect to fulfill the where condition? e. What kind of join takes place and in which node? f. What is the number of estimated rows for the join? g. What is the most expensive node and why? Hint: A stream operation in Netezza Performance Server Explain is an operation whose output data isn't persisted on disk but streamed to further computation nodes or snippets for a local join or local aggregation. No data is broadcast or redistributed in a stream operation. So let's walk through the questions: a. Which columns of Table CUSTOMER are used in further computations? The first node in the execution plan does a sequential scan of the CUSTOMER table on the SPUs. It estimates that 150000 rows are returned which we know is the number of rows in the CUSTOMER table. Node 1. [SPU Sequential Scan table \"CUSTOMER\" as \"C\" {(C.C_CUSTKEY)}] -- Estimated Rows = 150000, Width = 10, Cost = 0.0 .. 258.4, Conf = 100.0 **Projections:** **1:C.C_MKTSEGMENT** **[SPU Broadcast]** The statement that tells us which columns are used in further computations is the \"Projections:\" clause. We can see that only the C_MKTSEGMENT column is carried on from the CUSTOMER table. All other columns are thrown away. Since C_MKTSEGMENT is a CHAR(10) column the returned result set has a width of 10. b. Is Table CUSTOMER redistributed, broadcast or can it be joined locally? During scan the CUSTOMER table is broadcast to the other SPUs as seen by the \"[SPU Broadcast]\" clause in Node 1. This means that a complete CUSTOMER table is assembled on the host and broadcast to each SPU for further computation of the query. This may seem surprising at first since we have a substantial number of rows. But since the width of the result set is only 10 Bytes we are talking about 150000 rows * 10 Bytes = 1.5MB. This is a small amount of data for a warehousing system. c. Is Table ORDERS redistributed, broadcast or can it be joined locally? Node 2. [SPU Sequential Scan table \"ORDERS\" as \"O\" {(O.O_ORDERKEY)}] **-- Estimated Rows = 3000**, Width = 8, Cost = 0.0 .. 1653.2, Conf = 64.0 **Restrictions:** **((O.O_ORDERPRIORITY = '1-URGENT'::BPCHAR) AND (DATE_PART('YEAR'::\"VARCHAR\", O.O_ORDERDATE) = 1996))** Projections: 1:O.O_TOTALPRICE The second node of the execution plan does a scan of the ORDERS table. One column O_TOTALPRICE is projected and used in further computations. We cannot see any distribution or broadcast clauses so this table can be joined locally. This is true because the CUSTOMER table is broadcast to all SPUs. If one table of a join is broadcast the other table doesn't need any redistribution. d. In which node are the WHERE conditions applied and how many rows does Netezza Performance Server expect to fulfill the WHERE condition? We can see from the \"Restrictions\" clause in Node 2 that the WHERE conditions of our query are applied here. This should be clear since both WHERE conditions are applied to the ORDERS table and the restriction of rows can occur during the scan of the ORDERS table. As we can see in the \"Estimated Rows\" clause, the optimizer estimates a returned set of 3000 rows. We know this is not a perfect estimate since we found 46014 rows are returned from this table. e. What kind of join takes place and in which node? Node 3. **[SPU Nested Loop Stream \"Node 2\" with Temp \"Node 1\" {(O.O_ORDERKEY)}]** **-- Estimated Rows = 450000007**, Width = 18, **Cost = 660086.9 .. 7306828.5**, Conf = 64.0 Restrictions: 't'::BOOL Projections: 1:C.C_MKTSEGMENT 2:O.O_TOTALPRICE The third node of our execution plan contains the join between the two tables. It is a Nested Loop Join which means that every row of the first join set is compared to each row of the second join set. If the join condition holds true, the joined row is then added to the result set. This can be a very efficient join for small tables but for large tables its complexity is quadratic and therefore in general less fast than a Hash Join. However, Hash Joins cannot be used in the case of inequality join conditions, floating point join keys etc. Also look at the two columns that continue to be projected for further use in the query plan. These are the columns that were projected from Node 1 and Node 2. f. What is the number of estimated rows for the join? We can see in the \"Estimated Rows\" clause that the optimizer estimates this join node to return roughly 450m rows. The 450m rows is the number of rows from the first node times the number of rows from the second node. g. What is the most expensive node and why? The \"Cost\" clause provides an estimate of how expensive a Node is, where a higher cost usually leads to longer execution time. As we can see from the \"Cost\" clause in Node 3, the optimizer estimates that the join has a cost in the range from 660086.9 .. 7306828.5. This is a dramatically higher cost than what was estimated for Node 1 and Node 2. Node 4 and 5, which group and aggregate the result set, do not add much cost eitherl. So, our performance problems clearly originate in the join node 3. So, what is happening here? If we look at the query, we can assume that it is intended to compute the average order cost per market segment. This means we should join all customers to their corresponding order rows. But for this to happen we would need a join condition that joins the CUSTOMER table and the ORDERS table on the customer key. Instead the query performs a Cartesian Join, joining each customer row to each orders row. This is a very work intensive query that results in the behavior we have seen. The joined result set becomes huge. And the result does not answer the question that we intended to ask. So how do we fix this? By adding a join condition to the query that makes sure that customers are only joined to their orders. This additional join condition is O.O_CUSTKEY=C.C_CUSTKEY. Execute the following EXPLAIN command for the modified query. Input EXPLAIN VERBOSE SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; ] Output EXPLAIN VERBOSE SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"ORDERS\" as \"O\" {( O.O_ORDERKEY )}] -- Estimated Rows = 3000 , Width = 12 , Cost = 0 .0 .. 1653 .2, Conf = 64 .0 Restrictions: (( O.O_ORDERPRIORITY = '1-URGENT' ::BPCHAR ) AND ( DATE_PART ( 'YEAR' :: \"VARCHAR\" , O.O_ORDERDATE ) = 1996 )) Projections: 1 :O.O_TOTALPRICE **2:O.O_CUSTKEY** Cardinality: O.O_CUSTKEY 3 .0K ( Adjusted ) ** [ SPU Distribute on {( O.O_CUSTKEY )}] ** ** [ HashIt for Join ] ** Node 2 . [ SPU Sequential Scan table \"CUSTOMER\" as \"C\" {( C.C_CUSTKEY )}] -- Estimated Rows = 150000 , Width = 14 , Cost = 0 .0 .. 258 .4, Conf = 100 .0 Projections: 1 :C.C_MKTSEGMENT **2:C.C_CUSTKEY** Node 3 . ** [ SPU Hash Join Stream \"Node 2\" with Temp \"Node 1\" {( O.O_CUSTKEY,C.C_CUSTKEY )}] ** **-- Estimated Rows = 150000 **, Width = 18 , **Cost = 1653 .2 .. 2015 .2**, Conf = 51 .2 **Restrictions:** ** ( C.C_CUSTKEY = O.O_CUSTKEY ) ** Projections: 1 :C.C_MKTSEGMENT 2 :O.O_TOTALPRICE Cardinality: O.O_CUSTKEY 100 ( Adjusted ) ..<Removed Plan Text>.. As you can see there have been some changes to the execution plan. The ORDERS table is now scanned and distributed on the customer key rather than broadcast. The CUSTOMER table is already distributed on the customer key, so no redistribution needs to happen. Both tables are then joined in node 3 through a Hash Join on the customer key, as seen in the \"Restrictions\" clause. Also see how the join key of each table is now projected in Node 1 and Node 2 so the join can be done in Node 3. The estimated number of rows is now 150000, the same as the number of customers. Since we have a 1:n relationship between customers and orders this is as we would expect. Also, the estimated cost of Node 3 has come down by over a factor of 100! Let's make sure that the query performance has indeed improved. Switch on the display of elapsed query time with the following command: Input \\t ime If you want, you can later switch off the elapsed time display by executing the same command again. It is a toggle. Now execute our modified query: Input SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o,CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; Output C_MKTSEGMENT | AVG --------------+--------------- BUILDING | 151275 .977882 MACHINERY | 151348 .971079 HOUSEHOLD | 150196 .009267 FURNITURE | 150998 .129771 AUTOMOBILE | 151488 .825830 ( 5 rows ) Elapsed time: 0m0.796s Before we made our changes the query took so long that we couldn't wait for it to finish. After our changes the execution time has improved to slightly under a second. In this relatively simple case we might have been able to pinpoint the problem through analyzing the SQL on its own. But this can be almost impossible for complicated multi-join queries that are often used in warehousing. Reporting and BI tools tend to create very complicated portable SQL as well. In these cases, EXPLAIN can be a valuable tool to pinpoint the problem. HTML Explain In this section we will look at the HTML plangraph for the customer query that we just fixed. Besides the text descriptions of the exeution plan we used in the previous chapter, PureData System provides the ability to generate a graphical query tree as well. This is done with the help of HTML. So plangraph files can be created and viewed in your internet browser. PureData System can be configured to save a HTML plangraph or plantext file for every executed SQL query. But in this chapter we will use the basic EXPLAIN PLANGRAPH command and use Cut&Paste to export the file to your host computer. Enter the query with the keyword EXPLAIN PLANGRAPH to generate the HTML plangraph: Input EXPLAIN PLANGRAPH SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; Output NOTICE: QUERY PLAN: **<html xmlns:v = \"urn:schemas-microsoft-com:vml\" xmlns = \"http://www.w3.org/TR/REC-html40\" >** <head> <meta http-equiv = \"Content-Type\" content = \"text/html; charset=utf-8\" > <meta http-equiv = \"Generator\" content = \"Netezza Performance Server\" > <style> v \\: * { behavior:url ( #default#VML);} </style> </head> <body lang = \"en-US\" > <pre style = \"font:normal 68% verdana,arial,helvetica;background:#EEEEEE;margin-top:1em;margin-bottom:1em;margin-left:0px;padding:5pt;\" > EXPLAIN PLANGRAPH SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; </pre> <v:textbox style = \"position:absolute;margin-left:230pt;margin-top:19pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >AGG<br/>r = 100 w = 26 s = 2 .5KB</p></v:textbox> <v:oval style = \"position:absolute;margin-left:231pt;margin-top:15pt;width:78pt;height:25pt;z-index:9;\" fillcolor = \"#808080\" ></v:oval> <v:textbox style = \"position:absolute;margin-left:230pt;margin-top:0pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >ret</p></v:textbox> <v:textbox style = \"position:absolute;margin-left:230pt;margin-top:54pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >GROUP<br/>r = 100 w = 18 s = 1 .8KB</p></v:textbox> <v:oval style = \"position:absolute;margin-left:231pt;margin-top:50pt;width:78pt;height:25pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"270pt,27pt\" to = \"270pt,62pt\" /> <v:textbox style = \"position:absolute;margin-left:233pt;margin-top:42pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >snd,m-grp</p></v:textbox> <v:textbox style = \"position:absolute;margin-left:230pt;margin-top:89pt;width:80pt;height:31pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >HASHJOIN<br/>r = 150 .0K w = 18 s = 2 .6MB<br/> ( C_CUSTKEY = O_CUSTKEY ) </p></v:textbox> <v:oval style = \"position:absolute;margin-left:231pt;margin-top:85pt;width:78pt;height:31pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"270pt,62pt\" to = \"270pt,100pt\" /> <v:textbox style = \"position:absolute;margin-left:190pt;margin-top:124pt;width:80pt;height:31pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >SEQSCAN<br/>r = 150 .0K w = 14 s = 2 .0MB<br/>C</p></v:textbox> <v:oval style = \"position:absolute;margin-left:191pt;margin-top:120pt;width:78pt;height:31pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"270pt,97pt\" to = \"230pt,135pt\" /> <v:textbox style = \"position:absolute;margin-left:270pt;margin-top:124pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >HASH<br/>r = 3 .0K w = 12 s = 35 .2KB</p></v:textbox> <v:oval style = \"position:absolute;margin-left:271pt;margin-top:120pt;width:78pt;height:25pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"270pt,97pt\" to = \"310pt,132pt\" /> <v:textbox style = \"position:absolute;margin-left:253pt;margin-top:112pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >dst {( O_CUSTKEY )} </p></v:textbox> <v:textbox style = \"position:absolute;margin-left:270pt;margin-top:159pt;width:80pt;height:31pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >SEQSCAN<br/>r = 3 .0K w = 12 s = 35 .2KB<br/>O</p></v:textbox> <v:oval style = \"position:absolute;margin-left:271pt;margin-top:155pt;width:78pt;height:31pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"310pt,132pt\" to = \"310pt,170pt\" /> </body> **</html>** EXPLAIN Next open your host computer's text editor. If your workstation is windows open wordpad, if you use a Linux desktop use the default text editor like KEDIT, or GEDIT. Copy the output from the explain plangraph from your putty window into notepad. Make sure that you only copy the HTML file from the **<html ..** start tag to the <!--**html-->** end tag. Save the file as \"explain.html\" on your desktop. Now on your desktop double click on \"explain.html\". In windows make sure to open it with Google Chrome since this will result in the best output. You can see a graphical representation of the query we analyzed before. The left leg of the tree is the scan node of the Customer tables C, the right leg contains a scan of the Orders table O and a node hashing the result set from orders in preparation for the HASHJOIN node, that is joining the result sets of the two table scans on the customer key. After the join the result is fed into a GROUP node and an Aggregation node that computes the Average total price, before being returned to the caller. A graphical representation of the execution plan can be valuable for complicated multi-join queries to get an overview of the join. The Web Console can generate the graphically representation of the query. Run the query from \"Query Editor\". Open the NPS Web Console: <https://192.168.9.2:8443> from a browser. If the NPS Web Console doesn't start reinstall the console as follows: Stop active containers: Input docker ps Take note of the container IDs. Stop active containers: Input docker stop <container-ID> Remove the inactive containers: Input docker ps -a Take note of the container IDs. Stop active containers: Input docker rm <container-ID> Reinstall NPS Console: Input /root/cyclops_dockerrun/standalone-install.sh In the Query Editor: use: LABDB set schema: ADMIN set limit: 100 Query: SELECT c.c_mktsegment, AVG(o.o_totalprice) FROM orders AS o, CUSTOMER as c WHERE EXTRACT(YEAR FROM o.o_orderdate) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment; Click \"Plan graph\" Click \"Run\" Congratulations in this lab you have used NPS system Explain functionality to analyze a query.","title":"Query Optimization"},{"location":"nz-07-Query-Optimization/#query-optimization","text":"Netezza Performance Server uses a cost-based optimizer to determine the best method for scan and join operations, join order, and data movement between SPUs (redistribute or broadcast operations if necessary). For example, the planner tries to avoid redistributing large tables because of the performance impact. The optimizer can also dynamically rewrite queries to improve query performance. The optimizer takes a SQL query as input and creates a detailed execution or query plan for the database system. For the optimizer to create the best execution plan that results in the best performance, it must have the most up-to-date statistics. You can use EXPLAIN, HTML (also known as bubble), and text plans to analyze how the Netezza Performance Server system executes a query. Explain is a very useful tool to spot and identify performance problems, bad distribution keys, badly written SQL queries and out-of-date statistics.","title":"Query Optimization"},{"location":"nz-07-Query-Optimization/#objectives","text":"During our POC we have identified a couple of very long running customer queries that have significantly worse performance than the number of rows involved would suggest. In this lab we will use Explain functionality to identify the concrete bottlenecks and if possible, fix them to improve query performance.","title":"Objectives"},{"location":"nz-07-Query-Optimization/#lab-setup","text":"This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using putty If you are continuing from the previous lab and are already connected to NZSQL quit the NZSQL console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/queryOptimization/setupLab ./setupLab.sh Output DROP DATABASE CREATE DATABASE DROP USER CREATE USER ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully There may be error message at the beginning of the output since the script tries to clean up existing databases and users. Switch to the lab directory ~/labs/queryOptimization. To do this use the following command: (Notice that you can use bash auto complete by using the Tab key to complete folder and files names) Input cd ~/labs/queryOptimization Output [ nz@localhost queryOptimization ] $ The command line prompt changes to reflect the directory you are in (queryOptimization).","title":"Lab Setup"},{"location":"nz-07-Query-Optimization/#generate-statistics","text":"Cost based optimizers depend on accurate information about the data in the tables in order to generate well optimized query plans. This part of the lab looks at the importance of having up to date statistics about the table data, such as number of rows, size of the rows, size of the table, number of columns, number of unique values in each column, etc. The generate statistics command is used to collect the statistical information about the tables and columns. Our first long running customer query returns the average order price by customer segment for a given year and order priority. It joins the customer table for the market segment and the orders table for the total price of the order. Due to restrictive join conditions it shouldn't require too much processing time. But on our test systems it runs a very long time. In this chapter we will use Netezza Performance Server's Explain functionality to find out why this is the case. The customer query in question: SELECT c.c_mktsegment, AVG(o.o_totalprice) FROM orders AS o, CUSTOMER as c WHERE EXTRACT(YEAR FROM o.o_orderdate) = 1996 AND o.o_orderpriority = '1-URGENT' GROUP BY c.c_mktsegment; First, we will make sure that the system doesn't run a different workload that could influence our tests. Use the following nzsession command to verify that the system is free: Input nzsession show Output ID Type User Start Time PID Database Schema State Priority Name Client IP Client PID Command ----- ---- ----- ----------------------- ----- -------- ------ ------ ------------ --------- ---------- ------------------------ 21920 sql ADMIN 03 -Apr-20, 16 :35:54 PDT 15500 SYSTEM ADMIN active normal 127 .0.0.1 15497 SELECT session_id, clien This result shows that there is currently only one session connected to the database, which is the nzsession command itself. Per default the database user in your VM image is ADMIN. Executing this command before doing any performance measurements ensures that other workloads are not influencing the performance of the system. You can use the nzsession command as well to abort bad or locked sessions. After we verified that the system is free, we can start analyzing the query. Connect to the lab database with the following command: Input nzsql labdb labadmin !! abstract \"Output\" Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > Let's first have a look at the two tables and the WHERE conditions to get an idea of the row numbers involved. Our query joins the CUSTOMER table without any where condition applied to it and the ORDERS table that has two where conditions restricting it on the date and order priority. From the data distribution lab, we know that the CUSTOMER table has 150000 rows. To get the rows that are involved from the ORDERS table Execute the following COUNT(*) command: Input SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; Output COUNT ------- 46014 ( 1 row ) So, the ORDERS table has 46014 rows that fit the WHERE condition. We will use EXPLAIN functionality to check if the available Statistics allow the Netezza Performance Server optimizer to estimate this correctly for its plan creation. The Netezza Performance Server optimizer uses statistics about the data in the system to estimate the number of rows that result from WHERE conditions, joins, etc. Doing wrong approximations can lead to bad execution plans. For example, a huge result set could be broadcast for a join instead of doing a double redistribution. To see its estimated rows for the WHERE conditions in our query run the following EXPLAIN command: Input EXPLAIN VERBOSE SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; Output EXPLAIN VERBOSE SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"ORDERS\" {( ORDERS.O_ORDERKEY )}] -- **Estimated Rows = 150 **, Width = 0 , Cost = 0 .0 .. 1653 .2, Conf = 64 .0 Restrictions: (( ORDERS.O_ORDERPRIORITY = '1-URGENT' ::BPCHAR ) AND ( DATE_PART ( 'YEAR' :: \"VARCHAR\" , ORDERS.O_ORDERDATE ) = 1996 )) Projections: Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 8 , Cost = 1653 .2 .. 1653 .2, Conf = 0 .0 Projections: 1 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..<Removed Plan Text>.. The execution plan of this query consists of two nodes or snippets. First the table is scanned and the WHERE conditions are applied, which can be seen in the Restrictions sub node. Since we use a COUNT(*) the Projections node is empty. Then an Aggregation node (Node 2) is applied to count the rows that are returned by node 1. When we look at the estimated number of rows, we can see that it is way off the mark. The Netezza Performance Server Optimizer estimates from its available statistics that only 150 rows are returned by the WHERE conditions. We have seen before that in reality its 46014 or roughly 300 times as many. One way to help the optimizer in its estimates is the collection of detailed statistics about the involved tables. Execute the following command to generate full, detailed statistics about the ORDERS table: !! abstract \"Input\" GENERATE STATISTICS ON orders ; Output GENERATE STATISTICS Since generating full statistics involves a table scan this command may take some time to execute, especially on a table with many rows and columns. We will now check if generating statistics has improved the estimates. Execute the EXPLAIN command again: Input EXPLAIN VERBOSE SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; Output EXPLAIN VERBOSE SELECT COUNT ( * ) FROM orders WHERE EXTRACT ( YEAR FROM o_orderdate ) = 1996 AND o_orderpriority = '1-URGENT' ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"ORDERS\" {( ORDERS.O_ORDERKEY )}] -- **Estimated Rows = 3000 **, Width = 0 , Cost = 0 .0 .. 1653 .2, Conf = 64 .0 Restrictions: (( ORDERS.O_ORDERPRIORITY = '1-URGENT' ::BPCHAR ) AND ( DATE_PART ( 'YEAR' :: \"VARCHAR\" , ORDERS.O_ORDERDATE ) = 1996 )) Projections: Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 8 , Cost = 1653 .4 .. 1653 .4, Conf = 0 .0 Projections: 1 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. As we can see the estimated rows of the SELECT query have improved drastically. The optimizer now assumes this WHERE condition will apply to 3000 rows of the order table. Still significantly off the true number of 46000 but by a factor of 20 better than the original estimate of 150. Estimations are very difficult to make. Obviously, the optimizer cannot do the actual computation during planning. It relies on current statistics about the involved columns. Statistics include min/max values, distinct values, numbers of null values etc. Some of these statistics are collected on the fly but the most detailed statistics can be generated manually with the Generate Statistics command (or the nz_genstats command with \"full\" option). Generating full statistics after loading a table or changing its content significantly is one of the most important administration tasks in Netezza Performance Server. The Netezza Performance Server appliance will automatically generate express statistics after many tasks like load operations and just-in-time statistics during planning. Nevertheless, full statistics should be generated on a regular basis.","title":"Generate Statistics"},{"location":"nz-07-Query-Optimization/#identifying-join-problems","text":"In the last chapter we have taken a first look at the tables involved in our join query and have improved optimizer estimates by generating statistics on the involved tables. Now we will have a look at the complete execution plan, and we will have a specific look at the distribution and involved join. In our example we have a query that doesn't finish in a reasonable amount of time. It is taken much longer than you would expect from the involved data sizes. We will now analyze why this is the case. Let's analyze the execution plan for this query using the EXPLAIN VERBOSE command: Input EXPLAIN VERBOSE SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' GROUP BY c.c_mktsegment ; Output EXPLAIN VERBOSE SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' GROUP BY c.c_mktsegment ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"CUSTOMER\" as \"C\" {( C.C_CUSTKEY )}] -- Estimated Rows = 150000 , Width = 10 , Cost = 0 .0 .. 258 .4, Conf = 100 .0 Projections: 1 :C.C_MKTSEGMENT [ SPU Broadcast ] Node 2 . [ SPU Sequential Scan table \"ORDERS\" as \"O\" {( O.O_ORDERKEY )}] -- Estimated Rows = 3000 , Width = 8 , Cost = 0 .0 .. 1653 .2, Conf = 64 .0 Restrictions: (( O.O_ORDERPRIORITY = '1-URGENT' ::BPCHAR ) AND ( DATE_PART ( 'YEAR' :: \"VARCHAR\" , O.O_ORDERDATE ) = 1996 )) Projections: 1 :O.O_TOTALPRICE Node 3 . [ SPU Nested Loop Stream \"Node 2\" with Temp \"Node 1\" {( O.O_ORDERKEY )}] -- Estimated Rows = 450000007 , Width = 18 , Cost = 660086 .9 .. 7306828 .5, Conf = 64 .0 Restrictions: 't' ::BOOL Projections: 1 :C.C_MKTSEGMENT 2 :O.O_TOTALPRICE Node 4 . [ SPU Group ] -- Estimated Rows = 100 , Width = 18 , Cost = 660086 .9 .. 7341984 .7, Conf = 0 .0 Projections: 1 :C.C_MKTSEGMENT 2 :O.O_TOTALPRICE [ SPU Return ] [ HOST Merge Group ] Node 5 . [ Host Aggregate ] -- Estimated Rows = 100 , Width = 26 , Cost = 660086 .9 .. 7341984 .7, Conf = 0 .0 Projections: 1 :C.C_MKTSEGMENT 2 : ( SUM ( O.O_TOTALPRICE ) / \"NUMERIC\" ( COUNT ( O.O_TOTALPRICE ))) [ Host Return ] ..<Removed Plan Text>.. First try to answer the following questions through the execution plan yourself. Take your time. We will walk through the answers after that. Question Answer a. Which columns of Table CUSTOMER are used in further computations? b. Is Table CUSTOMER redistributed, broadcast or can it be joined locally? c. Is Table ORDERS redistributed, broadcast or can it be joined locally? d. In which node are the WHERE conditions applied and how many rows does Netezza Performance Server expect to fulfill the where condition? e. What kind of join takes place and in which node? f. What is the number of estimated rows for the join? g. What is the most expensive node and why? Hint: A stream operation in Netezza Performance Server Explain is an operation whose output data isn't persisted on disk but streamed to further computation nodes or snippets for a local join or local aggregation. No data is broadcast or redistributed in a stream operation. So let's walk through the questions: a. Which columns of Table CUSTOMER are used in further computations? The first node in the execution plan does a sequential scan of the CUSTOMER table on the SPUs. It estimates that 150000 rows are returned which we know is the number of rows in the CUSTOMER table. Node 1. [SPU Sequential Scan table \"CUSTOMER\" as \"C\" {(C.C_CUSTKEY)}] -- Estimated Rows = 150000, Width = 10, Cost = 0.0 .. 258.4, Conf = 100.0 **Projections:** **1:C.C_MKTSEGMENT** **[SPU Broadcast]** The statement that tells us which columns are used in further computations is the \"Projections:\" clause. We can see that only the C_MKTSEGMENT column is carried on from the CUSTOMER table. All other columns are thrown away. Since C_MKTSEGMENT is a CHAR(10) column the returned result set has a width of 10. b. Is Table CUSTOMER redistributed, broadcast or can it be joined locally? During scan the CUSTOMER table is broadcast to the other SPUs as seen by the \"[SPU Broadcast]\" clause in Node 1. This means that a complete CUSTOMER table is assembled on the host and broadcast to each SPU for further computation of the query. This may seem surprising at first since we have a substantial number of rows. But since the width of the result set is only 10 Bytes we are talking about 150000 rows * 10 Bytes = 1.5MB. This is a small amount of data for a warehousing system. c. Is Table ORDERS redistributed, broadcast or can it be joined locally? Node 2. [SPU Sequential Scan table \"ORDERS\" as \"O\" {(O.O_ORDERKEY)}] **-- Estimated Rows = 3000**, Width = 8, Cost = 0.0 .. 1653.2, Conf = 64.0 **Restrictions:** **((O.O_ORDERPRIORITY = '1-URGENT'::BPCHAR) AND (DATE_PART('YEAR'::\"VARCHAR\", O.O_ORDERDATE) = 1996))** Projections: 1:O.O_TOTALPRICE The second node of the execution plan does a scan of the ORDERS table. One column O_TOTALPRICE is projected and used in further computations. We cannot see any distribution or broadcast clauses so this table can be joined locally. This is true because the CUSTOMER table is broadcast to all SPUs. If one table of a join is broadcast the other table doesn't need any redistribution. d. In which node are the WHERE conditions applied and how many rows does Netezza Performance Server expect to fulfill the WHERE condition? We can see from the \"Restrictions\" clause in Node 2 that the WHERE conditions of our query are applied here. This should be clear since both WHERE conditions are applied to the ORDERS table and the restriction of rows can occur during the scan of the ORDERS table. As we can see in the \"Estimated Rows\" clause, the optimizer estimates a returned set of 3000 rows. We know this is not a perfect estimate since we found 46014 rows are returned from this table. e. What kind of join takes place and in which node? Node 3. **[SPU Nested Loop Stream \"Node 2\" with Temp \"Node 1\" {(O.O_ORDERKEY)}]** **-- Estimated Rows = 450000007**, Width = 18, **Cost = 660086.9 .. 7306828.5**, Conf = 64.0 Restrictions: 't'::BOOL Projections: 1:C.C_MKTSEGMENT 2:O.O_TOTALPRICE The third node of our execution plan contains the join between the two tables. It is a Nested Loop Join which means that every row of the first join set is compared to each row of the second join set. If the join condition holds true, the joined row is then added to the result set. This can be a very efficient join for small tables but for large tables its complexity is quadratic and therefore in general less fast than a Hash Join. However, Hash Joins cannot be used in the case of inequality join conditions, floating point join keys etc. Also look at the two columns that continue to be projected for further use in the query plan. These are the columns that were projected from Node 1 and Node 2. f. What is the number of estimated rows for the join? We can see in the \"Estimated Rows\" clause that the optimizer estimates this join node to return roughly 450m rows. The 450m rows is the number of rows from the first node times the number of rows from the second node. g. What is the most expensive node and why? The \"Cost\" clause provides an estimate of how expensive a Node is, where a higher cost usually leads to longer execution time. As we can see from the \"Cost\" clause in Node 3, the optimizer estimates that the join has a cost in the range from 660086.9 .. 7306828.5. This is a dramatically higher cost than what was estimated for Node 1 and Node 2. Node 4 and 5, which group and aggregate the result set, do not add much cost eitherl. So, our performance problems clearly originate in the join node 3. So, what is happening here? If we look at the query, we can assume that it is intended to compute the average order cost per market segment. This means we should join all customers to their corresponding order rows. But for this to happen we would need a join condition that joins the CUSTOMER table and the ORDERS table on the customer key. Instead the query performs a Cartesian Join, joining each customer row to each orders row. This is a very work intensive query that results in the behavior we have seen. The joined result set becomes huge. And the result does not answer the question that we intended to ask. So how do we fix this? By adding a join condition to the query that makes sure that customers are only joined to their orders. This additional join condition is O.O_CUSTKEY=C.C_CUSTKEY. Execute the following EXPLAIN command for the modified query. Input EXPLAIN VERBOSE SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; ] Output EXPLAIN VERBOSE SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"ORDERS\" as \"O\" {( O.O_ORDERKEY )}] -- Estimated Rows = 3000 , Width = 12 , Cost = 0 .0 .. 1653 .2, Conf = 64 .0 Restrictions: (( O.O_ORDERPRIORITY = '1-URGENT' ::BPCHAR ) AND ( DATE_PART ( 'YEAR' :: \"VARCHAR\" , O.O_ORDERDATE ) = 1996 )) Projections: 1 :O.O_TOTALPRICE **2:O.O_CUSTKEY** Cardinality: O.O_CUSTKEY 3 .0K ( Adjusted ) ** [ SPU Distribute on {( O.O_CUSTKEY )}] ** ** [ HashIt for Join ] ** Node 2 . [ SPU Sequential Scan table \"CUSTOMER\" as \"C\" {( C.C_CUSTKEY )}] -- Estimated Rows = 150000 , Width = 14 , Cost = 0 .0 .. 258 .4, Conf = 100 .0 Projections: 1 :C.C_MKTSEGMENT **2:C.C_CUSTKEY** Node 3 . ** [ SPU Hash Join Stream \"Node 2\" with Temp \"Node 1\" {( O.O_CUSTKEY,C.C_CUSTKEY )}] ** **-- Estimated Rows = 150000 **, Width = 18 , **Cost = 1653 .2 .. 2015 .2**, Conf = 51 .2 **Restrictions:** ** ( C.C_CUSTKEY = O.O_CUSTKEY ) ** Projections: 1 :C.C_MKTSEGMENT 2 :O.O_TOTALPRICE Cardinality: O.O_CUSTKEY 100 ( Adjusted ) ..<Removed Plan Text>.. As you can see there have been some changes to the execution plan. The ORDERS table is now scanned and distributed on the customer key rather than broadcast. The CUSTOMER table is already distributed on the customer key, so no redistribution needs to happen. Both tables are then joined in node 3 through a Hash Join on the customer key, as seen in the \"Restrictions\" clause. Also see how the join key of each table is now projected in Node 1 and Node 2 so the join can be done in Node 3. The estimated number of rows is now 150000, the same as the number of customers. Since we have a 1:n relationship between customers and orders this is as we would expect. Also, the estimated cost of Node 3 has come down by over a factor of 100! Let's make sure that the query performance has indeed improved. Switch on the display of elapsed query time with the following command: Input \\t ime If you want, you can later switch off the elapsed time display by executing the same command again. It is a toggle. Now execute our modified query: Input SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o,CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; Output C_MKTSEGMENT | AVG --------------+--------------- BUILDING | 151275 .977882 MACHINERY | 151348 .971079 HOUSEHOLD | 150196 .009267 FURNITURE | 150998 .129771 AUTOMOBILE | 151488 .825830 ( 5 rows ) Elapsed time: 0m0.796s Before we made our changes the query took so long that we couldn't wait for it to finish. After our changes the execution time has improved to slightly under a second. In this relatively simple case we might have been able to pinpoint the problem through analyzing the SQL on its own. But this can be almost impossible for complicated multi-join queries that are often used in warehousing. Reporting and BI tools tend to create very complicated portable SQL as well. In these cases, EXPLAIN can be a valuable tool to pinpoint the problem.","title":"Identifying Join Problems"},{"location":"nz-07-Query-Optimization/#html-explain","text":"In this section we will look at the HTML plangraph for the customer query that we just fixed. Besides the text descriptions of the exeution plan we used in the previous chapter, PureData System provides the ability to generate a graphical query tree as well. This is done with the help of HTML. So plangraph files can be created and viewed in your internet browser. PureData System can be configured to save a HTML plangraph or plantext file for every executed SQL query. But in this chapter we will use the basic EXPLAIN PLANGRAPH command and use Cut&Paste to export the file to your host computer. Enter the query with the keyword EXPLAIN PLANGRAPH to generate the HTML plangraph: Input EXPLAIN PLANGRAPH SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; Output NOTICE: QUERY PLAN: **<html xmlns:v = \"urn:schemas-microsoft-com:vml\" xmlns = \"http://www.w3.org/TR/REC-html40\" >** <head> <meta http-equiv = \"Content-Type\" content = \"text/html; charset=utf-8\" > <meta http-equiv = \"Generator\" content = \"Netezza Performance Server\" > <style> v \\: * { behavior:url ( #default#VML);} </style> </head> <body lang = \"en-US\" > <pre style = \"font:normal 68% verdana,arial,helvetica;background:#EEEEEE;margin-top:1em;margin-bottom:1em;margin-left:0px;padding:5pt;\" > EXPLAIN PLANGRAPH SELECT c.c_mktsegment, AVG ( o.o_totalprice ) FROM orders AS o, CUSTOMER as c WHERE EXTRACT ( YEAR FROM o.o_orderdate ) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment ; </pre> <v:textbox style = \"position:absolute;margin-left:230pt;margin-top:19pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >AGG<br/>r = 100 w = 26 s = 2 .5KB</p></v:textbox> <v:oval style = \"position:absolute;margin-left:231pt;margin-top:15pt;width:78pt;height:25pt;z-index:9;\" fillcolor = \"#808080\" ></v:oval> <v:textbox style = \"position:absolute;margin-left:230pt;margin-top:0pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >ret</p></v:textbox> <v:textbox style = \"position:absolute;margin-left:230pt;margin-top:54pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >GROUP<br/>r = 100 w = 18 s = 1 .8KB</p></v:textbox> <v:oval style = \"position:absolute;margin-left:231pt;margin-top:50pt;width:78pt;height:25pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"270pt,27pt\" to = \"270pt,62pt\" /> <v:textbox style = \"position:absolute;margin-left:233pt;margin-top:42pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >snd,m-grp</p></v:textbox> <v:textbox style = \"position:absolute;margin-left:230pt;margin-top:89pt;width:80pt;height:31pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >HASHJOIN<br/>r = 150 .0K w = 18 s = 2 .6MB<br/> ( C_CUSTKEY = O_CUSTKEY ) </p></v:textbox> <v:oval style = \"position:absolute;margin-left:231pt;margin-top:85pt;width:78pt;height:31pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"270pt,62pt\" to = \"270pt,100pt\" /> <v:textbox style = \"position:absolute;margin-left:190pt;margin-top:124pt;width:80pt;height:31pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >SEQSCAN<br/>r = 150 .0K w = 14 s = 2 .0MB<br/>C</p></v:textbox> <v:oval style = \"position:absolute;margin-left:191pt;margin-top:120pt;width:78pt;height:31pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"270pt,97pt\" to = \"230pt,135pt\" /> <v:textbox style = \"position:absolute;margin-left:270pt;margin-top:124pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >HASH<br/>r = 3 .0K w = 12 s = 35 .2KB</p></v:textbox> <v:oval style = \"position:absolute;margin-left:271pt;margin-top:120pt;width:78pt;height:25pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"270pt,97pt\" to = \"310pt,132pt\" /> <v:textbox style = \"position:absolute;margin-left:253pt;margin-top:112pt;width:80pt;height:25pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >dst {( O_CUSTKEY )} </p></v:textbox> <v:textbox style = \"position:absolute;margin-left:270pt;margin-top:159pt;width:80pt;height:31pt;z-index:10;\" > <p style = \"text-align:center;font-size:6pt;\" >SEQSCAN<br/>r = 3 .0K w = 12 s = 35 .2KB<br/>O</p></v:textbox> <v:oval style = \"position:absolute;margin-left:271pt;margin-top:155pt;width:78pt;height:31pt;z-index:9;\" ></v:oval> <v:line style = \"position:absolute;z-index:8;\" from = \"310pt,132pt\" to = \"310pt,170pt\" /> </body> **</html>** EXPLAIN Next open your host computer's text editor. If your workstation is windows open wordpad, if you use a Linux desktop use the default text editor like KEDIT, or GEDIT. Copy the output from the explain plangraph from your putty window into notepad. Make sure that you only copy the HTML file from the **<html ..** start tag to the <!--**html-->** end tag. Save the file as \"explain.html\" on your desktop. Now on your desktop double click on \"explain.html\". In windows make sure to open it with Google Chrome since this will result in the best output. You can see a graphical representation of the query we analyzed before. The left leg of the tree is the scan node of the Customer tables C, the right leg contains a scan of the Orders table O and a node hashing the result set from orders in preparation for the HASHJOIN node, that is joining the result sets of the two table scans on the customer key. After the join the result is fed into a GROUP node and an Aggregation node that computes the Average total price, before being returned to the caller. A graphical representation of the execution plan can be valuable for complicated multi-join queries to get an overview of the join. The Web Console can generate the graphically representation of the query. Run the query from \"Query Editor\". Open the NPS Web Console: <https://192.168.9.2:8443> from a browser. If the NPS Web Console doesn't start reinstall the console as follows: Stop active containers: Input docker ps Take note of the container IDs. Stop active containers: Input docker stop <container-ID> Remove the inactive containers: Input docker ps -a Take note of the container IDs. Stop active containers: Input docker rm <container-ID> Reinstall NPS Console: Input /root/cyclops_dockerrun/standalone-install.sh In the Query Editor: use: LABDB set schema: ADMIN set limit: 100 Query: SELECT c.c_mktsegment, AVG(o.o_totalprice) FROM orders AS o, CUSTOMER as c WHERE EXTRACT(YEAR FROM o.o_orderdate) = 1996 AND o.o_orderpriority = '1-URGENT' AND o.o_custkey = c.c_custkey GROUP BY c.c_mktsegment; Click \"Plan graph\" Click \"Run\" Congratulations in this lab you have used NPS system Explain functionality to analyze a query.","title":"HTML Explain"},{"location":"nz-08-Optimization-Objects/","text":"1 Optimizing Objects The Netezza Performance Server is designed to provide excellent performance in most cases without any specific tuning or index creation. One of the key technologies used to achieve this simplicity are zone maps: Automatically computed and maintained metadata about the data columns inside the extents of a database table. In general data is loaded into a data warehouses naturally ordered by a time dimension: so zone maps have the biggest performance impact on queries that restrict the time dimension as well. This approach works well for most situations, but Netezza Performance Server provides additional functionality to enhance specific workloads, which we will use in this chapter. We will first use materialized views to enhance performance of database queries against wide tables and for queries that only lookup small subsets of columns. Then we will use Cluster Based Tables to enhance query performance of queries which are using multiple lookup dimensions. Cluster Based Tables (CBT) are intended to order data based on specified columns so that the zone maps are most effective. 1.1 Objectives In the last labs we have recreated a customer database in our Netezza Performance Server system. We have picked distribution keys, loaded the data and made some first performance investigations. In this lab we will take a deeper look at some customer queries and try to enhance their performance by organizing data to use zone maps most effectively. Figure 1. LABDB database Above is the data model for our customer database. 2 Lab Setup This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using putty If you are continuing from the previous lab and are already connected to NZSQL quit the NZSQL console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/optimizationObjects/setupLab ./setupLab.sh Output DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully There may be error message at the beginning of the output since the script tries to clean up existing databases and users. Switch to the lab directory ~/labs/optimizationObjects. To do this use the following command: (Notice that you can use bash auto complete by using the Tab key to complete folder and files names) Input cd ~/labs/optimizationObjects Output [ nz@localhost optimizationObjects ] $ The command line prompt changes to reflect the directory you are in ( optimizationObjects ). 3 Materialized Views A materialized view is a view of a database table that projects a subset of the base table's columns and can be sorted on a specific set of the projected columns. When a materialized view is created, the sorted projection of the base table's data is stored in a materialized table on disk. Materialized views reduce the width of data being scanned in a base table. They are beneficial for wide tables that contain many columns (i.e. 50-500 columns) where typical queries only reference a small subset of the columns. Materialized views also provide fast, single or few record lookup operations. The thin materialized view is automatically substituted by the optimizer for the base table, allowing faster response, particularly for shorter tactical queries that examine only a small segment of the overall database table. 3.1 Wide Tables In our customer scenario we have a couple of queries that do some basic computations on the LINEITEM table, but only use a few columns of the table. If the query could automatically use another table that had only the columns used, the query could be faster. Let's look at creating this kind of \"materialized view\" table. Connect to the lab database with the following command: Input nzsql labdb labadmin Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > The first thing we need to do is to make sure table statistics have been generated so that more accurate estimated query costs can be reported by explain commands which we will be looking at. Please generate statistics for the ORDERS and LINEITEM tables using the following commands. Input GENERATE STATISTICS ON ORDERS ; GENERATE STATISTICS ON LINEITEM ; Output LABDB.ADMIN ( LABADMIN )= > GENERATE STATISTICS ON ORDERS ; GENERATE STATISTICS LABDB.ADMIN ( LABADMIN )= > GENERATE STATISTICS ON LINEITEM ; GENERATE STATISTICS The following query computes the total quantity of items shipped and their average tax rate for a given month. In this case the fourth month or April. Execute the following query: Input SELECT SUM ( L_QUANTITY ) , AVG ( L_TAX ) FROM LINEITEM WHERE EXTRACT ( MONTH FROM L_SHIPDATE ) = 4 ; Output SUM | AVG -------------+---------- 13136228 .00 | 0 .039974 ( 1 row ) Notice the EXTRACT(MONTH FROM L_SHIPDATE) command. The EXTRACT command can be used to retrieve parts of a date or time column like YEAR , MONTH or DAY . Now let's have a look at the cost of this query. To get the projected cost from the Optimizer we use the following EXPLAIN VERBOSE command: Input EXPLAIN VERBOSE SELECT SUM ( L_QUANTITY ) ,AVG ( L_TAX ) FROM LINEITEM WHERE EXTRACT ( MONTH FROM L_SHIPDATE ) = 4 ; Output EXPLAIN VERBOSE SELECT SUM ( L_QUANTITY ) , AVG ( L_TAX ) FROM LINEITEM WHERE EXTRACT ( MONTH FROM L_SHIPDATE ) = 4 ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"LINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 60012 , Width = 16 , Cost = 0 .0 .. 6907 .1, Conf = 80 .0 Restrictions: ( DATE_PART ( 'MONTH' :: \"VARCHAR\" , LINEITEM.L_SHIPDATE ) = 4 ) Projections: 1 :LINEITEM.L_QUANTITY 2 :LINEITEM.L_TAX Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 32 , Cost = 6921 .2 .. 6921 .2, Conf = 0 .0 Projections: 1 :SUM ( LINEITEM.L_QUANTITY ) 2 : ( SUM ( LINEITEM.L_TAX ) / \"NUMERIC\" ( COUNT ( LINEITEM.L_TAX ))) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. Notice the highlighted cost associated with the table scan. In our example it's a value of over 6000. Since this query is run frequently, we want to enhance the scanning performance. And since it only uses 3 of the 16 LINEITEM columns we have decided to create a materialized view covering these three columns. This should significantly increase scan speed since only a small subset of the data needs to be scanned. To create the materialized view THINLINEITEM execute the following command: Input CREATE MATERIALIZED VIEW THINLINEITEM AS SELECT L_QUANTITY, L_TAX, L_SHIPDATE FROM LINEITEM ; Output CREATE MATERIALIZED VIEW This command can take several minutes since we effectively create a copy of the three columns of the table. Repeat the explain call from step 2. Execute the following command: Input EXPLAIN VERBOSE SELECT SUM ( L_QUANTITY ) , AVG ( L_TAX ) FROM LINEITEM WHERE EXTRACT ( MONTH FROM L_SHIPDATE ) = 4 ; Output QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan mview \"_MTHINLINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 60012 , Width = 16 , Cost = 0 .0 .. 1841 .9, Conf = 80 .0 Restrictions: ( DATE_PART ( 'MONTH' :: \"VARCHAR\" , LINEITEM.L_SHIPDATE ) = 4 ) Projections: 1 :LINEITEM.L_QUANTITY 2 :LINEITEM.L_TAX Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 32 , Cost = 1856 .0 .. 1856 .0, Conf = 0 .0 Projections: 1 :SUM ( LINEITEM.L_QUANTITY ) 2 : ( SUM ( LINEITEM.L_TAX ) / \"NUMERIC\" ( COUNT ( LINEITEM.L_TAX ))) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. Notice that the Netezza Performance Server Optimizer has automatically replaced the LINEITEM table with the view THINLINEITEM. We didn't need to make any changes to the query. Also notice that the expected cost has been reduced from 6900 to 1800, which is 4 times less! In cases where you have wide database tables where queries only use a subset of the columns, a materialized view of the hot columns can significantly increase performance for these queries. And without any rewriting of the queries. 3.2 Lookup of small set of rows Materialized views not only reduce the width of tables, they can also be used in a similar way to indexes to increase the speed of queries that only access a very limited set of rows. First, we drop the view we used in the last chapter with the following command: Input DROP VIEW THINLINEITEM ; Output DROP VIEW The following command returns the number of returned shipments vs. total shipments for a specific shipping day. Execute the following command: Input SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output RET | TOTAL -----+------- 176 | 2550 ( 1 row ) You can see that on the 15^th^ June of 1995 there have been 176 returned shipments out of a total of 2550. Notice the use of the CASE statement to change the L_RETURNFLAG column into a Boolean 0-1 value, which is easily countable. We will now look at the underlying data distribution of the LINEITEM table and its zone map values. To do this exit the nzsql console by executing the \\q command. In our VM image we installed the Netezza Performance Server support tools. You can find them as an installation package in /nz/support/bin. One of these tools is the nz_zonemap tool that returns detailed information about the zone map values associated with a given database table. First let's have a look at the zone mappable columns of the LINEITEM table. Execute the following command: Input cd ~/labs/optimizationObjects /nz/support/bin/ [ nz_zonemap LABDB LINEITEM Output Database: LABDB Object Name: LINEITEM Object Type: TABLE Object ID : 201277 The zone-mapped columns are: Column # | Column Name | Data Type ----------+---------------+----------- 1 | L_ORDERKEY | INTEGER 2 | L_PARTKEY | INTEGER 3 | L_SUPPKEY | INTEGER 4 | L_LINENUMBER | INTEGER 11 | L_SHIPDATE | DATE 12 | L_COMMITDATE | DATE 13 | L_RECEIPTDATE | DATE ( 7 rows ) This command returns an overview of the zone-mappable columns of the LINEITEM table in the LABDB database. Seven of the sixteen columns have zone maps created for them. Zone-mappable columns include integer and date data types. We see that the L_SHIPDATE column we have in the WHERE condition of the customer query is zone-mappable. Now we will have a look at the zone map values for the L_SHIPDATE column. Execute the following command: Input nz_zonemap LABDB LINEITEM L_SHIPDATE Output Database: LABDB Object Name: LINEITEM Object Type: TABLE Object ID : 201277 Data Slice: 1 Column 1 : L_SHIPDATE ( DATE ) Extent # | gap | L_SHIPDATE(min) | L_SHIPDATE(max) | Sort ----------+-----+-----------------+-----------------+------ 1 | | 1992 -01-04 | 1998 -11-29 | 2 | | 1992 -01-06 | 1998 -11-30 | 3 | | 1992 -01-03 | 1998 -11-28 | 4 | | 1992 -01-02 | 1998 -11-29 | 5 | | 1992 -01-04 | 1998 -11-29 | 6 | | 1992 -01-03 | 1998 -11-28 | 7 | | 1992 -01-04 | 1998 -11-29 | 8 | | 1992 -01-04 | 1998 -11-30 | 9 | | 1992 -01-07 | 1998 -12-01 | 10 | | 1992 -01-03 | 1998 -11-28 | 11 | | 1992 -01-05 | 1998 -11-27 | 12 | | 1992 -01-03 | 1998 -12-01 | 13 | | 1992 -01-03 | 1998 -11-30 | 14 | | 1992 -01-04 | 1998 -11-30 | 15 | | 1992 -01-06 | 1998 -11-27 | 16 | | 1992 -01-03 | 1998 -11-30 | 17 | | 1992 -01-02 | 1998 -11-29 | 18 | | 1992 -01-07 | 1998 -11-29 | 19 | | 1992 -01-04 | 1998 -11-30 | 20 | | 1992 -01-04 | 1998 -11-30 | 21 | | 1992 -01-03 | 1998 -11-30 | 22 | | 1992 -01-04 | 1998 -11-29 | ( 22 rows ) This command returns a list of all extents that make up the LINEITEM table and the minimum and maximum values of the data in the L_SHIPDATE column for each extent. You can see that the LINEITEM table consists of 22 extents of data (3MB chunks on each data slice). We can also see the minimum and maximum values for the L_SHIPDATE column in each extent. These values are stored in the zone map and automatically updated when rows are inserted, updated or deleted. If a query has a where condition on the L_SHIPDATE column that falls outside of the data range of an extent, the whole extent can be discarded by Netezza Performance Server without scanning it. In this case the data has been equally distributed on all extents. This means that our query which has a WHERE condition on the 15^th^ June of 1995 (1995-06-15) doesn't profit from the zone maps and requires a full table scan. Not a single extent could be safely ruled out. Enter the NZSQL console again by entering the nzsql labdb labadmin command. Input nzsql labdb labadmin Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > We will now create a materialized view that is ordered on the L_SHIPDATE column. Execute the following command: Input CREATE MATERIALIZED VIEW SHIPLINEITEM AS SELECT L_SHIPDATE FROM LINEITEM ORDER BY L_SHIPDATE ; Output CREATE MATERIALIZED VIEW Note that our customer query has a WHERE condition on the L_SHIPDATE column but aggregates the L_RETURNFLAG column. However, we didn't add the L_RETURNFLAG column to the materialized view. We could have done it to enhance the performance of our specific query even more. But in this case we assume that there are lots of customer queries which are restricted on the ship date and access different columns of the LINEITEM table. A materialized view retains the information about the location of a parent row in the base table and can be used for lookups even if columns of the parent table are accessed in the SELECT clause. You can specify more than one order column. In this case the rows are first ordered by column one. For rows where column one has the same value the next column is used to order rows, and so on. In general, only the first order column provides a significant impact on performance. Let's have a look at the zone map of the newly created view. Leave the nzsql console again with the \\q command. Display the zone map values of the materialized view SHIPLINEITEM with the following command: Input /nz/support/bin/nz_zonemap LABDB SHIPLINEITEM L_SHIPDATE Output Database: LABDB Object Name: SHIPLINEITEM Object Type: MATERIALIZED VIEW Object ID : 201320 Data Slice: 1 Column 1 : L_SHIPDATE ( DATE ) Extent # | gap | L_SHIPDATE(min) | L_SHIPDATE(max) | Sort ----------+-----+-----------------+-----------------+------ 1 | | 1992 -01-02 | 1993 -04-12 | 2 | | 1993 -04-12 | 1994 -05-28 | true 3 | | 1994 -05-28 | 1995 -07-08 | true 4 | | 1995 -07-08 | 1996 -08-20 | true 5 | | 1996 -08-20 | 1997 -10-01 | true ( 5 rows ) We can make a couple of observations here. First the materialized view is significantly smaller than the base table, since it only contains one column. We can also see that the data values in the extent are ordered on the L_SHIPDATE column. This means that for our query, which is accessing data from the 15^th^ June of 1995, only extent 3 needs to be accessed at all, since only this extent has a data range that contains this date value. Now let's verify that our materialized view is indeed used for this query. Enter the nzsql console by entering the following command: nzsql labdb labadmin . Input nzsql labdb labadmin Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > Use the EXPLAIN command again to verify that our materialized view is used by the Optimizer: Input EXPLAIN VERBOSE SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan mview index \"_MSHIPLINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 2359 , Width = 1 , Cost = 0 .0 .. 3 .2, Conf = 80 .0 Restrictions: ( LINEITEM.L_SHIPDATE = '1995-06-15' ::DATE ) Projections: 1 :LINEITEM.L_RETURNFLAG Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 24 , Cost = 3 .5 .. 3 .5, Conf = 0 .0 Projections: 1 :SUM ( CASE WHEN ( LINEITEM.L_RETURNFLAG <> 'N' ::BPCHAR ) THEN 1 ELSE 0 END ) 2 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. Notice that the Optimizer has automatically changed the table scan to a scan of the view SHIPLINEITEM we just created. This is possible even though the projection is taking place on column L_RETURNFLAG of the base table. In some cases, you might want to disable or suspend an associated materialized view. For troubleshooting or administrative tasks on the base table. For these cases use the following command to suspend the view: Input ALTER VIEW SHIPLINEITEM MATERIALIZE SUSPEND ; Output NOTICE: MATERIALIZE SUSPEND: SHIPLINEITEM ALTER VIEW We want to make sure that the view is not used anymore during query execution. Execute the EXPLAIN command for our query again: Input EXPLAIN VERBOSE SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"LINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 2359 , Width = 1 , Cost = 0 .0 .. 6907 .1, Conf = 80 .0 Restrictions: ( LINEITEM.L_SHIPDATE = '1995-06-15' ::DATE ) Projections: 1 :LINEITEM.L_RETURNFLAG Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 24 , Cost = 6907 .5 .. 6907 .5, Conf = 0 .0 Projections: 1 :SUM ( CASE WHEN ( LINEITEM.L_RETURNFLAG <> 'N' ::BPCHAR ) THEN 1 ELSE 0 END ) 2 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] Scroll up till you see your explain query. With the view suspended we can see that the optimizer again scans the original table LINEITEM. And the cost has increased significantly. Note that we have only suspended our view not dropped it. We will now reactivate it with the following refresh command: Input ALTER VIEW SHIPLINEITEM MATERIALIZE REFRESH ; Output ```bash NOTICE: MATERIALIZE REFRESH: SHIPLINEITEM ALTER VIEW This command can also be used to reorder materialized views in case the base table has been changed. While INSERTs, UPDATEs and DELETEs into the base table are automatically reflected in associated materialized views, the view is not reordered for every change. Therefore, it is advisable to refresh them periodically -- especially after major changes to the base table. To check that the Optimizer again uses the materialized view for query execution, execute the following command: Input EXPLAIN VERBOSE SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan mview index \"_MSHIPLINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 2359 , Width = 1 , Cost = 0 .0 .. 3 .2, Conf = 80 .0 Restrictions: ( LINEITEM.L_SHIPDATE = '1995-06-15' ::DATE ) Projections: 1 :LINEITEM.L_RETURNFLAG Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 24 , Cost = 3 .5 .. 3 .5, Conf = 0 .0 Projections: 1 :SUM ( CASE WHEN ( LINEITEM.L_RETURNFLAG <> 'N' ::BPCHAR ) THEN 1 ELSE 0 END ) 2 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. Make sure that the Optimizer again uses the materialized view for its first scan operation. The output should again look like before you suspended the view. If you execute the query again you should get the same results as you got before creating the materialized view. Execute the query again: Input SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output RET | TOTAL -----+------- 176 | 2550 ( 1 row ) You have just created a materialized view to speed up queries that lookup small numbers of rows. A materialized view can provide a significant performance improvement and is transparent to end users and applications accessing the database. But it also creates additional overhead during INSERTs, UPDATEs and DELETEs, requires additional disc space, and it may require regular maintenance. Therefore, materialized views should be used sparingly. In the next chapter we will discuss an alternative approach to speed up scan speeds on a database table. 4 Cluster Based Tables (CBT) We have received a set of new customer queries on the ORDERS table that not only restricts the table by order date, but also restricts orders to a specified price range. These queries make up a significant part of the system workload and we will look at ways to increase the performance for them. The following query is a template for the queries in question. It returns the aggregated total price of all orders by order priority for a given year (in this case 1996) and price range (in this case between 150000 and 180000). SELECT O_ORDERPRIORITY, SUM(O_TOTALPRICE) FROM ORDERS WHERE EXTRACT(YEAR FROM O_ORDERDATE) = 1996 AND O_TOTALPRICE > 150000 AND O_TOTALPRICE <= 180000 GROUP BY O_ORDERPRIORITY; In this example we have a very restrictive WHERE condition on two columns: O_ORDERDATE and O_TOTALPRICE. This can help us to increase performance. The ORDERS table has around 220,000 rows with an order date of 1996 and 160,000 rows with the specified price range. But it only has 20,000 columns that satisfy both conditions. Materialized views provide their main performance improvements on one column. Also, INSERTs to the ORDERS table are frequent and time critical, so we would prefer not to use materialized views. Instead, we investigate the use of cluster based tables in this chapter. Cluster based tables are Netezza Performance Server tables that are created with an ORGANIZE ON keyword. They use a special space filling algorithm to organize a table by up to 4 columns. Zone maps for a Cluster Based Table (CBT) will provide approximately the same performance increases for all organization columns. This is useful if your query restricts a table on more than one column or if your workload consists of multiple queries hitting the same table but using different columns in WHERE conditions. In contrast to materialized views no additional disc space is needed, since the base table itself is reordered to maximize the effectiveness of zone maps. 4.1 Cluster Based Table Usage Cluster based tables are created like normal Netezza Performance Server database tables. They need to be flagged as a CBT during table creation by specifying up to four organization columns. An Netezza Performance Server table can be altered at any time to become a Cluster Based Table as well. We are going to change the create table command for ORDERS to create a Cluster Based Table. We will create a new CBT called ORDERS_CBT. Exit the nzsql console by executing the \\q command. Switch to the optimization lab directory by executing the following command: Input cd ~/labs/optimizationObjects We have supplied a script for the creation of the ORDERS_CBT table but we need to add the ORGANIZE ON (O_ORDERDATE, O_TOTALPRICE) clause to create the table as a cluster based table organized on the O_ORDERDATE and O_TOTALPRICE columns. To change the CREATE statement open the orders_cbt.sql script in the vi editor with the following command: Input vi orders_cbt.sql Enter the insert mode by pressing \"i\", the editor should now show an ---INSERT MODE--- statement in the bottom line. Navigate the cursor on the semicolon ending the statement. Press enter to move it into a new line. Enter the line \"organize on (o_orderdate, o_totalprice)\" before it. Your screen should now look like the following. create table orders_cbt ( o_orderkey integer not null , o_custkey integer not null , o_orderstatus char(1) not null , o_totalprice decimal(15,2) not null , o_orderdate date not null , o_orderpriority char(15) not null , o_clerk char(15) not null , o_shippriority integer not null , o_comment varchar(79) not null ) distribute on (o_orderkey) organize on (o_orderdate, o_totalprice); -- INSERT -- Exit the insert mode by pressing Esc . Enter :wq! In the command line and press Enter to save and exit without questions. Create and load the ORDERS_CBT table by executing the following script: Input ./create_orders_test.sh Output ERROR: relation does not exist LABDB.ADMIN.ORDERS_CBT CREATE TABLE Load session of table 'ORDERS_CBT' completed successfully This may take a couple minutes because of our virtualized environment. You may see an error message that the table ORDERS_CBT does not exist. This is expected since the script first tries to clean up an existing ORDERS_CBT table. We will now have a look at how Netezza has organized the data in this table. For this we use the nz_zonemap utility again. Execute the following command: Input /nz/support/bin/nz_zonemap labdb orders_cbt Output Database: LABDB Object Name: ORDERS_CBT Object Type: TABLE Object ID : 201883 The zone-mapped columns are: Column # | Column Name | Data Type ----------+----------------+--------------- 1 | O_ORDERKEY | INTEGER 2 | O_CUSTKEY | INTEGER 4 | O_TOTALPRICE | NUMERIC ( 15 ,2 ) 5 | O_ORDERDATE | DATE 8 | O_SHIPPRIORITY | INTEGER ( 5 rows ) This command shows you the zone mappable columns of the ORDERS_CBT table. If you compare it with the output of the nz_zonemap tool for the ORDERS table, you will see that it contains the additional column O_TOTALPRICE. Numeric columns are not zone mapped per default for performance reasons, but zone maps are created for them if they are part of the organization columns. Execute the following command to see the zone map values of the O_ORDERDATE column: Input /nz/support/bin/nz_zonemap labdb orders_cbt o_orderdate Output Database: LABDB Object Name: ORDERS_CBT Object Type: TABLE Object ID : 201883 Data Slice: 1 Column 1 : O_ORDERDATE ( DATE ) Extent # | gap | O_ORDERDATE(min) | O_ORDERDATE(max) | Sort ----------+-----+------------------+------------------+------ 1 | | 1992 -01-01 | 1998 -08-02 | 2 | | 1992 -01-01 | 1998 -08-02 | 3 | | 1992 -01-01 | 1998 -08-02 | 4 | | 1992 -01-01 | 1998 -08-02 | 5 | | 1992 -01-01 | 1998 -08-02 | 6 | | 1992 -01-01 | 1998 -08-02 | ( 6 rows ) This is unexpected. Since we used O_ORDERDATE as an organization column we would have expected an ordering in the data values. But they are again distributed equally over all extents. The reason for this is that the organization process takes place during a command called GROOM, not during a nzload of the table. Instead of creating a new table we could also have altered the existing ORDERS table to become a Cluster Based Table. Creating or altering a table to become a CBT doesn't change the physical table layout until the groom command has been used. This command will be covered in detail in the following presentation and lab. But we will use it in the next chapter to reorganize the table. 4.2 Cluster Based Table Maintenance When a table is created as a CBT in Netezza the data isn't organized during load time. Similar to ordered materialized views, a Cluster Based Table can become partially unordered due to INSERTs, UPDATEs and DELETEs. A threshold is defined for reorganization and the groom command can be used at any time to reorganize a CBT, based on its organization keys. To organize the table you created in the last chapter you need to switch to the nzsql console again. Execute the following command: nzsql labdb labadmin Input nzsql labdb labadmin Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > Execute the following command to groom your cluster-based table: Input groom table orders_cbt ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2021 -04-01 05 :56:33. NOTICE: Groom processed 588 pages ; purged 0 records ; scan size grew by 32 pages ; table size unchanged. GROOM ORGANIZE READY This command does a variety of things which will be covered in a further presentation and lab. In this case it organizes the CBT based on its organization keys. Warning This command requires a lot of RAM on the SPUs to operate. Our VMWare systems have been tuned so the command should be able to finish. Since the whole table is reordered it may take a couple of minutes to finish but should you get the impression that the system is stuck please inform the lecturer. Let's have a look at the data organization in the table. To do this quit the nzsql console with the \\q command. Review the zone maps of the two organization columns by executing the following command: Input /nz/support/bin/nz_zonemap labdb orders_cbt o_orderdate o_totalprice Output Database: LABDB Object Name: ORDERS_CBT Object Type: TABLE Object ID : 201883 Data Slice: 1 Column 1 : O_ORDERDATE ( DATE ) Column 2 : O_TOTALPRICE ( NUMERIC ( 15 ,2 )) Extent # | gap | O_ORDERDATE(min) | O_ORDERDATE(max) | O_TOTALPRICE(min) | O_TOTALPRICE(max) ----------+-----+------------------+------------------+--------------------+-------------------- 1 | | 1992 -01-01 | 1995 -04-18 | 875 .52 | 111093 .84 2 | | 1992 -01-01 | 1995 -04-18 | 77984 .34 | 215553 .23 3 | | 1992 -01-01 | 1995 -04-18 | 178526 .74 | 555285 .16 4 | | 1993 -08-27 | 1996 -12-08 | 144451 .84 | 487405 .74 5 | | 1996 -06-22 | 1998 -08-02 | 77992 .67 | 530604 .44 6 | | 1995 -04-18 | 1998 -03-05 | 945 .99 | 144446 .76 ( 6 rows ) Your results should look like the above (we removed the \"SORT\" columns from the results to make it more readable) You can see that both columns have some form of order now. Our query is restricting rows in two ranges Condition 1: O_ORDERDATE = 1996 AND Condition 2: 150000 < O_TOTALPRICE <= 180000 Below we have summarized the Extents on the full extent level of 3MB from our above result. Min(Date) Max(Date) Min(Price) Max(Price) Cond 1 Cond 2 Both Cond 1992-01-01 1995-04-18 875.52 111093.84 1992-01-01 1995-04-18 77984.34 215553.23 X 1992-01-01 1995-04-18 178526.74 555285.16 X 1993-08-27 1996-12-08 144451.84 487405.74 X X X 1996-06-22 1998-08-02 77992.67 530604.44 X X X 1995-04-18 1998-03-05 945.99 144446.76 X As you can see there are now 3 extents that have rows from 1996 in them and 4 extents that contain rows in the price range from 150000 to 180000. But we have only two extents that contains rows that satisfy both conditions and needs to be scanned during query execution. The above shows the zone map ranges at the extent boundaries of 3MB. But Netezza Performance Server zone maps are even kept at the more granular level of pages. Netezza page size is 128K, and so there are 24 pages in one extent. You can look at the more granular zone map ranges for pages using the \"-page\" option of the nz_zonemap command: Input /nz/support/bin/nz_zonemap labdb orders_cbt o_orderdate o_totalprice -page Output Database: LABDB Object Name: ORDERS_CBT Object Type: TABLE Object ID : 201883 Data Slice: 1 Column 1 : O_ORDERDATE ( DATE ) Column 2 : O_TOTALPRICE ( NUMERIC ( 15 ,2 )) Extent # | Page # | O_ORDERDATE(min) | O_ORDERDATE(max) | O_TOTALPRICE(min) | O_TOTALPRICE(max) ----------+--------+------------------+------------------+--------------------+------------- ------+ 1 | extent | 1992 -01-01 | 1995 -04-18 | 875 .52 | 111091 .08 | 1 | 1 | 1992 -01-01 | 1992 -10-28 | 912 .10 | 26749 .39 | 1 | 2 | 1992 -03-17 | 1992 -10-28 | 15080 .65 | 45906 .12 | 1 | 3 | 1992 -01-01 | 1992 -05-31 | 26812 .15 | 65825 .06 | 1 | 4 | 1992 -01-01 | 1992 -10-27 | 61941 .14 | 77977 .57 | 1 | 5 | 1992 -05-31 | 1993 -03-28 | 45915 .67 | 71760 .90 | 1 | 6 | 1992 -10-28 | 1993 -04-25 | 49997 .93 | 77978 .41 | 1 | 7 | 1993 -03-28 | 1993 -08-27 | 45922 .72 | 77973 .85 | 1 | 8 | 1992 -10-29 | 1993 -08-27 | 26801 .08 | 54004 .34 | 1 | 9 | 1992 -10-28 | 1993 -06-12 | 961 .54 | 41500 .41 | 1 | 10 | 1993 -03-28 | 1994 -01-25 | 875 .52 | 26798 .03 | 1 | 11 | 1993 -08-27 | 1994 -04-09 | 15020 .44 | 45905 .64 | 1 | 12 | 1994 -01-25 | 1994 -07-29 | 947 .81 | 45779 .43 | 1 | 13 | 1994 -06-22 | 1995 -04-18 | 1004 .66 | 26775 .18 | 1 | 14 | 1994 -09-06 | 1995 -04-18 | 15087 .48 | 45906 .49 | 1 | 15 | 1994 -06-23 | 1994 -11-19 | 26821 .38 | 61934 .12 | 1 | 16 | 1994 -10-01 | 1995 -04-18 | 45916 .16 | 73765 .70 | 1 | 17 | 1994 -04-09 | 1995 -04-17 | 61957 .24 | 77974 .31 | 1 | 18 | 1994 -01-25 | 1994 -06-22 | 45917 .62 | 77944 .43 | 1 | 19 | 1993 -08-27 | 1994 -04-09 | 45926 .79 | 73846 .86 | 1 | 20 | 1993 -08-27 | 1994 -01-25 | 69917 .85 | 102685 .89 | 1 | 21 | 1993 -08-27 | 1994 -06-22 | 94343 .90 | 111089 .11 | 1 | 22 | 1994 -01-25 | 1994 -10-02 | 77986 .61 | 100585 .11 | 1 | 23 | 1994 -09-06 | 1995 -04-18 | 77983 .23 | 102725 .22 | 1 | 24 | 1994 -06-22 | 1995 -04-18 | 94331 .09 | 111091 .08 | 2 | extent | 1992 -01-01 | 1995 -04-18 | 77984 .34 | 215553 .23 | 2 | 1 | 1994 -06-22 | 1995 -04-18 | 106892 .35 | 127708 .22 | 2 | 2 | 1994 -09-05 | 1995 -04-17 | 119478 .05 | 144451 .22 | ... ( 161 rows ) There are 161 zone map ranges where reading the page can potentially be eliminated, not just 6 ranges on the extent boundaries. Of the 161 pages, the number that satisfy the conditions and need to be read are: 43 pages that might have O_ORDERDATE in 1996 54 pages that might have O_TOTALPRICE between 150000 and 180000 9 pages for which both conditions apply This means by using CBTs in Netezza Performance Server architecture we can restrict the amount of data that needs to be queried by a factor of 16. This is 3-4 times less than would need to be read if the table is only ordered on a single column. Congratulations, you have finished the Optimization Objects lab. In this lab you have created materialized views to speedup scans of wide tables and queries that only look up small numbers of rows. Finally, you created a Cluster Based Table and used the groom command to organize it. Throughout the lab you have used the nz_zonemap tool to see zone maps and get a better idea on how data is stored in the Netezza appliance.","title":"Optimization Objects"},{"location":"nz-08-Optimization-Objects/#1-optimizing-objects","text":"The Netezza Performance Server is designed to provide excellent performance in most cases without any specific tuning or index creation. One of the key technologies used to achieve this simplicity are zone maps: Automatically computed and maintained metadata about the data columns inside the extents of a database table. In general data is loaded into a data warehouses naturally ordered by a time dimension: so zone maps have the biggest performance impact on queries that restrict the time dimension as well. This approach works well for most situations, but Netezza Performance Server provides additional functionality to enhance specific workloads, which we will use in this chapter. We will first use materialized views to enhance performance of database queries against wide tables and for queries that only lookup small subsets of columns. Then we will use Cluster Based Tables to enhance query performance of queries which are using multiple lookup dimensions. Cluster Based Tables (CBT) are intended to order data based on specified columns so that the zone maps are most effective.","title":"1 Optimizing Objects"},{"location":"nz-08-Optimization-Objects/#11-objectives","text":"In the last labs we have recreated a customer database in our Netezza Performance Server system. We have picked distribution keys, loaded the data and made some first performance investigations. In this lab we will take a deeper look at some customer queries and try to enhance their performance by organizing data to use zone maps most effectively. Figure 1. LABDB database Above is the data model for our customer database.","title":"1.1 Objectives"},{"location":"nz-08-Optimization-Objects/#2-lab-setup","text":"This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using putty If you are continuing from the previous lab and are already connected to NZSQL quit the NZSQL console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/optimizationObjects/setupLab ./setupLab.sh Output DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully There may be error message at the beginning of the output since the script tries to clean up existing databases and users. Switch to the lab directory ~/labs/optimizationObjects. To do this use the following command: (Notice that you can use bash auto complete by using the Tab key to complete folder and files names) Input cd ~/labs/optimizationObjects Output [ nz@localhost optimizationObjects ] $ The command line prompt changes to reflect the directory you are in ( optimizationObjects ).","title":"2 Lab Setup"},{"location":"nz-08-Optimization-Objects/#3-materialized-views","text":"A materialized view is a view of a database table that projects a subset of the base table's columns and can be sorted on a specific set of the projected columns. When a materialized view is created, the sorted projection of the base table's data is stored in a materialized table on disk. Materialized views reduce the width of data being scanned in a base table. They are beneficial for wide tables that contain many columns (i.e. 50-500 columns) where typical queries only reference a small subset of the columns. Materialized views also provide fast, single or few record lookup operations. The thin materialized view is automatically substituted by the optimizer for the base table, allowing faster response, particularly for shorter tactical queries that examine only a small segment of the overall database table.","title":"3 Materialized Views"},{"location":"nz-08-Optimization-Objects/#31-wide-tables","text":"In our customer scenario we have a couple of queries that do some basic computations on the LINEITEM table, but only use a few columns of the table. If the query could automatically use another table that had only the columns used, the query could be faster. Let's look at creating this kind of \"materialized view\" table. Connect to the lab database with the following command: Input nzsql labdb labadmin Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > The first thing we need to do is to make sure table statistics have been generated so that more accurate estimated query costs can be reported by explain commands which we will be looking at. Please generate statistics for the ORDERS and LINEITEM tables using the following commands. Input GENERATE STATISTICS ON ORDERS ; GENERATE STATISTICS ON LINEITEM ; Output LABDB.ADMIN ( LABADMIN )= > GENERATE STATISTICS ON ORDERS ; GENERATE STATISTICS LABDB.ADMIN ( LABADMIN )= > GENERATE STATISTICS ON LINEITEM ; GENERATE STATISTICS The following query computes the total quantity of items shipped and their average tax rate for a given month. In this case the fourth month or April. Execute the following query: Input SELECT SUM ( L_QUANTITY ) , AVG ( L_TAX ) FROM LINEITEM WHERE EXTRACT ( MONTH FROM L_SHIPDATE ) = 4 ; Output SUM | AVG -------------+---------- 13136228 .00 | 0 .039974 ( 1 row ) Notice the EXTRACT(MONTH FROM L_SHIPDATE) command. The EXTRACT command can be used to retrieve parts of a date or time column like YEAR , MONTH or DAY . Now let's have a look at the cost of this query. To get the projected cost from the Optimizer we use the following EXPLAIN VERBOSE command: Input EXPLAIN VERBOSE SELECT SUM ( L_QUANTITY ) ,AVG ( L_TAX ) FROM LINEITEM WHERE EXTRACT ( MONTH FROM L_SHIPDATE ) = 4 ; Output EXPLAIN VERBOSE SELECT SUM ( L_QUANTITY ) , AVG ( L_TAX ) FROM LINEITEM WHERE EXTRACT ( MONTH FROM L_SHIPDATE ) = 4 ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"LINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 60012 , Width = 16 , Cost = 0 .0 .. 6907 .1, Conf = 80 .0 Restrictions: ( DATE_PART ( 'MONTH' :: \"VARCHAR\" , LINEITEM.L_SHIPDATE ) = 4 ) Projections: 1 :LINEITEM.L_QUANTITY 2 :LINEITEM.L_TAX Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 32 , Cost = 6921 .2 .. 6921 .2, Conf = 0 .0 Projections: 1 :SUM ( LINEITEM.L_QUANTITY ) 2 : ( SUM ( LINEITEM.L_TAX ) / \"NUMERIC\" ( COUNT ( LINEITEM.L_TAX ))) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. Notice the highlighted cost associated with the table scan. In our example it's a value of over 6000. Since this query is run frequently, we want to enhance the scanning performance. And since it only uses 3 of the 16 LINEITEM columns we have decided to create a materialized view covering these three columns. This should significantly increase scan speed since only a small subset of the data needs to be scanned. To create the materialized view THINLINEITEM execute the following command: Input CREATE MATERIALIZED VIEW THINLINEITEM AS SELECT L_QUANTITY, L_TAX, L_SHIPDATE FROM LINEITEM ; Output CREATE MATERIALIZED VIEW This command can take several minutes since we effectively create a copy of the three columns of the table. Repeat the explain call from step 2. Execute the following command: Input EXPLAIN VERBOSE SELECT SUM ( L_QUANTITY ) , AVG ( L_TAX ) FROM LINEITEM WHERE EXTRACT ( MONTH FROM L_SHIPDATE ) = 4 ; Output QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan mview \"_MTHINLINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 60012 , Width = 16 , Cost = 0 .0 .. 1841 .9, Conf = 80 .0 Restrictions: ( DATE_PART ( 'MONTH' :: \"VARCHAR\" , LINEITEM.L_SHIPDATE ) = 4 ) Projections: 1 :LINEITEM.L_QUANTITY 2 :LINEITEM.L_TAX Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 32 , Cost = 1856 .0 .. 1856 .0, Conf = 0 .0 Projections: 1 :SUM ( LINEITEM.L_QUANTITY ) 2 : ( SUM ( LINEITEM.L_TAX ) / \"NUMERIC\" ( COUNT ( LINEITEM.L_TAX ))) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. Notice that the Netezza Performance Server Optimizer has automatically replaced the LINEITEM table with the view THINLINEITEM. We didn't need to make any changes to the query. Also notice that the expected cost has been reduced from 6900 to 1800, which is 4 times less! In cases where you have wide database tables where queries only use a subset of the columns, a materialized view of the hot columns can significantly increase performance for these queries. And without any rewriting of the queries.","title":"3.1 Wide Tables"},{"location":"nz-08-Optimization-Objects/#32-lookup-of-small-set-of-rows","text":"Materialized views not only reduce the width of tables, they can also be used in a similar way to indexes to increase the speed of queries that only access a very limited set of rows. First, we drop the view we used in the last chapter with the following command: Input DROP VIEW THINLINEITEM ; Output DROP VIEW The following command returns the number of returned shipments vs. total shipments for a specific shipping day. Execute the following command: Input SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output RET | TOTAL -----+------- 176 | 2550 ( 1 row ) You can see that on the 15^th^ June of 1995 there have been 176 returned shipments out of a total of 2550. Notice the use of the CASE statement to change the L_RETURNFLAG column into a Boolean 0-1 value, which is easily countable. We will now look at the underlying data distribution of the LINEITEM table and its zone map values. To do this exit the nzsql console by executing the \\q command. In our VM image we installed the Netezza Performance Server support tools. You can find them as an installation package in /nz/support/bin. One of these tools is the nz_zonemap tool that returns detailed information about the zone map values associated with a given database table. First let's have a look at the zone mappable columns of the LINEITEM table. Execute the following command: Input cd ~/labs/optimizationObjects /nz/support/bin/ [ nz_zonemap LABDB LINEITEM Output Database: LABDB Object Name: LINEITEM Object Type: TABLE Object ID : 201277 The zone-mapped columns are: Column # | Column Name | Data Type ----------+---------------+----------- 1 | L_ORDERKEY | INTEGER 2 | L_PARTKEY | INTEGER 3 | L_SUPPKEY | INTEGER 4 | L_LINENUMBER | INTEGER 11 | L_SHIPDATE | DATE 12 | L_COMMITDATE | DATE 13 | L_RECEIPTDATE | DATE ( 7 rows ) This command returns an overview of the zone-mappable columns of the LINEITEM table in the LABDB database. Seven of the sixteen columns have zone maps created for them. Zone-mappable columns include integer and date data types. We see that the L_SHIPDATE column we have in the WHERE condition of the customer query is zone-mappable. Now we will have a look at the zone map values for the L_SHIPDATE column. Execute the following command: Input nz_zonemap LABDB LINEITEM L_SHIPDATE Output Database: LABDB Object Name: LINEITEM Object Type: TABLE Object ID : 201277 Data Slice: 1 Column 1 : L_SHIPDATE ( DATE ) Extent # | gap | L_SHIPDATE(min) | L_SHIPDATE(max) | Sort ----------+-----+-----------------+-----------------+------ 1 | | 1992 -01-04 | 1998 -11-29 | 2 | | 1992 -01-06 | 1998 -11-30 | 3 | | 1992 -01-03 | 1998 -11-28 | 4 | | 1992 -01-02 | 1998 -11-29 | 5 | | 1992 -01-04 | 1998 -11-29 | 6 | | 1992 -01-03 | 1998 -11-28 | 7 | | 1992 -01-04 | 1998 -11-29 | 8 | | 1992 -01-04 | 1998 -11-30 | 9 | | 1992 -01-07 | 1998 -12-01 | 10 | | 1992 -01-03 | 1998 -11-28 | 11 | | 1992 -01-05 | 1998 -11-27 | 12 | | 1992 -01-03 | 1998 -12-01 | 13 | | 1992 -01-03 | 1998 -11-30 | 14 | | 1992 -01-04 | 1998 -11-30 | 15 | | 1992 -01-06 | 1998 -11-27 | 16 | | 1992 -01-03 | 1998 -11-30 | 17 | | 1992 -01-02 | 1998 -11-29 | 18 | | 1992 -01-07 | 1998 -11-29 | 19 | | 1992 -01-04 | 1998 -11-30 | 20 | | 1992 -01-04 | 1998 -11-30 | 21 | | 1992 -01-03 | 1998 -11-30 | 22 | | 1992 -01-04 | 1998 -11-29 | ( 22 rows ) This command returns a list of all extents that make up the LINEITEM table and the minimum and maximum values of the data in the L_SHIPDATE column for each extent. You can see that the LINEITEM table consists of 22 extents of data (3MB chunks on each data slice). We can also see the minimum and maximum values for the L_SHIPDATE column in each extent. These values are stored in the zone map and automatically updated when rows are inserted, updated or deleted. If a query has a where condition on the L_SHIPDATE column that falls outside of the data range of an extent, the whole extent can be discarded by Netezza Performance Server without scanning it. In this case the data has been equally distributed on all extents. This means that our query which has a WHERE condition on the 15^th^ June of 1995 (1995-06-15) doesn't profit from the zone maps and requires a full table scan. Not a single extent could be safely ruled out. Enter the NZSQL console again by entering the nzsql labdb labadmin command. Input nzsql labdb labadmin Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > We will now create a materialized view that is ordered on the L_SHIPDATE column. Execute the following command: Input CREATE MATERIALIZED VIEW SHIPLINEITEM AS SELECT L_SHIPDATE FROM LINEITEM ORDER BY L_SHIPDATE ; Output CREATE MATERIALIZED VIEW Note that our customer query has a WHERE condition on the L_SHIPDATE column but aggregates the L_RETURNFLAG column. However, we didn't add the L_RETURNFLAG column to the materialized view. We could have done it to enhance the performance of our specific query even more. But in this case we assume that there are lots of customer queries which are restricted on the ship date and access different columns of the LINEITEM table. A materialized view retains the information about the location of a parent row in the base table and can be used for lookups even if columns of the parent table are accessed in the SELECT clause. You can specify more than one order column. In this case the rows are first ordered by column one. For rows where column one has the same value the next column is used to order rows, and so on. In general, only the first order column provides a significant impact on performance. Let's have a look at the zone map of the newly created view. Leave the nzsql console again with the \\q command. Display the zone map values of the materialized view SHIPLINEITEM with the following command: Input /nz/support/bin/nz_zonemap LABDB SHIPLINEITEM L_SHIPDATE Output Database: LABDB Object Name: SHIPLINEITEM Object Type: MATERIALIZED VIEW Object ID : 201320 Data Slice: 1 Column 1 : L_SHIPDATE ( DATE ) Extent # | gap | L_SHIPDATE(min) | L_SHIPDATE(max) | Sort ----------+-----+-----------------+-----------------+------ 1 | | 1992 -01-02 | 1993 -04-12 | 2 | | 1993 -04-12 | 1994 -05-28 | true 3 | | 1994 -05-28 | 1995 -07-08 | true 4 | | 1995 -07-08 | 1996 -08-20 | true 5 | | 1996 -08-20 | 1997 -10-01 | true ( 5 rows ) We can make a couple of observations here. First the materialized view is significantly smaller than the base table, since it only contains one column. We can also see that the data values in the extent are ordered on the L_SHIPDATE column. This means that for our query, which is accessing data from the 15^th^ June of 1995, only extent 3 needs to be accessed at all, since only this extent has a data range that contains this date value. Now let's verify that our materialized view is indeed used for this query. Enter the nzsql console by entering the following command: nzsql labdb labadmin . Input nzsql labdb labadmin Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > Use the EXPLAIN command again to verify that our materialized view is used by the Optimizer: Input EXPLAIN VERBOSE SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan mview index \"_MSHIPLINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 2359 , Width = 1 , Cost = 0 .0 .. 3 .2, Conf = 80 .0 Restrictions: ( LINEITEM.L_SHIPDATE = '1995-06-15' ::DATE ) Projections: 1 :LINEITEM.L_RETURNFLAG Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 24 , Cost = 3 .5 .. 3 .5, Conf = 0 .0 Projections: 1 :SUM ( CASE WHEN ( LINEITEM.L_RETURNFLAG <> 'N' ::BPCHAR ) THEN 1 ELSE 0 END ) 2 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. Notice that the Optimizer has automatically changed the table scan to a scan of the view SHIPLINEITEM we just created. This is possible even though the projection is taking place on column L_RETURNFLAG of the base table. In some cases, you might want to disable or suspend an associated materialized view. For troubleshooting or administrative tasks on the base table. For these cases use the following command to suspend the view: Input ALTER VIEW SHIPLINEITEM MATERIALIZE SUSPEND ; Output NOTICE: MATERIALIZE SUSPEND: SHIPLINEITEM ALTER VIEW We want to make sure that the view is not used anymore during query execution. Execute the EXPLAIN command for our query again: Input EXPLAIN VERBOSE SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"LINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 2359 , Width = 1 , Cost = 0 .0 .. 6907 .1, Conf = 80 .0 Restrictions: ( LINEITEM.L_SHIPDATE = '1995-06-15' ::DATE ) Projections: 1 :LINEITEM.L_RETURNFLAG Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 24 , Cost = 6907 .5 .. 6907 .5, Conf = 0 .0 Projections: 1 :SUM ( CASE WHEN ( LINEITEM.L_RETURNFLAG <> 'N' ::BPCHAR ) THEN 1 ELSE 0 END ) 2 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] Scroll up till you see your explain query. With the view suspended we can see that the optimizer again scans the original table LINEITEM. And the cost has increased significantly. Note that we have only suspended our view not dropped it. We will now reactivate it with the following refresh command: Input ALTER VIEW SHIPLINEITEM MATERIALIZE REFRESH ; Output ```bash NOTICE: MATERIALIZE REFRESH: SHIPLINEITEM ALTER VIEW This command can also be used to reorder materialized views in case the base table has been changed. While INSERTs, UPDATEs and DELETEs into the base table are automatically reflected in associated materialized views, the view is not reordered for every change. Therefore, it is advisable to refresh them periodically -- especially after major changes to the base table. To check that the Optimizer again uses the materialized view for query execution, execute the following command: Input EXPLAIN VERBOSE SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan mview index \"_MSHIPLINEITEM\" {( LINEITEM.L_ORDERKEY )}] -- Estimated Rows = 2359 , Width = 1 , Cost = 0 .0 .. 3 .2, Conf = 80 .0 Restrictions: ( LINEITEM.L_SHIPDATE = '1995-06-15' ::DATE ) Projections: 1 :LINEITEM.L_RETURNFLAG Node 2 . [ SPU Aggregate ] -- Estimated Rows = 1 , Width = 24 , Cost = 3 .5 .. 3 .5, Conf = 0 .0 Projections: 1 :SUM ( CASE WHEN ( LINEITEM.L_RETURNFLAG <> 'N' ::BPCHAR ) THEN 1 ELSE 0 END ) 2 :COUNT ( * ) [ SPU Return ] [ HOST Merge Aggs ] [ Host Return ] ..< Removed Plan Text >.. Make sure that the Optimizer again uses the materialized view for its first scan operation. The output should again look like before you suspended the view. If you execute the query again you should get the same results as you got before creating the materialized view. Execute the query again: Input SELECT SUM ( CASE WHEN L_RETURNFLAG <> 'N' THEN 1 ELSE 0 END ) AS RET, COUNT ( * ) AS TOTAL FROM LINEITEM WHERE L_SHIPDATE = '1995-06-15' ; Output RET | TOTAL -----+------- 176 | 2550 ( 1 row ) You have just created a materialized view to speed up queries that lookup small numbers of rows. A materialized view can provide a significant performance improvement and is transparent to end users and applications accessing the database. But it also creates additional overhead during INSERTs, UPDATEs and DELETEs, requires additional disc space, and it may require regular maintenance. Therefore, materialized views should be used sparingly. In the next chapter we will discuss an alternative approach to speed up scan speeds on a database table.","title":"3.2 Lookup of small set of rows"},{"location":"nz-08-Optimization-Objects/#4-cluster-based-tables-cbt","text":"We have received a set of new customer queries on the ORDERS table that not only restricts the table by order date, but also restricts orders to a specified price range. These queries make up a significant part of the system workload and we will look at ways to increase the performance for them. The following query is a template for the queries in question. It returns the aggregated total price of all orders by order priority for a given year (in this case 1996) and price range (in this case between 150000 and 180000). SELECT O_ORDERPRIORITY, SUM(O_TOTALPRICE) FROM ORDERS WHERE EXTRACT(YEAR FROM O_ORDERDATE) = 1996 AND O_TOTALPRICE > 150000 AND O_TOTALPRICE <= 180000 GROUP BY O_ORDERPRIORITY; In this example we have a very restrictive WHERE condition on two columns: O_ORDERDATE and O_TOTALPRICE. This can help us to increase performance. The ORDERS table has around 220,000 rows with an order date of 1996 and 160,000 rows with the specified price range. But it only has 20,000 columns that satisfy both conditions. Materialized views provide their main performance improvements on one column. Also, INSERTs to the ORDERS table are frequent and time critical, so we would prefer not to use materialized views. Instead, we investigate the use of cluster based tables in this chapter. Cluster based tables are Netezza Performance Server tables that are created with an ORGANIZE ON keyword. They use a special space filling algorithm to organize a table by up to 4 columns. Zone maps for a Cluster Based Table (CBT) will provide approximately the same performance increases for all organization columns. This is useful if your query restricts a table on more than one column or if your workload consists of multiple queries hitting the same table but using different columns in WHERE conditions. In contrast to materialized views no additional disc space is needed, since the base table itself is reordered to maximize the effectiveness of zone maps.","title":"4 Cluster Based Tables (CBT)"},{"location":"nz-08-Optimization-Objects/#41-cluster-based-table-usage","text":"Cluster based tables are created like normal Netezza Performance Server database tables. They need to be flagged as a CBT during table creation by specifying up to four organization columns. An Netezza Performance Server table can be altered at any time to become a Cluster Based Table as well. We are going to change the create table command for ORDERS to create a Cluster Based Table. We will create a new CBT called ORDERS_CBT. Exit the nzsql console by executing the \\q command. Switch to the optimization lab directory by executing the following command: Input cd ~/labs/optimizationObjects We have supplied a script for the creation of the ORDERS_CBT table but we need to add the ORGANIZE ON (O_ORDERDATE, O_TOTALPRICE) clause to create the table as a cluster based table organized on the O_ORDERDATE and O_TOTALPRICE columns. To change the CREATE statement open the orders_cbt.sql script in the vi editor with the following command: Input vi orders_cbt.sql Enter the insert mode by pressing \"i\", the editor should now show an ---INSERT MODE--- statement in the bottom line. Navigate the cursor on the semicolon ending the statement. Press enter to move it into a new line. Enter the line \"organize on (o_orderdate, o_totalprice)\" before it. Your screen should now look like the following. create table orders_cbt ( o_orderkey integer not null , o_custkey integer not null , o_orderstatus char(1) not null , o_totalprice decimal(15,2) not null , o_orderdate date not null , o_orderpriority char(15) not null , o_clerk char(15) not null , o_shippriority integer not null , o_comment varchar(79) not null ) distribute on (o_orderkey) organize on (o_orderdate, o_totalprice); -- INSERT -- Exit the insert mode by pressing Esc . Enter :wq! In the command line and press Enter to save and exit without questions. Create and load the ORDERS_CBT table by executing the following script: Input ./create_orders_test.sh Output ERROR: relation does not exist LABDB.ADMIN.ORDERS_CBT CREATE TABLE Load session of table 'ORDERS_CBT' completed successfully This may take a couple minutes because of our virtualized environment. You may see an error message that the table ORDERS_CBT does not exist. This is expected since the script first tries to clean up an existing ORDERS_CBT table. We will now have a look at how Netezza has organized the data in this table. For this we use the nz_zonemap utility again. Execute the following command: Input /nz/support/bin/nz_zonemap labdb orders_cbt Output Database: LABDB Object Name: ORDERS_CBT Object Type: TABLE Object ID : 201883 The zone-mapped columns are: Column # | Column Name | Data Type ----------+----------------+--------------- 1 | O_ORDERKEY | INTEGER 2 | O_CUSTKEY | INTEGER 4 | O_TOTALPRICE | NUMERIC ( 15 ,2 ) 5 | O_ORDERDATE | DATE 8 | O_SHIPPRIORITY | INTEGER ( 5 rows ) This command shows you the zone mappable columns of the ORDERS_CBT table. If you compare it with the output of the nz_zonemap tool for the ORDERS table, you will see that it contains the additional column O_TOTALPRICE. Numeric columns are not zone mapped per default for performance reasons, but zone maps are created for them if they are part of the organization columns. Execute the following command to see the zone map values of the O_ORDERDATE column: Input /nz/support/bin/nz_zonemap labdb orders_cbt o_orderdate Output Database: LABDB Object Name: ORDERS_CBT Object Type: TABLE Object ID : 201883 Data Slice: 1 Column 1 : O_ORDERDATE ( DATE ) Extent # | gap | O_ORDERDATE(min) | O_ORDERDATE(max) | Sort ----------+-----+------------------+------------------+------ 1 | | 1992 -01-01 | 1998 -08-02 | 2 | | 1992 -01-01 | 1998 -08-02 | 3 | | 1992 -01-01 | 1998 -08-02 | 4 | | 1992 -01-01 | 1998 -08-02 | 5 | | 1992 -01-01 | 1998 -08-02 | 6 | | 1992 -01-01 | 1998 -08-02 | ( 6 rows ) This is unexpected. Since we used O_ORDERDATE as an organization column we would have expected an ordering in the data values. But they are again distributed equally over all extents. The reason for this is that the organization process takes place during a command called GROOM, not during a nzload of the table. Instead of creating a new table we could also have altered the existing ORDERS table to become a Cluster Based Table. Creating or altering a table to become a CBT doesn't change the physical table layout until the groom command has been used. This command will be covered in detail in the following presentation and lab. But we will use it in the next chapter to reorganize the table. 4.2 Cluster Based Table Maintenance When a table is created as a CBT in Netezza the data isn't organized during load time. Similar to ordered materialized views, a Cluster Based Table can become partially unordered due to INSERTs, UPDATEs and DELETEs. A threshold is defined for reorganization and the groom command can be used at any time to reorganize a CBT, based on its organization keys. To organize the table you created in the last chapter you need to switch to the nzsql console again. Execute the following command: nzsql labdb labadmin Input nzsql labdb labadmin Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN ( LABADMIN )= > Execute the following command to groom your cluster-based table: Input groom table orders_cbt ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2021 -04-01 05 :56:33. NOTICE: Groom processed 588 pages ; purged 0 records ; scan size grew by 32 pages ; table size unchanged. GROOM ORGANIZE READY This command does a variety of things which will be covered in a further presentation and lab. In this case it organizes the CBT based on its organization keys. Warning This command requires a lot of RAM on the SPUs to operate. Our VMWare systems have been tuned so the command should be able to finish. Since the whole table is reordered it may take a couple of minutes to finish but should you get the impression that the system is stuck please inform the lecturer. Let's have a look at the data organization in the table. To do this quit the nzsql console with the \\q command. Review the zone maps of the two organization columns by executing the following command: Input /nz/support/bin/nz_zonemap labdb orders_cbt o_orderdate o_totalprice Output Database: LABDB Object Name: ORDERS_CBT Object Type: TABLE Object ID : 201883 Data Slice: 1 Column 1 : O_ORDERDATE ( DATE ) Column 2 : O_TOTALPRICE ( NUMERIC ( 15 ,2 )) Extent # | gap | O_ORDERDATE(min) | O_ORDERDATE(max) | O_TOTALPRICE(min) | O_TOTALPRICE(max) ----------+-----+------------------+------------------+--------------------+-------------------- 1 | | 1992 -01-01 | 1995 -04-18 | 875 .52 | 111093 .84 2 | | 1992 -01-01 | 1995 -04-18 | 77984 .34 | 215553 .23 3 | | 1992 -01-01 | 1995 -04-18 | 178526 .74 | 555285 .16 4 | | 1993 -08-27 | 1996 -12-08 | 144451 .84 | 487405 .74 5 | | 1996 -06-22 | 1998 -08-02 | 77992 .67 | 530604 .44 6 | | 1995 -04-18 | 1998 -03-05 | 945 .99 | 144446 .76 ( 6 rows ) Your results should look like the above (we removed the \"SORT\" columns from the results to make it more readable) You can see that both columns have some form of order now. Our query is restricting rows in two ranges Condition 1: O_ORDERDATE = 1996 AND Condition 2: 150000 < O_TOTALPRICE <= 180000 Below we have summarized the Extents on the full extent level of 3MB from our above result. Min(Date) Max(Date) Min(Price) Max(Price) Cond 1 Cond 2 Both Cond 1992-01-01 1995-04-18 875.52 111093.84 1992-01-01 1995-04-18 77984.34 215553.23 X 1992-01-01 1995-04-18 178526.74 555285.16 X 1993-08-27 1996-12-08 144451.84 487405.74 X X X 1996-06-22 1998-08-02 77992.67 530604.44 X X X 1995-04-18 1998-03-05 945.99 144446.76 X As you can see there are now 3 extents that have rows from 1996 in them and 4 extents that contain rows in the price range from 150000 to 180000. But we have only two extents that contains rows that satisfy both conditions and needs to be scanned during query execution. The above shows the zone map ranges at the extent boundaries of 3MB. But Netezza Performance Server zone maps are even kept at the more granular level of pages. Netezza page size is 128K, and so there are 24 pages in one extent. You can look at the more granular zone map ranges for pages using the \"-page\" option of the nz_zonemap command: Input /nz/support/bin/nz_zonemap labdb orders_cbt o_orderdate o_totalprice -page Output Database: LABDB Object Name: ORDERS_CBT Object Type: TABLE Object ID : 201883 Data Slice: 1 Column 1 : O_ORDERDATE ( DATE ) Column 2 : O_TOTALPRICE ( NUMERIC ( 15 ,2 )) Extent # | Page # | O_ORDERDATE(min) | O_ORDERDATE(max) | O_TOTALPRICE(min) | O_TOTALPRICE(max) ----------+--------+------------------+------------------+--------------------+------------- ------+ 1 | extent | 1992 -01-01 | 1995 -04-18 | 875 .52 | 111091 .08 | 1 | 1 | 1992 -01-01 | 1992 -10-28 | 912 .10 | 26749 .39 | 1 | 2 | 1992 -03-17 | 1992 -10-28 | 15080 .65 | 45906 .12 | 1 | 3 | 1992 -01-01 | 1992 -05-31 | 26812 .15 | 65825 .06 | 1 | 4 | 1992 -01-01 | 1992 -10-27 | 61941 .14 | 77977 .57 | 1 | 5 | 1992 -05-31 | 1993 -03-28 | 45915 .67 | 71760 .90 | 1 | 6 | 1992 -10-28 | 1993 -04-25 | 49997 .93 | 77978 .41 | 1 | 7 | 1993 -03-28 | 1993 -08-27 | 45922 .72 | 77973 .85 | 1 | 8 | 1992 -10-29 | 1993 -08-27 | 26801 .08 | 54004 .34 | 1 | 9 | 1992 -10-28 | 1993 -06-12 | 961 .54 | 41500 .41 | 1 | 10 | 1993 -03-28 | 1994 -01-25 | 875 .52 | 26798 .03 | 1 | 11 | 1993 -08-27 | 1994 -04-09 | 15020 .44 | 45905 .64 | 1 | 12 | 1994 -01-25 | 1994 -07-29 | 947 .81 | 45779 .43 | 1 | 13 | 1994 -06-22 | 1995 -04-18 | 1004 .66 | 26775 .18 | 1 | 14 | 1994 -09-06 | 1995 -04-18 | 15087 .48 | 45906 .49 | 1 | 15 | 1994 -06-23 | 1994 -11-19 | 26821 .38 | 61934 .12 | 1 | 16 | 1994 -10-01 | 1995 -04-18 | 45916 .16 | 73765 .70 | 1 | 17 | 1994 -04-09 | 1995 -04-17 | 61957 .24 | 77974 .31 | 1 | 18 | 1994 -01-25 | 1994 -06-22 | 45917 .62 | 77944 .43 | 1 | 19 | 1993 -08-27 | 1994 -04-09 | 45926 .79 | 73846 .86 | 1 | 20 | 1993 -08-27 | 1994 -01-25 | 69917 .85 | 102685 .89 | 1 | 21 | 1993 -08-27 | 1994 -06-22 | 94343 .90 | 111089 .11 | 1 | 22 | 1994 -01-25 | 1994 -10-02 | 77986 .61 | 100585 .11 | 1 | 23 | 1994 -09-06 | 1995 -04-18 | 77983 .23 | 102725 .22 | 1 | 24 | 1994 -06-22 | 1995 -04-18 | 94331 .09 | 111091 .08 | 2 | extent | 1992 -01-01 | 1995 -04-18 | 77984 .34 | 215553 .23 | 2 | 1 | 1994 -06-22 | 1995 -04-18 | 106892 .35 | 127708 .22 | 2 | 2 | 1994 -09-05 | 1995 -04-17 | 119478 .05 | 144451 .22 | ... ( 161 rows ) There are 161 zone map ranges where reading the page can potentially be eliminated, not just 6 ranges on the extent boundaries. Of the 161 pages, the number that satisfy the conditions and need to be read are: 43 pages that might have O_ORDERDATE in 1996 54 pages that might have O_TOTALPRICE between 150000 and 180000 9 pages for which both conditions apply This means by using CBTs in Netezza Performance Server architecture we can restrict the amount of data that needs to be queried by a factor of 16. This is 3-4 times less than would need to be read if the table is only ordered on a single column. Congratulations, you have finished the Optimization Objects lab. In this lab you have created materialized views to speedup scans of wide tables and queries that only look up small numbers of rows. Finally, you created a Cluster Based Table and used the groom command to organize it. Throughout the lab you have used the nz_zonemap tool to see zone maps and get a better idea on how data is stored in the Netezza appliance.","title":"4.1 Cluster Based Table Usage"},{"location":"nz-09-Groom/","text":"1 Data Grooming As part of your routine database maintenance activities, you should plan to recover disk space occupied by outdated or deleted rows. In normal Netezza Performance Server operation, an UPDATE or DELETE of a table row does not remove the physical row on the hard disc. Instead the old row is marked as deleted together with a transaction id of the deleting transaction and in case of update a new row is created. This approach is called multiversioning. Rows that could potentially be visible to other transactions with an older transaction id are still accessible. Over time however, the outdated or deleted rows are of no interest to any transaction anymore and need to be removed to free up hard disc space and improve performance. After the rows have been captured in a backup, you can reclaim the space they occupy using the SQL GROOM TABLE command. The GROOM TABLE command does not lock a table while it is running; you can continue to SELECT, UPDATE, and INSERT into the table while the table is being groomed. 1.1 Objectives In this lab we will use the GROOM command to prepare our tables for the customer. During the course of the POC we have deleted and update a number of rows. At the end of a POC it is sensible to clean up the system. Use Groom on the created tables, Generate Statistics, and other cleanup tasks. 2 Lab Setup This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using putty If you are continuing from the previous lab and are already connected to NZSQL quit the NZSQL console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/groom/setupLab ./setupLab.sh Output DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully There may be error message at the beginning of the output since the script tries to clean up existing databases and users. 3 Transactions In this section we will show how transactions can leave logically deleted rows in a table which later as an administrative task need to be removed with the groom command. We will go through the different transaction types and show you what happens under the covers in an Netezza Performance Server Appliance. 3.1 Insert Transaction In this chapter we will add a new row to the regions table and review the hidden fields that are saved in the database. As you remember from the Transactions presentation, Netezza Performance Server uses a concept called multi-versioning for transactions. Each transaction has its own image of the table and doesn't influence other transactions. This is done by adding a number of hidden fields to the Netezza Performance Server table. The most important ones are the CREATEXID and the DELETEXID. Each Netezza Performance Server transaction has a unique transaction id that is increasing with each new transaction. In this subsection we will add a new row to the REGION table. Connect to your NPS system using a terminal application (i.e.: PuTTY or Terminal). Login to <ip-provided-by-your-instructor> as user nz with password nz. ( <ip-provided-by-your-instructor> is the default IP address for a lab system). Start nzsql from the Linux command line as following: Input nzsql Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM.ADMIN ( ADMIN )= > You will be entered into the nzsql interactive terminal. Connect to the database LABDB as user LABADMIN by typing the following command: Input \\c LABDB LABADMIN Output You are now connected to database LABDB as user LABADMIN. LABDB.ADMIN ( LABADMIN )= > Notice the prompt has changed to show the new connection information. Select all rows from the REGION table: Input SELECT * FROM REGION ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific ( 4 rows ) You should see the following output with 4 existing regions. Insert a new row into the REGIONS table for the region Australia with the following SQL command Input INSERT INTO REGION VALUES ( 5 , 'as' , 'australia' ) ; Output INSERTED 0 1 Now we will again do a select on the REGION table. But this time we will also query the hidden fields CREATEXID, DELETEXID and ROWID: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific 17514 | 0 | 37428000 | 5 | as | australia ( 5 rows ) As you can see, we now have five rows in the REGION table. The new row for Australia has the ID of the last transaction as CREATEXID and 0 as DELETEXID since it has not yet been deleted. Other transactions with a lower transaction ID that might still be running will not be able to see this new row. Note also that each row has a unique ROWID. ROWIDs do not need to be consecutive, but they are unique across all data slices for one table. 3.2 Update and Delete Transactions Delete transactions in Netezza Performance Server do not physically remove rows but update the DELETEXID field of a row to mark it as logically deleted. These logically deleted rows need to be removed regularly with the administrative GROOM command. Update transactions in Netezza Performance Server consist of a logical delete of the old row and an insert of a new row with the updated fields. To show this effectively we will need to change a system parameter in Netezza Performance Server that allows us to switch off the invisibility lists in Netezza Performance Server. Note that the parameter we will be using is dangerous and shouldn't be used in a real Netezza Performance Server environment. There is also a safer environment variable, but this has some restrictions. To see deleted rows without changing the system registry parameters do the following: nzsql labdb labadmin password set show_deleted_records = true; select * from table_with_deleted_rows; set show_deleted_records = false; The above method is not used in this lab, please follow the steps below. First, we will change the system variable that allows us to see deleted rows in the system, to do this exit the console with \\q . Check the Netezza Performance Server system registry for the parameters host.fpgaAllowXIDOverride and system.useFpgaPrep, each should be set to yes: Input nzsystem showregistry | grep -iE 'fpgaAllowXIDOverride|useFpgaPrep' Output host.fpgaAllowXIDOverride = no system.useFpgaPrep = yes To change these system parameters, first pause the system with the following command: Input nzsystem pause Are you sure you want to pause the system ( y | n ) ? [ n ] y Next, update the system parameters with the following command: Input nzsystem set -arg host.fpgaAllowXIDOverride = yes Are you sure you want to change the system configuration ( y | n ) ? [ n ] y Ensure both parameters host.fpgaAllowXIDOverride and system.useFpgaPrep are set to yes . Resume the system with the following command: Input nzsystem resume Re-check the Netezza Performance Server system registry for the parameters host.fpgaAllowXIDOverride and system.useFpgaPrep , each should be set to yes: Input nzsystem showregistry | grep -iE 'fpgaAllowXIDOverride|useFpgaPrep' Output host.fpgaAllowXIDOverride = yes system.useFpgaPrep = yes Start nzsql from the Linux command line as following: Input nzsql labdb labadmin password Now we will update the row we inserted in the last chapter to the REGION table: Input UPDATE REGION SET R_COMMENT = 'Australia' WHERE R_REGIONKEY = 5 ; Do a SELECT on the REGION table again: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific 17514 | 21506 | 37428000 | 5 | as | australia 21506 | 0 | 37428000 | 5 | as | Australia ( 6 rows ) Normally you would now see 5 rows with the update value. But since we disabled the invisibility lists you now see 6 rows in the REGION table. Our transaction that updated the row had the transaction id 369666. You can see that the original row with the lowercase australia in the comment column is still there and now has a DELETXID field that contains the transaction id of the transaction that deleted it. Transactions with a higher transaction id will not see a row with a DELETEXID that indicates that it has been logically deleted before the transaction is run. We also see a newly inserted row with the new comment value Australia. It has the same ROWID as the deleted row and the same CREATEXID as the transaction that did the insert. Finally let's clean up the table again by deleting the Australia row: Input DELETE FROM REGION WHERE R_REGIONKEY = 5 ; Output DELETE 1 Do a SELECT on the REGION table again: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific 17514 | 21506 | 37428000 | 5 | as | australia 21506 | 21510 | 37428000 | 5 | as | Australia ( 6 rows ) We can now see that we have logically deleted our updated row as well. It has now a DELETEXID field with the value of the new transaction. New transactions will see the original table from the start of this lab again. If you do a SELECT, the FPGA will filter out all rows that: have a CREATEXID which is bigger than the current transaction id. have a CREATEXID of an uncommitted transaction. have a DELETENXID which is smaller than the current transaction, but only if the transaction of the DELETEXID field is committed. have a DELETEXID of 1 which means that the insert has been aborted. 3.3 Aborting Transactions Netezza Performance Server never deletes a row during transactions even if transactions are rolled back. In this section we will show what happens if a transaction is rolled back. Since an update transaction consists of a DELETE and INSERT transaction, we will demonstrate the behavior for all tree transaction types with this. To start a transaction that we can later rollback we need to use the BEGIN keyword. Input BEGIN ; Output BEGIN Per default all SQL statements entered into the nzsql console are auto-committed. To start a multi command transaction the BEGIN keyword needs to be used. All SQL statements that are executed after it will belong to a single transaction. To end the transaction two keywords can be used COMMIT to commit the transaction or ROLLBACK to rollback the transaction and all changes since the BEGIN statement was executed. Update the row for the AP region: Input UPDATE REGION SET R_COMMENT = 'AP' WHERE R_REGIONKEY = 4 ; Output UPDATE 1 Do a SELECT on the REGION table again: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 21514 | 28765003 | 4 | ap | asia pacific 17514 | 21506 | 37428000 | 5 | as | australia 21506 | 21510 | 37428000 | 5 | as | Australia 21514 | 0 | 28765003 | 4 | ap | AP ( 7 rows ) Note: we have the same results as in the last chapter, the original row for the AP region was logically deleted by updating its DELETEXID field, and a new row with the updated comment and new ROWID has been added. Note that its CREATEXID is the same as the DELETEXID of the old row, since they were updated by the same transaction. Now let's rollback the transaction: Input ROLLBACK ; Output ROLLBACK Do a SELECT on the REGION table again: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific 17514 | 21506 | 37428000 | 5 | as | australia 21506 | 21510 | 37428000 | 5 | as | Australia 21514 | 1 | 28765003 | 4 | ap | AP ( 7 rows ) We can see that the transaction has been rolled back. The DELETEXID of the old version of the row has been reset to 0, which means that it is a valid row that can be seen by other transactions, and the DELETEXID of the new row has been set to 1 which marks it as aborted. 3.4 Cleaning up In this section we will use the GROOM command to remove the logically deleted rows we have entered, and we will remove the system parameter from the configuration file. The Groom command will be used in more detail in the next chapter. It is the main maintenance command in Netezza Performance Server, and we have already used it in the Cluster-based Table labs to reorder a CBT. It also removes all logically deleted rows from a table and frees up the space on the machine again. Execute the GROOM command on the REGION table: Input GROOM TABLE REGION ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2020 -04-02 19 :18:49. NOTICE: Groom processed 1 pages ; purged 3 records ; scan size unchanged ; table size unchanged. GROOM RECORDS ALL You can see that the GROOM command purged 3 rows, exactly the number of aborted and logically deleted rows we have generated in the previous chapter. Now select the rows from the REGION table again. Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific ( 4 rows ) You can see that the GROOM command has removed all logically deleted rows from the table. Remember that we still have the parameter switched on that allows us to see any logically deleted rows. Especially in tables that are heavily changed with lots and updates and deletes running the groom command will free up hard drive space and increase performance. Finally, we will change the system variables back to the original settings, to do this exit the console with \\q . To change these system parameters, first pause the system with the following command: Input nzsystem pause Are you sure you want to pause the system ( y | n ) ? [ n ] y Next, update the system parameters with the following command: Input nzsystem set -arg host.fpgaAllowXIDOverride = no Are you sure you want to change the system configuration ( y | n ) ? [ n ] y Ensure both parameters host.fpgaAllowXIDOverride=no and system.useFpgaPrep=yes. Resume the system with the following command: Input nzsystem resume Re-check the Netezza Performance Server system registry for the parameters host.fpgaAllowXIDOverride and system.useFpgaPrep, each should be set to yes: Input nzsystem showregistry | grep -iE 'fpgaAllowXIDOverride|useFpgaPrep' Output host.fpgaAllowXIDOverride = no system.useFpgaPrep = yes 4 Grooming Logically Deleted Rows In this section we will delete rows and determine that they have not really been deleted from the disk. Then using GROOM we will physically delete the rows. First determine the physical size on disk of the table ORDERS using the following command: You should see the following results: Input /nz/support/bin/nz_db_size LABDB Output Object | Name | Bytes | KB | MB | GB | TB -----------+----------------------------------+----------------------+------------------+--------------+------------+-------- Appliance | localhost | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 Database | LABDB | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 .schema | ADMIN | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 Table | CUSTOMER | 13 ,107,200 | 12 ,800 | 13 | .0 | .0 Table | LINEITEM | 284 ,688,384 | 278 ,016 | 272 | .3 | .0 Table | NATION | 131 ,072 | 128 | 0 | .0 | .0 Table | ORDERS | 76 ,283,904 | 74 ,496 | 73 | .1 | .0 Table | PART | 11 ,534,336 | 11 ,264 | 11 | .0 | .0 Table | PARTSUPP | 66 ,322,432 | 64 ,768 | 63 | .1 | .0 Table | REGION | 131 ,072 | 128 | 0 | .0 | .0 Table | SUPPLIER | 786 ,432 | 768 | 1 | .0 | .0 Notice that the ORDERS table is 75 MB in size. Now we are going to delete some rows from ORDERS table. Delete all rows where the ORDERSTATUS is marked as F for finished using the following command: Input nzsql LABDB LABADMIN password DELETE FROM ORDERS WHERE O_ORDERSTATUS = 'F' ; Output DELETE 729413 Now check the physical table size for ORDERS and see if the size decreased using the same command as before. You must first exit nzsql to shell using \\q. Input \\q /nz/support/bin/nz_db_size LABDB Output Object | Name | Bytes | KB | MB | GB | TB -----------+----------------------------------+----------------------+------------------+--------------+------------+-------- Appliance | localhost | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 Database | LABDB | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 .schema | ADMIN | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 Table | CUSTOMER | 13 ,107,200 | 12 ,800 | 13 | .0 | .0 Table | LINEITEM | 284 ,688,384 | 278 ,016 | 272 | .3 | .0 Table | NATION | 131 ,072 | 128 | 0 | .0 | .0 Table | ORDERS | 76 ,283,904 | 74 ,496 | 73 | .1 | .0 Table | PART | 11 ,534,336 | 11 ,264 | 11 | .0 | .0 Table | PARTSUPP | 66 ,322,432 | 64 ,768 | 63 | .1 | .0 Table | REGION | 131 ,072 | 128 | 0 | .0 | .0 Table | SUPPLIER | 786 ,432 | 768 | 1 | .0 | .0 The output should be the same as above showing that the ORDERS table did not change in size and is still 75 MB. This is because the deleted rows were logically deleted but are still left on disk. The rows will still accessible to transactions that started before the DELETE statement which we just executed. (i.e. have a lower transaction id) Next let's physically delete what we just logically deleted using the GROOM TABLE command and specifying table ORDERS. When you run the GROOM TABLE command, it removes outdated and deleted records from tables. Input nzsql LABDB LABADMIN password GROOM TABLE ORDERS ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2020 -04-02 19 :56:57. NOTICE: Groom processed 582 pages ; purged 729413 records ; scan size shrunk by 280 pages ; table size shrunk by 12 extents. GROOM RECORDS ALL You can see that 729413 rows were removed from disk resulting in the table size shrinking by 12 extents. Notice that this is the same number of rows we deleted in the previous step. Check if the ORDERS table size on disk has shrunk using the nz_db_size command. You must first exit nzsql to shell using \\q. Input \\q /nz/support/bin/nz_db_size LABDB Output Object | Name | Bytes | KB | MB | GB | TB -----------+----------------------------------+----------------------+------------------+--------------+------------+-------- Appliance | localhost | 416 ,284,672 | 406 ,528 | 397 | .4 | .0 Database | LABDB | 416 ,284,672 | 406 ,528 | 397 | .4 | .0 .schema | ADMIN | 416 ,284,672 | 406 ,528 | 397 | .4 | .0 Table | CUSTOMER | 13 ,107,200 | 12 ,800 | 13 | .0 | .0 Table | LINEITEM | 284 ,688,384 | 278 ,016 | 272 | .3 | .0 Table | NATION | 131 ,072 | 128 | 0 | .0 | .0 Table | ORDERS | 39 ,583,744 | 38 ,656 | 38 | .0 | .0 Table | PART | 11 ,534,336 | 11 ,264 | 11 | .0 | .0 Table | PARTSUPP | 66 ,322,432 | 64 ,768 | 63 | .1 | .0 Table | REGION | 131 ,072 | 128 | 0 | .0 | .0 Table | SUPPLIER | 786 ,432 | 768 | 1 | .0 | .0 Notice the reduced size of the ORDERS table. We can see that GROOM did purge the deleted rows from disk. GROOM reported that the table size was reduced by 12 extents and we can confirm this because we can see that the size of the table reduced by 36MB which is the correct size for 12 extents. (1 extent's size is 3 MB). 5 Performance Benefits of GROOM In this section we will show that grooming a table can also result in a performance benefit because the amount of data that needs to be scanned is smaller. Outdated rows are still present on the hard disc. They can be dismissed by the FPGA but the system still needs to read them from disc. In this example we need for accounting reasons increase the order price of all columns. This means that we need to update every row in the ORDERS table. We will measure query performance before and after Grooming the table. Update the ORDERS table so that the price of everything is increased by $1. Do this using the following command: Input nzsql LABDB LABADMIN password UPDATE ORDERS SET O_TOTALPRICE = O_TOTALPRICE+1 ; Output UPDATE 770587 All rows will be affected by the update resulting in a doubled number of physical rows in the table. This is because the UPDATE operation leaves a copy of the rows before the UPDATE occurred in case a transaction is still operating on the rows. New rows are created, and the results of the UPDATE are put in these rows. The old rows that are left on disk are marked as logically deleted. To measure the performance of our test query, we can configure the nzsql console to show the elapsed execution time using the following command: Input \\t ime Output Query time printout on Run our given test query and note the performance: Input SELECT COUNT ( * ) FROM ORDERS ; Output COUNT -------- 770587 ( 1 row ) Elapsed time: 0m0.641s Please rerun the query once or twice more to see roughly what a consistent query time is on your machine. Input SELECT COUNT ( * ) FROM ORDERS ; Output ????? Now run the GROOM TABLE command on the ORDER table again: Input GROOM TABLE ORDERS ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2020 -04-02 20 :12:07. NOTICE: Groom processed 604 pages ; purged 770587 records ; scan size shrunk by 302 pages ; table size shrunk by 13 extents. GROOM RECORDS ALL Elapsed time: 0m2.026s Can you tell how much disk space this saved? (It's the number of extents times 3MB) Now run our chosen test query again and you should see a difference in performance: Input SELECT COUNT ( * ) FROM ORDERS ; Output COUNT -------- 770587 ( 1 row ) Elapsed time: 0m0.082s You should see that the query ran faster than before. This is because GROOM reduced the number of rows that must be scanned to complete the query. The COUNT(*) command on the table will return the same number of rows before and after the GROOM command was run since it can only see the current version of the table, which means all rows that have not been deleted by a lower transaction id. Since our UPDATE command hasn't changed the number of logical rows this will not change. Nevertheless, the outdated rows, which have been logically deleted by our UPDATE command, are still present on disk. The COUNT(*) query cannot access these rows but they do take up space on disk and need to be scanned. GROOM is used to purge these logically deleted rows from disk which increase disk usage and scan distance. You should GROOM tables that receive frequent updates or deletes more often than tables that are seldom updated. You might want to schedule tasks that routinely GROOM the frequently updated tables or run a GROOM command as part of you ETL process. 6 Changing the Data Type of a Column In some situations, you will realize that the initially used data types are not suitable for long-term use, for example because new entries exceed the range of an initially picked integer value. You cannot directly change the data type by using the ALTER statement but there are two approaches that allow you to do it without loading and unloading the data. The first approach is to: Create a CTAS table from the old table with a CAST to the new datatype for the column you want to change Drop the old table Rename the new table to the name of the old table In general, this is a good approach because it lets you keep the order of the columns. But in this example, we will use a second approach to highlight the groom command and its role during ADD and DROP column commands. Its disadvantages are that the order of the columns will change, which may result in difficulties for third party applications that access columns by their order. In this chapter we will: Add a new column to the table with the new datatype Copy over all values from the old row to the new one with an UPDATE command Drop the old column Rename the new column to the name of the old one Use the groom command to materialize the results of our table changes For our example we find out that we have a new Region we want to add to our REGIONS table which has a name that exceeds the limits of the CHAR(25) field R_NAME. \"Australia, New Zealand, and Tasmania\". And we decide to increase the R_NAME field to a CHAR(40) field. Add a new column to the region table with name R_NAME_TEMP and data type CHAR(40) Input \\t ime Output Query time printout on Input ALTER TABLE REGION ADD COLUMN R_NAME_TEMP CHAR ( 40 ) ; Output ALTER TABLE Notice that the ALTER command is practically instantaneous. This even holds true for huge tables. Under the cover the system will create a new empty version of the table. It will not lock and change the whole table. Let's insert a row into the table using the new name column Input INSERT INTO REGION VALUES ( 5 , '' , 'South Pacific Region' , 'Australia, New Zealand, and Tasmania' ) ; Output INSERT 0 1 Now do a select on the table: Input SELECT * FROM REGION ; Output R_REGIONKEY | R_NAME | R_COMMENT | R_NAME_TEMP -------------+---------------------------+-----------------------------+------------------------------------------ 3 | emea | europe, middle east, africa | 1 | na | north america | 2 | sa | south america | 4 | ap | asia pacific | 5 | | South Pacific Region | Australia, New Zealand, and Tasmania ( 5 rows ) You can see that the results are exactly as you would expect them to be, but how does the system actually achieve this. Remember inside the Netezza Performance Server appliances we have two versions of the table, one containing the old columns and rows and one containing the new row column. Let's do an EXPLAIN on the SELECT query Input EXPLAIN VERBOSE SELECT * FROM REGION ; Output NOTICE: QUERY PLAN: QUERY SQL: EXPLAIN VERBOSE SELECT * FROM REGION ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"\" _TV_203063_2 \"\" {( \"_TV_203063_2\" .R_REGIONKEY )}] -- Estimated Rows = 1 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 100 .0 User table: REGION version 2 Projections: 1 : \"_TV_203063_2\" .R_REGIONKEY 2 : \"_TV_203063_2\" .R_NAME 3 : \"_TV_203063_2\" .R_COMMENT 4 : \"_TV_203063_2\" .R_NAME_TEMP Node 2 . [ SPU Sub-query Scan table \"*SELECT* 1\" Node \"1\" {( 0 . \"1\" )}] -- Estimated Rows = 1 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 0 .0 Projections: 1 :0. \"1\" 2 :0. \"2\" 3 :0. \"3\" 4 :0. \"4\" Node 3 . [ SPU Sequential Scan table \"\" _TV_203063_1 \"\" {( \"_TV_203063_1\" .R_REGIONKEY )}] -- Estimated Rows = 4 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 100 .0 User table: REGION version 1 Projections: 1 : \"_TV_203063_1\" .R_REGIONKEY 2 : \"_TV_203063_1\" .R_NAME 3 : \"_TV_203063_1\" .R_COMMENT 4 : ( NULL::BPCHAR ) ::CHAR ( 40 ) Node 4 . [ SPU Sub-query Scan table \"*SELECT* 2\" Node \"3\" {( 0 . \"1\" )}] -- Estimated Rows = 4 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 0 .0 Projections: 1 :0. \"1\" 2 :0. \"2\" 3 :0. \"3\" 4 :0. \"4\" Node 5 . [ SPU Append Nodes: , \"2\" , \"4 (stream)\" {( 0 . \"1\" )}] -- Estimated Rows = 5 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 0 .0 Projections: 1 :0. \"1\" 2 :0. \"2\" 3 :0. \"3\" 4 :0. \"4\" Node 6 . [ SPU Sub-query Scan table \"_BV_203063\" Node \"5\" {( \"_BV_203063\" .R_REGIONKEY )}] -- Estimated Rows = 5 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 100 .0 Projections: 1 : \"_BV_203063\" .R_REGIONKEY 2 : \"_BV_203063\" .R_NAME 3 : \"_BV_203063\" .R_COMMENT 4 : \"_BV_203063\" .R_NAME_TEMP [ SPU Return ] [ Host Return ] QUERY PLANTEXT: Sub-query Scan table \"_BV_203063\" ( cost = 0 .0..0.0 rows = 5 width = 221 conf = 100 ) {( \"_BV_203063\" .R_REGIONKEY )} ( xpath_none, locus = spu subject = self ) ( xpath_none, locus = spu subject = self ) ( xpath_none, locus = spu subject = self ) ( spu_send, locus = host subject = self ) ( host_return, locus = host subject = self ) l: Append ( cost = 0 .0..0.0 rows = 5 width = 221 conf = 0 ) {( 0 . \"1\" )} ( xpath_none, locus = spu subject = self ) ( xpath_none, locus = spu subject = self ) a: Sub-query Scan table \"*SELECT* 1\" ( cost = 0 .0..0.0 rows = 1 width = 221 conf = 0 ) {( 0 . \"1\" )} ( xpath_none, locus = spu subject = self ) l: Sequential Scan table \"\" _TV_203063_2 \"\" ( cost = 0 .0..0.0 rows = 1 width = 221 conf = 100 ) {( \"_TV_203063_2\" .R_REGIONKEY )} ( User table: REGION version 2 ) ( xpath_none, locus = spu subject = self ) a: Sub-query Scan table \"*SELECT* 2\" ( cost = 0 .0..0.0 rows = 4 width = 221 conf = 0 ) {( 0 . \"1\" )} ( xpath_none, locus = spu subject = self ) l: Sequential Scan table \"\" _TV_203063_1 \"\" ( cost = 0 .0..0.0 rows = 4 width = 221 conf = 100 ) {( \"_TV_203063_1\" .R_REGIONKEY )} ( User table: REGION version 1 ) ( xpath_none, locus = spu subject = self ) EXPLAIN Normally the query would result in a single table scan node. But now we see a more complicated query plan. The Optimizer automatically translates the simple SELECT into a UNION of two tables. The two tables are internal and are called _TV_203063_2 , which is the old version of the table before the ALTER statement. And _TV_203063_2 , which is the new version of the table after the table statement containing the new column R_NAME_TEMP. Notice that in the old table a 4^th^ column of CHAR(40) with default value NULL is added. This is necessary for the UNION to succeed. The merger of those tables is done in Node 5, which takes both result sets and appends them. But let's proceed with our data type change operation. Let's remove the new row again. Input DELETE FROM REGION WHERE R_REGIONKEY > 4 ; Output DELETE 1 Now we will move all values of the R_NAME column to the R_NAME_TEMP column by updating them Input UPDATE REGION SET R_NAME_TEMP = R_NAME ; Output UPDATE 4 Let's have a look at the table again: Input SELECT * FROM REGION ; Output R_REGIONKEY | R_NAME | R_COMMENT | R_NAME_TEMP -------------+---------------------------+-----------------------------+------------------------------------------ 3 | emea | europe, middle east, africa | emea 1 | na | north america | na 2 | sa | south america | sa 4 | ap | asia pacific | ap ( 4 rows ) Now let's remove the old column: Input ALTER TABLE REGION DROP COLUMN R_NAME RESTRICT ; Output ALTER TABLE Rename the column name R_NAME_TEMP to R_NAME Input ALTER TABLE REGION RENAME COLUMN R_NAME_TEMP TO R_NAME ; Output ALTER TABLE Let's have a look at the table again: Input SELECT * FROM REGION ; Output R_REGIONKEY | R_COMMENT | R_NAME -------------+-----------------------------+------------------------------------------ 3 | europe, middle east, africa | emea 1 | north america | na 2 | south america | sa 4 | asia pacific | ap ( 4 rows ) We have achieved to change the data type of the R_NAME column. The column order has changed but our R_NAME column has the same values as before and now supports longer region names. But we have one last step to do. Under the cover the system now has three different versions of the table which are merged for each call against the REGION table. This not only uses up space it is also bad for the query performance. So, we have to materialize these table changes with the GROOM command. GROOM the REGION table with the VERSIONS keyword to merge table versions: Input GROOM TABLE REGION VERSIONS ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2020 -04-03 04 :04:56. NOTICE: If this process is interrupted please either repeat GROOM VERSIONS or issue 'GENERATE STATISTICS ON \"REGION\"' NOTICE: Groom processed 2 pages ; purged 5 records ; scan size shrunk by 1 pages ; table size shrunk by 1 extents. GROOM VERSIONS Finally, we will look at the EXPLAIN output again: Input EXPLAIN VERBOSE SELECT * FROM REGION ; Output NOTICE: QUERY PLAN: QUERY SQL: EXPLAIN VERBOSE SELECT * FROM REGION ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"REGION\" {( REGION.R_REGIONKEY )}] -- Estimated Rows = 4 , Width = 196 , Cost = 0 .0 .. 0 .0, Conf = 100 .0 Projections: 1 :REGION.R_REGIONKEY 2 :REGION.R_COMMENT 3 :REGION.R_NAME [ SPU Return ] [ Host Return ] QUERY PLANTEXT: Sequential Scan table \"REGION\" ( cost = 0 .0..0.0 rows = 4 width = 196 conf = 100 ) {( REGION.R_REGIONKEY )} ( xpath_none, locus = spu subject = self ) ( spu_send, locus = host subject = self ) ( host_return, locus = host subject = self ) EXPLAIN Now this is much nicer. As we would expect we only have a single table scan snippet in the query plan and a single version of the REGION table. Finally, we will return the REGION table to the old column ordering to not interfere with future labs, to do this we will use a CTAS statement Input CREATE TABLE REGION_NEW AS SELECT R.R_REGIONKEY, R.R_NAME, R.R_COMMENT FROM REGION R ; Output INSERT 0 4 Now drop the REGION table: Input DROP TABLE REGION ; Output DROP TABLE Finally, rename the REGION_NEW table to make the transformation complete: Input ALTER TABLE REGION_NEW RENAME TO REGION ; Output ALTER TABLE If a table can be inaccessible for a short period of time using CTAS tables can be the better solution to change data types than using an ALTER TABLE statement. In this lab you have looked behind the scenes of the Netezza Performance Server appliances. You have seen how transactions are implemented and we have shown different reasons for using the groom command. It not only removes logically deleted rows from INSERT and UPDATE operations, aborted INSERTS and Loads, it also materializes table changes and reorders cluster-based tables.","title":"Grooming Data"},{"location":"nz-09-Groom/#1-data-grooming","text":"As part of your routine database maintenance activities, you should plan to recover disk space occupied by outdated or deleted rows. In normal Netezza Performance Server operation, an UPDATE or DELETE of a table row does not remove the physical row on the hard disc. Instead the old row is marked as deleted together with a transaction id of the deleting transaction and in case of update a new row is created. This approach is called multiversioning. Rows that could potentially be visible to other transactions with an older transaction id are still accessible. Over time however, the outdated or deleted rows are of no interest to any transaction anymore and need to be removed to free up hard disc space and improve performance. After the rows have been captured in a backup, you can reclaim the space they occupy using the SQL GROOM TABLE command. The GROOM TABLE command does not lock a table while it is running; you can continue to SELECT, UPDATE, and INSERT into the table while the table is being groomed.","title":"1 Data Grooming"},{"location":"nz-09-Groom/#11-objectives","text":"In this lab we will use the GROOM command to prepare our tables for the customer. During the course of the POC we have deleted and update a number of rows. At the end of a POC it is sensible to clean up the system. Use Groom on the created tables, Generate Statistics, and other cleanup tasks.","title":"1.1 Objectives"},{"location":"nz-09-Groom/#2-lab-setup","text":"This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using putty If you are continuing from the previous lab and are already connected to NZSQL quit the NZSQL console with the \\q command. Prepare for this lab by running the setup script. To do this use the following two commands: Input cd ~/labs/groom/setupLab ./setupLab.sh Output DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table 'NATION' completed successfully Load session of table 'REGION' completed successfully Load session of table 'CUSTOMER' completed successfully Load session of table 'SUPPLIER' completed successfully Load session of table 'PART' completed successfully Load session of table 'PARTSUPP' completed successfully Load session of table 'ORDERS' completed successfully Load session of table 'LINEITEM' completed successfully There may be error message at the beginning of the output since the script tries to clean up existing databases and users.","title":"2 Lab Setup"},{"location":"nz-09-Groom/#3-transactions","text":"In this section we will show how transactions can leave logically deleted rows in a table which later as an administrative task need to be removed with the groom command. We will go through the different transaction types and show you what happens under the covers in an Netezza Performance Server Appliance.","title":"3 Transactions"},{"location":"nz-09-Groom/#31-insert-transaction","text":"In this chapter we will add a new row to the regions table and review the hidden fields that are saved in the database. As you remember from the Transactions presentation, Netezza Performance Server uses a concept called multi-versioning for transactions. Each transaction has its own image of the table and doesn't influence other transactions. This is done by adding a number of hidden fields to the Netezza Performance Server table. The most important ones are the CREATEXID and the DELETEXID. Each Netezza Performance Server transaction has a unique transaction id that is increasing with each new transaction. In this subsection we will add a new row to the REGION table. Connect to your NPS system using a terminal application (i.e.: PuTTY or Terminal). Login to <ip-provided-by-your-instructor> as user nz with password nz. ( <ip-provided-by-your-instructor> is the default IP address for a lab system). Start nzsql from the Linux command line as following: Input nzsql Output Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit SYSTEM.ADMIN ( ADMIN )= > You will be entered into the nzsql interactive terminal. Connect to the database LABDB as user LABADMIN by typing the following command: Input \\c LABDB LABADMIN Output You are now connected to database LABDB as user LABADMIN. LABDB.ADMIN ( LABADMIN )= > Notice the prompt has changed to show the new connection information. Select all rows from the REGION table: Input SELECT * FROM REGION ; Output R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 3 | emea | europe, middle east, africa 1 | na | north america 2 | sa | south america 4 | ap | asia pacific ( 4 rows ) You should see the following output with 4 existing regions. Insert a new row into the REGIONS table for the region Australia with the following SQL command Input INSERT INTO REGION VALUES ( 5 , 'as' , 'australia' ) ; Output INSERTED 0 1 Now we will again do a select on the REGION table. But this time we will also query the hidden fields CREATEXID, DELETEXID and ROWID: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific 17514 | 0 | 37428000 | 5 | as | australia ( 5 rows ) As you can see, we now have five rows in the REGION table. The new row for Australia has the ID of the last transaction as CREATEXID and 0 as DELETEXID since it has not yet been deleted. Other transactions with a lower transaction ID that might still be running will not be able to see this new row. Note also that each row has a unique ROWID. ROWIDs do not need to be consecutive, but they are unique across all data slices for one table.","title":"3.1 Insert Transaction"},{"location":"nz-09-Groom/#32-update-and-delete-transactions","text":"Delete transactions in Netezza Performance Server do not physically remove rows but update the DELETEXID field of a row to mark it as logically deleted. These logically deleted rows need to be removed regularly with the administrative GROOM command. Update transactions in Netezza Performance Server consist of a logical delete of the old row and an insert of a new row with the updated fields. To show this effectively we will need to change a system parameter in Netezza Performance Server that allows us to switch off the invisibility lists in Netezza Performance Server. Note that the parameter we will be using is dangerous and shouldn't be used in a real Netezza Performance Server environment. There is also a safer environment variable, but this has some restrictions. To see deleted rows without changing the system registry parameters do the following: nzsql labdb labadmin password set show_deleted_records = true; select * from table_with_deleted_rows; set show_deleted_records = false; The above method is not used in this lab, please follow the steps below. First, we will change the system variable that allows us to see deleted rows in the system, to do this exit the console with \\q . Check the Netezza Performance Server system registry for the parameters host.fpgaAllowXIDOverride and system.useFpgaPrep, each should be set to yes: Input nzsystem showregistry | grep -iE 'fpgaAllowXIDOverride|useFpgaPrep' Output host.fpgaAllowXIDOverride = no system.useFpgaPrep = yes To change these system parameters, first pause the system with the following command: Input nzsystem pause Are you sure you want to pause the system ( y | n ) ? [ n ] y Next, update the system parameters with the following command: Input nzsystem set -arg host.fpgaAllowXIDOverride = yes Are you sure you want to change the system configuration ( y | n ) ? [ n ] y Ensure both parameters host.fpgaAllowXIDOverride and system.useFpgaPrep are set to yes . Resume the system with the following command: Input nzsystem resume Re-check the Netezza Performance Server system registry for the parameters host.fpgaAllowXIDOverride and system.useFpgaPrep , each should be set to yes: Input nzsystem showregistry | grep -iE 'fpgaAllowXIDOverride|useFpgaPrep' Output host.fpgaAllowXIDOverride = yes system.useFpgaPrep = yes Start nzsql from the Linux command line as following: Input nzsql labdb labadmin password Now we will update the row we inserted in the last chapter to the REGION table: Input UPDATE REGION SET R_COMMENT = 'Australia' WHERE R_REGIONKEY = 5 ; Do a SELECT on the REGION table again: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific 17514 | 21506 | 37428000 | 5 | as | australia 21506 | 0 | 37428000 | 5 | as | Australia ( 6 rows ) Normally you would now see 5 rows with the update value. But since we disabled the invisibility lists you now see 6 rows in the REGION table. Our transaction that updated the row had the transaction id 369666. You can see that the original row with the lowercase australia in the comment column is still there and now has a DELETXID field that contains the transaction id of the transaction that deleted it. Transactions with a higher transaction id will not see a row with a DELETEXID that indicates that it has been logically deleted before the transaction is run. We also see a newly inserted row with the new comment value Australia. It has the same ROWID as the deleted row and the same CREATEXID as the transaction that did the insert. Finally let's clean up the table again by deleting the Australia row: Input DELETE FROM REGION WHERE R_REGIONKEY = 5 ; Output DELETE 1 Do a SELECT on the REGION table again: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific 17514 | 21506 | 37428000 | 5 | as | australia 21506 | 21510 | 37428000 | 5 | as | Australia ( 6 rows ) We can now see that we have logically deleted our updated row as well. It has now a DELETEXID field with the value of the new transaction. New transactions will see the original table from the start of this lab again. If you do a SELECT, the FPGA will filter out all rows that: have a CREATEXID which is bigger than the current transaction id. have a CREATEXID of an uncommitted transaction. have a DELETENXID which is smaller than the current transaction, but only if the transaction of the DELETEXID field is committed. have a DELETEXID of 1 which means that the insert has been aborted.","title":"3.2 Update and Delete Transactions"},{"location":"nz-09-Groom/#33-aborting-transactions","text":"Netezza Performance Server never deletes a row during transactions even if transactions are rolled back. In this section we will show what happens if a transaction is rolled back. Since an update transaction consists of a DELETE and INSERT transaction, we will demonstrate the behavior for all tree transaction types with this. To start a transaction that we can later rollback we need to use the BEGIN keyword. Input BEGIN ; Output BEGIN Per default all SQL statements entered into the nzsql console are auto-committed. To start a multi command transaction the BEGIN keyword needs to be used. All SQL statements that are executed after it will belong to a single transaction. To end the transaction two keywords can be used COMMIT to commit the transaction or ROLLBACK to rollback the transaction and all changes since the BEGIN statement was executed. Update the row for the AP region: Input UPDATE REGION SET R_COMMENT = 'AP' WHERE R_REGIONKEY = 4 ; Output UPDATE 1 Do a SELECT on the REGION table again: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 21514 | 28765003 | 4 | ap | asia pacific 17514 | 21506 | 37428000 | 5 | as | australia 21506 | 21510 | 37428000 | 5 | as | Australia 21514 | 0 | 28765003 | 4 | ap | AP ( 7 rows ) Note: we have the same results as in the last chapter, the original row for the AP region was logically deleted by updating its DELETEXID field, and a new row with the updated comment and new ROWID has been added. Note that its CREATEXID is the same as the DELETEXID of the old row, since they were updated by the same transaction. Now let's rollback the transaction: Input ROLLBACK ; Output ROLLBACK Do a SELECT on the REGION table again: Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific 17514 | 21506 | 37428000 | 5 | as | australia 21506 | 21510 | 37428000 | 5 | as | Australia 21514 | 1 | 28765003 | 4 | ap | AP ( 7 rows ) We can see that the transaction has been rolled back. The DELETEXID of the old version of the row has been reset to 0, which means that it is a valid row that can be seen by other transactions, and the DELETEXID of the new row has been set to 1 which marks it as aborted.","title":"3.3 Aborting Transactions"},{"location":"nz-09-Groom/#34-cleaning-up","text":"In this section we will use the GROOM command to remove the logically deleted rows we have entered, and we will remove the system parameter from the configuration file. The Groom command will be used in more detail in the next chapter. It is the main maintenance command in Netezza Performance Server, and we have already used it in the Cluster-based Table labs to reorder a CBT. It also removes all logically deleted rows from a table and frees up the space on the machine again. Execute the GROOM command on the REGION table: Input GROOM TABLE REGION ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2020 -04-02 19 :18:49. NOTICE: Groom processed 1 pages ; purged 3 records ; scan size unchanged ; table size unchanged. GROOM RECORDS ALL You can see that the GROOM command purged 3 rows, exactly the number of aborted and logically deleted rows we have generated in the previous chapter. Now select the rows from the REGION table again. Input SELECT CREATEXID, DELETEXID, ROWID,* FROM REGION ; Output CREATEXID | DELETEXID | ROWID | R_REGIONKEY | R_NAME | R_COMMENT -----------+-----------+----------+-------------+---------------------------+----------------------------- 17498 | 0 | 28765000 | 3 | emea | europe, middle east, africa 17498 | 0 | 28765001 | 1 | na | north america 17498 | 0 | 28765002 | 2 | sa | south america 17498 | 0 | 28765003 | 4 | ap | asia pacific ( 4 rows ) You can see that the GROOM command has removed all logically deleted rows from the table. Remember that we still have the parameter switched on that allows us to see any logically deleted rows. Especially in tables that are heavily changed with lots and updates and deletes running the groom command will free up hard drive space and increase performance. Finally, we will change the system variables back to the original settings, to do this exit the console with \\q . To change these system parameters, first pause the system with the following command: Input nzsystem pause Are you sure you want to pause the system ( y | n ) ? [ n ] y Next, update the system parameters with the following command: Input nzsystem set -arg host.fpgaAllowXIDOverride = no Are you sure you want to change the system configuration ( y | n ) ? [ n ] y Ensure both parameters host.fpgaAllowXIDOverride=no and system.useFpgaPrep=yes. Resume the system with the following command: Input nzsystem resume Re-check the Netezza Performance Server system registry for the parameters host.fpgaAllowXIDOverride and system.useFpgaPrep, each should be set to yes: Input nzsystem showregistry | grep -iE 'fpgaAllowXIDOverride|useFpgaPrep' Output host.fpgaAllowXIDOverride = no system.useFpgaPrep = yes","title":"3.4 Cleaning up"},{"location":"nz-09-Groom/#4-grooming-logically-deleted-rows","text":"In this section we will delete rows and determine that they have not really been deleted from the disk. Then using GROOM we will physically delete the rows. First determine the physical size on disk of the table ORDERS using the following command: You should see the following results: Input /nz/support/bin/nz_db_size LABDB Output Object | Name | Bytes | KB | MB | GB | TB -----------+----------------------------------+----------------------+------------------+--------------+------------+-------- Appliance | localhost | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 Database | LABDB | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 .schema | ADMIN | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 Table | CUSTOMER | 13 ,107,200 | 12 ,800 | 13 | .0 | .0 Table | LINEITEM | 284 ,688,384 | 278 ,016 | 272 | .3 | .0 Table | NATION | 131 ,072 | 128 | 0 | .0 | .0 Table | ORDERS | 76 ,283,904 | 74 ,496 | 73 | .1 | .0 Table | PART | 11 ,534,336 | 11 ,264 | 11 | .0 | .0 Table | PARTSUPP | 66 ,322,432 | 64 ,768 | 63 | .1 | .0 Table | REGION | 131 ,072 | 128 | 0 | .0 | .0 Table | SUPPLIER | 786 ,432 | 768 | 1 | .0 | .0 Notice that the ORDERS table is 75 MB in size. Now we are going to delete some rows from ORDERS table. Delete all rows where the ORDERSTATUS is marked as F for finished using the following command: Input nzsql LABDB LABADMIN password DELETE FROM ORDERS WHERE O_ORDERSTATUS = 'F' ; Output DELETE 729413 Now check the physical table size for ORDERS and see if the size decreased using the same command as before. You must first exit nzsql to shell using \\q. Input \\q /nz/support/bin/nz_db_size LABDB Output Object | Name | Bytes | KB | MB | GB | TB -----------+----------------------------------+----------------------+------------------+--------------+------------+-------- Appliance | localhost | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 Database | LABDB | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 .schema | ADMIN | 452 ,984,832 | 442 ,368 | 432 | .4 | .0 Table | CUSTOMER | 13 ,107,200 | 12 ,800 | 13 | .0 | .0 Table | LINEITEM | 284 ,688,384 | 278 ,016 | 272 | .3 | .0 Table | NATION | 131 ,072 | 128 | 0 | .0 | .0 Table | ORDERS | 76 ,283,904 | 74 ,496 | 73 | .1 | .0 Table | PART | 11 ,534,336 | 11 ,264 | 11 | .0 | .0 Table | PARTSUPP | 66 ,322,432 | 64 ,768 | 63 | .1 | .0 Table | REGION | 131 ,072 | 128 | 0 | .0 | .0 Table | SUPPLIER | 786 ,432 | 768 | 1 | .0 | .0 The output should be the same as above showing that the ORDERS table did not change in size and is still 75 MB. This is because the deleted rows were logically deleted but are still left on disk. The rows will still accessible to transactions that started before the DELETE statement which we just executed. (i.e. have a lower transaction id) Next let's physically delete what we just logically deleted using the GROOM TABLE command and specifying table ORDERS. When you run the GROOM TABLE command, it removes outdated and deleted records from tables. Input nzsql LABDB LABADMIN password GROOM TABLE ORDERS ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2020 -04-02 19 :56:57. NOTICE: Groom processed 582 pages ; purged 729413 records ; scan size shrunk by 280 pages ; table size shrunk by 12 extents. GROOM RECORDS ALL You can see that 729413 rows were removed from disk resulting in the table size shrinking by 12 extents. Notice that this is the same number of rows we deleted in the previous step. Check if the ORDERS table size on disk has shrunk using the nz_db_size command. You must first exit nzsql to shell using \\q. Input \\q /nz/support/bin/nz_db_size LABDB Output Object | Name | Bytes | KB | MB | GB | TB -----------+----------------------------------+----------------------+------------------+--------------+------------+-------- Appliance | localhost | 416 ,284,672 | 406 ,528 | 397 | .4 | .0 Database | LABDB | 416 ,284,672 | 406 ,528 | 397 | .4 | .0 .schema | ADMIN | 416 ,284,672 | 406 ,528 | 397 | .4 | .0 Table | CUSTOMER | 13 ,107,200 | 12 ,800 | 13 | .0 | .0 Table | LINEITEM | 284 ,688,384 | 278 ,016 | 272 | .3 | .0 Table | NATION | 131 ,072 | 128 | 0 | .0 | .0 Table | ORDERS | 39 ,583,744 | 38 ,656 | 38 | .0 | .0 Table | PART | 11 ,534,336 | 11 ,264 | 11 | .0 | .0 Table | PARTSUPP | 66 ,322,432 | 64 ,768 | 63 | .1 | .0 Table | REGION | 131 ,072 | 128 | 0 | .0 | .0 Table | SUPPLIER | 786 ,432 | 768 | 1 | .0 | .0 Notice the reduced size of the ORDERS table. We can see that GROOM did purge the deleted rows from disk. GROOM reported that the table size was reduced by 12 extents and we can confirm this because we can see that the size of the table reduced by 36MB which is the correct size for 12 extents. (1 extent's size is 3 MB).","title":"4 Grooming Logically Deleted Rows"},{"location":"nz-09-Groom/#5-performance-benefits-of-groom","text":"In this section we will show that grooming a table can also result in a performance benefit because the amount of data that needs to be scanned is smaller. Outdated rows are still present on the hard disc. They can be dismissed by the FPGA but the system still needs to read them from disc. In this example we need for accounting reasons increase the order price of all columns. This means that we need to update every row in the ORDERS table. We will measure query performance before and after Grooming the table. Update the ORDERS table so that the price of everything is increased by $1. Do this using the following command: Input nzsql LABDB LABADMIN password UPDATE ORDERS SET O_TOTALPRICE = O_TOTALPRICE+1 ; Output UPDATE 770587 All rows will be affected by the update resulting in a doubled number of physical rows in the table. This is because the UPDATE operation leaves a copy of the rows before the UPDATE occurred in case a transaction is still operating on the rows. New rows are created, and the results of the UPDATE are put in these rows. The old rows that are left on disk are marked as logically deleted. To measure the performance of our test query, we can configure the nzsql console to show the elapsed execution time using the following command: Input \\t ime Output Query time printout on Run our given test query and note the performance: Input SELECT COUNT ( * ) FROM ORDERS ; Output COUNT -------- 770587 ( 1 row ) Elapsed time: 0m0.641s Please rerun the query once or twice more to see roughly what a consistent query time is on your machine. Input SELECT COUNT ( * ) FROM ORDERS ; Output ????? Now run the GROOM TABLE command on the ORDER table again: Input GROOM TABLE ORDERS ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2020 -04-02 20 :12:07. NOTICE: Groom processed 604 pages ; purged 770587 records ; scan size shrunk by 302 pages ; table size shrunk by 13 extents. GROOM RECORDS ALL Elapsed time: 0m2.026s Can you tell how much disk space this saved? (It's the number of extents times 3MB) Now run our chosen test query again and you should see a difference in performance: Input SELECT COUNT ( * ) FROM ORDERS ; Output COUNT -------- 770587 ( 1 row ) Elapsed time: 0m0.082s You should see that the query ran faster than before. This is because GROOM reduced the number of rows that must be scanned to complete the query. The COUNT(*) command on the table will return the same number of rows before and after the GROOM command was run since it can only see the current version of the table, which means all rows that have not been deleted by a lower transaction id. Since our UPDATE command hasn't changed the number of logical rows this will not change. Nevertheless, the outdated rows, which have been logically deleted by our UPDATE command, are still present on disk. The COUNT(*) query cannot access these rows but they do take up space on disk and need to be scanned. GROOM is used to purge these logically deleted rows from disk which increase disk usage and scan distance. You should GROOM tables that receive frequent updates or deletes more often than tables that are seldom updated. You might want to schedule tasks that routinely GROOM the frequently updated tables or run a GROOM command as part of you ETL process.","title":"5 Performance Benefits of GROOM"},{"location":"nz-09-Groom/#6-changing-the-data-type-of-a-column","text":"In some situations, you will realize that the initially used data types are not suitable for long-term use, for example because new entries exceed the range of an initially picked integer value. You cannot directly change the data type by using the ALTER statement but there are two approaches that allow you to do it without loading and unloading the data. The first approach is to: Create a CTAS table from the old table with a CAST to the new datatype for the column you want to change Drop the old table Rename the new table to the name of the old table In general, this is a good approach because it lets you keep the order of the columns. But in this example, we will use a second approach to highlight the groom command and its role during ADD and DROP column commands. Its disadvantages are that the order of the columns will change, which may result in difficulties for third party applications that access columns by their order. In this chapter we will: Add a new column to the table with the new datatype Copy over all values from the old row to the new one with an UPDATE command Drop the old column Rename the new column to the name of the old one Use the groom command to materialize the results of our table changes For our example we find out that we have a new Region we want to add to our REGIONS table which has a name that exceeds the limits of the CHAR(25) field R_NAME. \"Australia, New Zealand, and Tasmania\". And we decide to increase the R_NAME field to a CHAR(40) field. Add a new column to the region table with name R_NAME_TEMP and data type CHAR(40) Input \\t ime Output Query time printout on Input ALTER TABLE REGION ADD COLUMN R_NAME_TEMP CHAR ( 40 ) ; Output ALTER TABLE Notice that the ALTER command is practically instantaneous. This even holds true for huge tables. Under the cover the system will create a new empty version of the table. It will not lock and change the whole table. Let's insert a row into the table using the new name column Input INSERT INTO REGION VALUES ( 5 , '' , 'South Pacific Region' , 'Australia, New Zealand, and Tasmania' ) ; Output INSERT 0 1 Now do a select on the table: Input SELECT * FROM REGION ; Output R_REGIONKEY | R_NAME | R_COMMENT | R_NAME_TEMP -------------+---------------------------+-----------------------------+------------------------------------------ 3 | emea | europe, middle east, africa | 1 | na | north america | 2 | sa | south america | 4 | ap | asia pacific | 5 | | South Pacific Region | Australia, New Zealand, and Tasmania ( 5 rows ) You can see that the results are exactly as you would expect them to be, but how does the system actually achieve this. Remember inside the Netezza Performance Server appliances we have two versions of the table, one containing the old columns and rows and one containing the new row column. Let's do an EXPLAIN on the SELECT query Input EXPLAIN VERBOSE SELECT * FROM REGION ; Output NOTICE: QUERY PLAN: QUERY SQL: EXPLAIN VERBOSE SELECT * FROM REGION ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"\" _TV_203063_2 \"\" {( \"_TV_203063_2\" .R_REGIONKEY )}] -- Estimated Rows = 1 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 100 .0 User table: REGION version 2 Projections: 1 : \"_TV_203063_2\" .R_REGIONKEY 2 : \"_TV_203063_2\" .R_NAME 3 : \"_TV_203063_2\" .R_COMMENT 4 : \"_TV_203063_2\" .R_NAME_TEMP Node 2 . [ SPU Sub-query Scan table \"*SELECT* 1\" Node \"1\" {( 0 . \"1\" )}] -- Estimated Rows = 1 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 0 .0 Projections: 1 :0. \"1\" 2 :0. \"2\" 3 :0. \"3\" 4 :0. \"4\" Node 3 . [ SPU Sequential Scan table \"\" _TV_203063_1 \"\" {( \"_TV_203063_1\" .R_REGIONKEY )}] -- Estimated Rows = 4 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 100 .0 User table: REGION version 1 Projections: 1 : \"_TV_203063_1\" .R_REGIONKEY 2 : \"_TV_203063_1\" .R_NAME 3 : \"_TV_203063_1\" .R_COMMENT 4 : ( NULL::BPCHAR ) ::CHAR ( 40 ) Node 4 . [ SPU Sub-query Scan table \"*SELECT* 2\" Node \"3\" {( 0 . \"1\" )}] -- Estimated Rows = 4 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 0 .0 Projections: 1 :0. \"1\" 2 :0. \"2\" 3 :0. \"3\" 4 :0. \"4\" Node 5 . [ SPU Append Nodes: , \"2\" , \"4 (stream)\" {( 0 . \"1\" )}] -- Estimated Rows = 5 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 0 .0 Projections: 1 :0. \"1\" 2 :0. \"2\" 3 :0. \"3\" 4 :0. \"4\" Node 6 . [ SPU Sub-query Scan table \"_BV_203063\" Node \"5\" {( \"_BV_203063\" .R_REGIONKEY )}] -- Estimated Rows = 5 , Width = 221 , Cost = 0 .0 .. 0 .0, Conf = 100 .0 Projections: 1 : \"_BV_203063\" .R_REGIONKEY 2 : \"_BV_203063\" .R_NAME 3 : \"_BV_203063\" .R_COMMENT 4 : \"_BV_203063\" .R_NAME_TEMP [ SPU Return ] [ Host Return ] QUERY PLANTEXT: Sub-query Scan table \"_BV_203063\" ( cost = 0 .0..0.0 rows = 5 width = 221 conf = 100 ) {( \"_BV_203063\" .R_REGIONKEY )} ( xpath_none, locus = spu subject = self ) ( xpath_none, locus = spu subject = self ) ( xpath_none, locus = spu subject = self ) ( spu_send, locus = host subject = self ) ( host_return, locus = host subject = self ) l: Append ( cost = 0 .0..0.0 rows = 5 width = 221 conf = 0 ) {( 0 . \"1\" )} ( xpath_none, locus = spu subject = self ) ( xpath_none, locus = spu subject = self ) a: Sub-query Scan table \"*SELECT* 1\" ( cost = 0 .0..0.0 rows = 1 width = 221 conf = 0 ) {( 0 . \"1\" )} ( xpath_none, locus = spu subject = self ) l: Sequential Scan table \"\" _TV_203063_2 \"\" ( cost = 0 .0..0.0 rows = 1 width = 221 conf = 100 ) {( \"_TV_203063_2\" .R_REGIONKEY )} ( User table: REGION version 2 ) ( xpath_none, locus = spu subject = self ) a: Sub-query Scan table \"*SELECT* 2\" ( cost = 0 .0..0.0 rows = 4 width = 221 conf = 0 ) {( 0 . \"1\" )} ( xpath_none, locus = spu subject = self ) l: Sequential Scan table \"\" _TV_203063_1 \"\" ( cost = 0 .0..0.0 rows = 4 width = 221 conf = 100 ) {( \"_TV_203063_1\" .R_REGIONKEY )} ( User table: REGION version 1 ) ( xpath_none, locus = spu subject = self ) EXPLAIN Normally the query would result in a single table scan node. But now we see a more complicated query plan. The Optimizer automatically translates the simple SELECT into a UNION of two tables. The two tables are internal and are called _TV_203063_2 , which is the old version of the table before the ALTER statement. And _TV_203063_2 , which is the new version of the table after the table statement containing the new column R_NAME_TEMP. Notice that in the old table a 4^th^ column of CHAR(40) with default value NULL is added. This is necessary for the UNION to succeed. The merger of those tables is done in Node 5, which takes both result sets and appends them. But let's proceed with our data type change operation. Let's remove the new row again. Input DELETE FROM REGION WHERE R_REGIONKEY > 4 ; Output DELETE 1 Now we will move all values of the R_NAME column to the R_NAME_TEMP column by updating them Input UPDATE REGION SET R_NAME_TEMP = R_NAME ; Output UPDATE 4 Let's have a look at the table again: Input SELECT * FROM REGION ; Output R_REGIONKEY | R_NAME | R_COMMENT | R_NAME_TEMP -------------+---------------------------+-----------------------------+------------------------------------------ 3 | emea | europe, middle east, africa | emea 1 | na | north america | na 2 | sa | south america | sa 4 | ap | asia pacific | ap ( 4 rows ) Now let's remove the old column: Input ALTER TABLE REGION DROP COLUMN R_NAME RESTRICT ; Output ALTER TABLE Rename the column name R_NAME_TEMP to R_NAME Input ALTER TABLE REGION RENAME COLUMN R_NAME_TEMP TO R_NAME ; Output ALTER TABLE Let's have a look at the table again: Input SELECT * FROM REGION ; Output R_REGIONKEY | R_COMMENT | R_NAME -------------+-----------------------------+------------------------------------------ 3 | europe, middle east, africa | emea 1 | north america | na 2 | south america | sa 4 | asia pacific | ap ( 4 rows ) We have achieved to change the data type of the R_NAME column. The column order has changed but our R_NAME column has the same values as before and now supports longer region names. But we have one last step to do. Under the cover the system now has three different versions of the table which are merged for each call against the REGION table. This not only uses up space it is also bad for the query performance. So, we have to materialize these table changes with the GROOM command. GROOM the REGION table with the VERSIONS keyword to merge table versions: Input GROOM TABLE REGION VERSIONS ; Output NOTICE: Groom will not purge records deleted by transactions that started after 2020 -04-03 04 :04:56. NOTICE: If this process is interrupted please either repeat GROOM VERSIONS or issue 'GENERATE STATISTICS ON \"REGION\"' NOTICE: Groom processed 2 pages ; purged 5 records ; scan size shrunk by 1 pages ; table size shrunk by 1 extents. GROOM VERSIONS Finally, we will look at the EXPLAIN output again: Input EXPLAIN VERBOSE SELECT * FROM REGION ; Output NOTICE: QUERY PLAN: QUERY SQL: EXPLAIN VERBOSE SELECT * FROM REGION ; QUERY VERBOSE PLAN: Node 1 . [ SPU Sequential Scan table \"REGION\" {( REGION.R_REGIONKEY )}] -- Estimated Rows = 4 , Width = 196 , Cost = 0 .0 .. 0 .0, Conf = 100 .0 Projections: 1 :REGION.R_REGIONKEY 2 :REGION.R_COMMENT 3 :REGION.R_NAME [ SPU Return ] [ Host Return ] QUERY PLANTEXT: Sequential Scan table \"REGION\" ( cost = 0 .0..0.0 rows = 4 width = 196 conf = 100 ) {( REGION.R_REGIONKEY )} ( xpath_none, locus = spu subject = self ) ( spu_send, locus = host subject = self ) ( host_return, locus = host subject = self ) EXPLAIN Now this is much nicer. As we would expect we only have a single table scan snippet in the query plan and a single version of the REGION table. Finally, we will return the REGION table to the old column ordering to not interfere with future labs, to do this we will use a CTAS statement Input CREATE TABLE REGION_NEW AS SELECT R.R_REGIONKEY, R.R_NAME, R.R_COMMENT FROM REGION R ; Output INSERT 0 4 Now drop the REGION table: Input DROP TABLE REGION ; Output DROP TABLE Finally, rename the REGION_NEW table to make the transformation complete: Input ALTER TABLE REGION_NEW RENAME TO REGION ; Output ALTER TABLE If a table can be inaccessible for a short period of time using CTAS tables can be the better solution to change data types than using an ALTER TABLE statement. In this lab you have looked behind the scenes of the Netezza Performance Server appliances. You have seen how transactions are implemented and we have shown different reasons for using the groom command. It not only removes logically deleted rows from INSERT and UPDATE operations, aborted INSERTS and Loads, it also materializes table changes and reorders cluster-based tables.","title":"6 Changing the Data Type of a Column"},{"location":"nz-10-Stored-Proc/","text":"Stored Procedures Stored Procedures are subroutines that are saved in IBM Performance Server. They are executed inside the database server and are only available by accessing the NPS system. They combine the capabilities of SQL to query and manipulate database information with capabilities of procedural programming languages, like branching and iterations. This makes them an ideal solution for tasks like data validation, writing event logs or encrypting data. They are especially suited for repetitive tasks that can be easily encapsulated in a sub-routine. Objectives In the last labs we have created our database, loaded the data and we have done some optimization and administration tasks. In this lab we will enhance the database by a couple of stored procedures. As we mentioned in a previous chapter Netezza Performance Serverdoesn't check referential or unique constraints. This is normally not critical since data loading in a data warehousing environment is a controlled task. In our Netezza Performance Serverimplementation we get the requirement to allow some non-administrative database users to add new customers to the customer table. This happens rarely so there are no performance requirements and we have decided to implement this with a stored procedure that is accessible for these users and checks the input values and referential constraints. In a second part we will implement a business logic function as a stored procedure returning a result set. TODO describe function. {width=\"4.248510498687664in\" height=\"3.9270833333333335in\"} Figure 1 LABDB database Lab Setup This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using putty If you are continuing from the previous lab and are already connected to NZSQL quit the NZSQL console with the [\\q]{.mark} command. Prepare for this lab by running the setup script. To do this use the following two commands: Input: [nz@localhost labs]\\$ [cd \\~/labs/storedProcedure/setupLab]{.mark} [nz@localhost setupLab]\\$ [./setupLab.sh]{.mark} Output: DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table \\'NATION\\' completed successfully Load session of table \\'REGION\\' completed successfully Load session of table \\'CUSTOMER\\' completed successfully Load session of table \\'SUPPLIER\\' completed successfully Load session of table \\'PART\\' completed successfully Load session of table \\'PARTSUPP\\' completed successfully Load session of table \\'ORDERS\\' completed successfully Load session of table \\'LINEITEM\\' completed successfully There may be error message at the beginning of the output since the script tries to clean up existing databases and users. Implementing the addCustomer stored procedure In this chapter we will create a stored procedure to insert data into the CUSTOMER table. The information that is added for a new customer will be the customer key (C_CUSTKEY), name (C_NAME), phone number (C_PHONE) and nation key (C_NATIONKEY), the rest of the information is updated through other processes. Create Insert Stored Procedure First, review the customer table and define the interface of the insert stored procedure. Connect to your Netezza image using a terminal application (i.e.: PuTTY or Terminal.app). Login to \\<ip-provided-by-your-instructor> as user nz with password nz. Access the lab directory for this lab with the following command, this folder already contains empty files for the stored procedure scripts we will later create. If you want review them with the ls command: [nz@localhost \\~]\\$ [cd labs/storedProcedure/]{.mark} Enter nzsql and connect to LABDB as user LABADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql LABDB LABADMIN]{.mark} Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit Describe the customer table with the following command \\d customer You should see the following: Input: [nz@localhost storedProcedure]\\$ \\d customer Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit We will now create a stored procedure that adds a new customer entry and sets the 4 fields: C_CUSTKEY, C_NAME, C_NATIONKEY, and C_PHONE, all other fields will be set with an empty value or 0, since the fields are flagged as NOT NULL. Exit the nzsql console by executing the \\q command. Input: LABDB.ADMIN(LABADMIN)=> \\q Output: [nz@localhost storedProcedure]\\$ To create a stored procedure we will use the internal vi editor. Open the already existing empty file addCustomer.sql with the following command: [nz@netezza storedProcedure]\\$ vi addCustomer.sql You are now in the familiar vi interface and you can edit the file. Switch to INSERT mode by pressing i. We will now create the interface of the stored procedure so we can test creating it. We need the 4 input field mentioned above and will return an integer return code. Enter the text as seen in the following, then exit the insert mode by pressing ESC and enter wq! and enter to save the file and quit vi. [CREATE OR REPLACE PROCEDURE addCustomer(integer, varchar(25), integer, varchar(15))]{.mark} [LANGUAGE NZPLSQL RETURNS INT4 AS]{.mark} [BEGIN_PROC]{.mark} [END_PROC;]{.mark} The minimal stored procedure we create here doesn't yet do anything, since it has an empty body. We simply create the signature with the input and output variables. We use the command CREATE OR REPLACE so we can later execute the same command multiple times to update the stored procedure with more code. The input variables cannot be given names so we only add the datatypes for our input parameters key, name, nation and phone. We also return an integer return code. Note that we have to specify the procedure language even though nzplsql is the only available option in Netezza Performance Server system. Enter nzsql and connect to LABDB as user LABADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql LABDB LABADMIN]{.mark} Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit Back in the nzsql command line execute the script we just created with \\i addCustomer.sql You should see, that the procedure has been created successfully Input: LABDB.ADMIN(LABADMIN)=> [\\i addCustomer.sql]{.mark} Output: CREATE PROCEDURE Display all stored procedures in the LABDB database with the following command: Input: LABDB.ADMIN(LABADMIN)=> [SHOW PROCEDURE;]{.mark} Output: SCHEMA | RESULT | PROCEDURE | BUILTIN | ARGUMENTS --------+---------+-------------+---------+------------------------------------------------------------------ ADMIN | INTEGER | ADDCUSTOMER | f | (INTEGER, CHARACTER VARYING(25), INTEGER, CHARACTER VARYING(15)) (1 row) You can see the procedure ADDCUSTOMER with the arguments we specified. Execute the stored procedure with the following dummy input parameters: Input: LABDB.ADMIN(LABADMIN)=> [call addcustomer(1,\\'test\\', 2, \\'test\\');]{.mark} Output: NOTICE: plpgsql: ERROR during compile of ADDCUSTOMER near line 1 ERROR: syntax error, unexpected \\<EOF>, expecting BEGIN at or near \\\"\\\" The result shows that we have a syntax error in our stored procedure. Every stored procedure needs at least one BEGIN .. END block that encapsulates the code that is to be executed. Stored procedures are compiled when they are first executed not when they are created, therefore errors in the code can only be seen during execution. Exit the nzsql console by executing the \\q command. Input: LABDB.ADMIN(LABADMIN)=> [\\q]{.mark} Output: [nz@localhost storedProcedure]\\$ Edit the addCustomer.sql file with vi with the following command nz@netezza storedProcedure]\\$ [vi addCustomer.sql]{.mark} We will now create a simple stored procedure that inserts the new entry into the customer table. But first we will add some variables that alias the input variables \\$1, \\$2 etc. After the BEGIN_PROC statement enter the following lines (open a line by pressing o while the cursor is positioned on the line BEGIN_PROC, this will enter you into the INSERT mode of vi.): [DECLARE]{.mark} [C_KEY ALIAS FOR \\$1;]{.mark} [C_NAME ALIAS FOR \\$2;]{.mark} [N_KEY ALIAS FOR \\$3;]{.mark} [PHONE ALIAS FOR \\$4;]{.mark} Each BEGIN..END block in the stored procedure can have its own DECLARE section. Variables are valid in the block they belong to. It is a good best practice to change the input parameters into readable variable names to make the stored procedure code maintainable. We will later add some additional parameters to our procedures as well. Be careful not to use variable names that are restricted by Netezza Performance Server system, for example NAME. Next we will add the BEGIN..END block with the INSERT statement. [BEGIN]{.mark} [INSERT INTO CUSTOMER VALUES (C_KEY, C_NAME, \\'\\', N_KEY, PHONE, 0, \\'\\', \\'\\');]{.mark} [END;]{.mark} This statement will add a new row to the customer table using the input variables. It will replace the remaining fields like account balance with default values that can be later filled. It is also possible to execute dynamic SQL queries which we will do in a later chapter. Your complete stored procedure should now look like the following: [CREATE OR REPLACE PROCEDURE addCustomer(integer, varchar(25), integer, varchar(15))]{.mark} [LANGUAGE NZPLSQL RETURNS INT4 AS]{.mark} [BEGIN_PROC]{.mark} [DECLARE]{.mark} [C_KEY ALIAS FOR \\$1;]{.mark} [C_NAME ALIAS FOR \\$2;]{.mark} [N_KEY ALIAS FOR \\$3;]{.mark} [PHONE ALIAS FOR \\$4;]{.mark} [BEGIN]{.mark} [INSERT INTO CUSTOMER VALUES (C_KEY, C_NAME, \\'\\', N_KEY, PHONE, 0 \\'\\', \\'\\');]{.mark} [END;]{.mark} [END_PROC;]{.mark} Save and exit vi again by pressing ESC to enter the command mode and entering wq! and pressing enter. This will bring you back to the Linux command line. 17. Enter NZSQL and connect to LABDB as user LABADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql LABDB LABADMIN]{.mark} Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit Execute the stored procedure script with the following command: \\i addCustomer.sql Input: LABDB.ADMIN(LABADMIN)=> [\\i addCustomer.sql]{.mark} Output: CREATE PROCEDURE Now try the stored procedure lets add a new customer John Smith with customer key 999999, phone number 555-5555 and nation 2 (which is the key for the United States in our NATION table). You can also check first that the customer doesn't yet exist if you want. Input: LABDB.ADMIN(LABADMIN)=> [CALL addCustomer(999999,\\'John Smith\\', 2, \\'555-5555\\');]{.mark} Output: ADDCUSTOMER ------------- (1 row) Check if the insert was successful: Input: LABDB.ADMIN(LABADMIN)=> [SELECT * FROM CUSTOMER WHERE C_CUSTKEY = 999999;]{.mark} Output: C_CUSTKEY | C_NAME | C_ADDRESS | C_NATIONKEY | C_PHONE | C_ACCTBAL | C_MKTSEGMENT | C_COMMENT -----------+------------+-----------+-------------+-----------------+-----------+--------------+----------- 999999 | John Smith | | 2 | 555-5555 | 0.00 | | (1 row) Congratulations, you have built your first Netezza Performance Serverstored procedure. Adding integrity checks In this chapter we will add integrity checks to the stored procedure we just created. We will make sure that no duplicate customer is entered into the CUSTOMER table by querying it before the insert. We will then check with an IF condition if the value had already been inserted into the CUSTOMER table and abort the insert in that case. We will also check the foreign key relationship to the nation table and make sure that no customer is inserted for a nation that doesn't exist. If any of these conditions aren't met the procedure will abort and display an error message. Exit the nzsql console by executing the \\q command. Input: LABDB.ADMIN(LABADMIN)=> [\\q]{.mark} Output: [nz@localhost storedProcedure]\\$ Edit the addCustomer.sql file with vi with the following command nz@netezza storedProcedure]\\$ [vi addCustomer.sql]{.mark} In case of a message warning about duplicate files press enter. Add a new variable REC with the type RECORD in the DECLARE section (open a line by pressing o while the cursor is positioned on the line with the variable PHONE, this will enter you into the INSERT mode of vi.): [REC RECORD;]{.mark} A RECORD type is a row set with dynamic fields. It can refer to any row that is selected in a SELECT INTO statement. You can later refer to fields with for example REC.C_PHONE. Add the following statement before the INSERT statement, press ESC and move your cursor to the line with BEGIN and press o to open a line and enter INSERT mode: SELECT * INTO REC FROM CUSTOMER WHERE C_CUSTKEY = C_KEY; This statement fills the REC variable with the results of the query. If there are already one or more customers with the specified key it will contain the first. Otherwise the variable will be null. Now we add the IF condition to abort the stored procedure in case a record already exists. After the newly added SELECT statement add the following lines. Press ESC and move your cursor to the line with SELECT and press o to open a line and enter INSERT mode: IF FOUND REC THEN RAISE EXCEPTION \\'Customer with key % already exists\\', C_KEY; END IF; In this case we use an IF condition to check if a customer record with the key already exists and has been selected by the previous SELECT condition. We could do an implicit check on the record or any of its fields and see if it compares to the null value, but Netezza Performance Serverprovides a number of special variables that make this more convenient. FOUND specifies if the last SELECT INTO statement has returned any records ROW_COUNT contains the number of found rows in the last SELECT INTO statement LAST_OID is the object id of the last inserted row, this variable is not very useful unless used for catalog tables. Finally, we use a RAISE EXCEPTION statement to throw an error and abort the stored procedure. To add variable values to the return string use the % symbol anywhere in the string. This is a similar approach as used for example by the C printf statement. We will also check the foreign key relationship to NATION, add the following lines after the last END IF:. Press ESC and move your cursor to the line with END IF and press o to open a line and enter INSERT mode: SELECT * INTO REC FROM NATION WHERE N_NATIONKEY = N_KEY; IF NOT FOUND REC THEN RAISE EXCEPTION \\'No Nation with nation key %\\', N_KEY; END IF; This is very similar to the last check, only that we this time check if a record was NOT FOUND. Notice that we can reuse the REC record since it is not typed to a particular table. Your stored procedure should now look like the following: CREATE OR REPLACE PROCEDURE addCustomer(integer, varchar(25), integer, varchar(15)) LANGUAGE NZPLSQL RETURNS INT4 AS BEGIN_PROC DECLARE C_KEY ALIAS FOR \\$1; C_NAME ALIAS FOR \\$2; N_KEY ALIAS FOR \\$3; PHONE ALIAS FOR \\$4; REC RECORD; BEGIN SELECT * INTO REC FROM CUSTOMER WHERE C_CUSTKEY = C_KEY; IF FOUND REC THEN RAISE EXCEPTION \\'Customer with key % already exists\\', C_KEY; END IF; SELECT * INTO REC FROM NATION WHERE N_NATIONKEY = N_KEY; IF NOT FOUND REC THEN RAISE EXCEPTION \\'No Nation with nation key %\\', N_KEY; END IF; INSERT INTO CUSTOMER VALUES (C_KEY, C_NAME, \\'\\', N_KEY, PHONE, 0 ,\\'\\', \\'\\'); END; END_PROC; Save the stored procedure by pressing ESC, and then entering wq! and pressing Enter. Enter nzsql and connect to LABDB as user ADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql labdb admin]{.mark} Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit In nzsql create the stored procedure from the script by executing the following command (remember that you can cycle through previous commands by pressing the UP key) Input: LABDB.ADMIN(LABADMIN)=> [\\i addCustomer.sql]{.mark} Output: CREATE PROCEDURE Now test the check for duplicate customer ids by repeating our last CALL statement, we already know that a customer record with the id 999999 already exists: Input: LABDB.ADMIN(LABADMIN)=> CALL addCustomer(999999,\\'John Smith\\', 2, \\'555-5555\\'); Output: ERROR: Customer with key 999999 already exists This is expected, the key value already exists, and an error condition is thrown. Now let's check the foreign key integrity by executing the following command with a customer id that does not yet exist and a nation key that doesn't exist in the NATION table as well. You can double check this using SELECT statements if you want: Input: LABDB.ADMIN(LABADMIN)=> CALL addcustomer(999998,\\'James Brown\\', 99, \\'555-5555\\'); Output: ERROR: No Nation with nation key 99 This is also as we have expected. The customer key does not yet exist so the first IF condition is not thrown but the check for the nation key table throws an error. Finally let's try a working example, execute the following command with a customer id that doesn't yet exist and the NATION key 2 for United States. Input: LABDB.ADMIN(LABADMIN)=> CALL addCustomer(999998,\\'James Brown\\', 2, \\'555-5555\\'); Output: ADDCUSTOMER ------------- (1 row) You should see a successful execution. Check that the value was correctly inserted: Input: LABDB.ADMIN(LABADMIN)=> [SELECT C_CUSTKEY, C_NAME FROM CUSTOMER WHERE C_CUSTKEY = 999998;]{.mark} Output: C_CUSTKEY | C_NAME -----------+------------- 999998 | James Brown (1 row) You have successfully created a stored procedure that can be used to insert values into the CUSTOMER table and checks for unique and foreign key constraints. You should remember that Netezza Performance Server system isn't optimized to do lookup queries so this will be a pretty slow operation and shouldn't be used for thousands of inserts. But for the occasional management it is a perfectly valid solution to the problem of missing constraints in Netezza Performance Server system. Managing your stored procedure In the last chapters we have created a stored procedure that inserts values to the CUSTOMER table and does check constraints. We will now give rights to execute this procedure to a user and we will use the management functions to make changes to the stored procedure and verify them. First, we will create a user CUSTADMIN database ID will be responsible for adding customers, to do this we will need to switch to the admin user since users are global objects. Enter nzsql and connect to LABDB as user ADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql LABDB ADMIN]{.mark} Note: if your Linux environment variable NZ_USER is set to ADMIN you can omit ADMIN. Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN(ADMIN)=> Create the CUSTADMIN user using the following command: Input: LABDB.ADMIN(ADMIN)=> [create user custadmin with password \\'password\\';]{.mark} Output: CREATE USER You can see that he has the same password as the other users in our labs. We do this for simplification, since it allows us to omit the nzsql password switch/option, this would of course not be done in a production environment. [nz@localhost storedProcedure]\\$ printenv | grep NZ_PASS NZ_PASSWORD=password Now grant access to the LABDB database, otherwise CUSTADMIN cannot connect or read tables. Use the following data control language (DCL) to grant permissions to the user CUSTADMIN to allow connect on the database LABDB. Input: LABDB.ADMIN(ADMIN)=> [grant list, select on labdb to custadmin;]{.mark} Output: GRANT Grant CUSTADMIN the right to SELECT, INSERT from the CUSTOMER table: Input: LABDB.ADMIN(ADMIN)=> [grant select, insert on customer to custadmin;]{.mark} Output: GRANT Grant CUSTADMIN the right to SELECT from the NATION table: Input: LABDB.ADMIN(ADMIN)=> [grant select on nation to custadmin;]{.mark} Output: GRANT Test the connect permission on LABDB for the user CUSTADMIN: Input: LABDB.ADMIN(ADMIN)=> [\\c labdb custadmin password]{.mark} Output: You are now connected to database labdb as user custadmin. LABDB.ADMIN(CUSTADMIN)=> SELECT something from the NATION table to verify that the user only has access to the CUSTOMER table: Input: LABDB.ADMIN(CUSTADMIN)=> [select * from nation;]{.mark} Output: LABDB.ADMIN(CUSTADMIN)=> select * from nation; N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand (14 rows) SELECT C_CUSTKEY 999998 from the CUSTOMER table: Input: LABDB.ADMIN(CUSTADMIN)=> [select c_custkey, c_name]{.mark} LABDB.ADMIN(CUSTADMIN)-> [from customer]{.mark} LABDB.ADMIN(CUSTADMIN)-> [where c_custkey = 999998]{.mark} LABDB.ADMIN(CUSTADMIN)-> [;]{.mark} Notice that multiple lines can be used with nzsql and the semicolon terminates the statement. Press enter to continue to the next line. => Start of the SQL statement -> Continuation of the SQL statement -> ; End of SQL statement Output: C_CUSTKEY | C_NAME -----------+------------- 999998 | James Brown (1 row) The user should be able to select the row from the CUSTOMER table. Now connect as the ADMIN user to give CUSTADMIN the rights to execute the stored procedure: Input: LABDB.ADMIN(CUSTADMIN)=> [\\c labdb admin password]{.mark} Output: You are now connected to database labdb as user admin. LABDB.ADMIN(ADMIN)=> To grant the right to execute a specific stored procedure we need to specify the full name including all input parameters. The easiest way to get these in the correct syntax is to first list them with the SHOW PROCEDURE command: Input: LABDB.ADMIN(CUSTADMIN)=> [show procedure all;]{.mark} Output: SCHEMA | RESULT | PROCEDURE | BUILTIN | ARGUMENTS --------+---------+-------------+---------+------------------------------------------------------------------ ADMIN | INTEGER | ADDCUSTOMER | f | (INTEGER, CHARACTER VARYING(25), INTEGER, CHARACTER VARYING(15)) (1 row) Cut&paste the arguments or copy them manually for the next command. Grant the right to execute this stored procedure to CUSTADMIN: Input: LABDB.ADMIN(ADMIN)=> [grant execute on addcustomer]{.mark} LABDB.ADMIN(ADMIN)-> [(INTEGER, CHARACTER VARYING(25), INTEGER, CHARACTER VARYING(15))]{.mark} LABDB.ADMIN(ADMIN)-> [to custadmin;]{.mark} Output: GRANT Check the rights of the CUSTADMIN user now with \\dpu custadmin You should get the following results: Input: LABDB.ADMIN(ADMIN)=> \\dpu custadmin Output: User object permissions for user \\'CUSTADMIN\\' Database Name | Schema Name | Object Name | L S I U D T L A D B L G O E C R X A | D G U S T E X Q Y V M I B R C S H F A L P N S R ---------------+-------------+-------------+-------------------------------------+------------------------------------------------- LABDB | ADMIN | CUSTOMER | X X | LABDB | ADMIN | NATION | X X | LABDB | ADMIN | ADDCUSTOMER | X | GLOBAL | GLOBAL | LABDB | X X | (4 rows) Object Privileges (L)ist (S)elect (I)nsert (U)pdate (D)elete (T)runcate (L)ock (A)lter (D)rop a(B)ort (L)oad (G)enstats Gr(O)om (E)xecute Label-A(C)cess Label-(R)estrict Label-E(X)pand Execute-(A)s Administration Privilege (D)atabase (G)roup (U)ser (S)chema (T)able T(E)mp E(X)ternal Se(Q)uence S(Y)nonym (V)iew (M)aterialized View (I)ndex (B)ackup (R)estore va(C)uum (S)ystem (H)ardware (F)unction (A)ggregate (L)ibrary (P)rocedure U(N)fence (S)ecurity Scheduler (R)ule You can see that the CUSTADMIN user has only the rights granted. Test procedure as CUSTADMIN user, first connect to LABDB with the following command: Input: LABDB.ADMIN(ADMIN)=> [\\c labdb custadmin password]{.mark} Output: You are now connected to database labdb as user custadmin. LABDB.ADMIN(CUSTADMIN)=> INSERT another customer to the CUSTOMER table: Input: LABDB.ADMIN(CUSTADMIN)=> [call addcustomer(999997,\\'Jake Jones\\', 2, \\'555-5554\\');]{.mark} Output: ADDCUSTOMER ------------- (1 row) The insert will have been successful, and you will have another row in your table, you can check this with a SELECT query if you want. 15. Now make some changes to the stored procedure to do this connect to LABDB as ADMIN: Input: LABDB.ADMIN(CUSTADMIN)=> [\\c labdb admin]{.mark} Output: You are now connected to database labdb as user admin. LABDB.ADMIN(ADMIN)=> Modify the stored procedure but first look at the details. Input: LABDB.ADMIN(ADMIN)=> [show procedure addcustomer verbose;]{.mark} Output: SCHEMA | RESULT | PROCEDURE | BUILTIN | ARGUMENTS | OWNER | EXECUTEDASOWNER | VARARGS | DESCRIPTION | PROCEDURESOURCE --------+---------+-------------+---------+------------------------------------------------------------------+----------+-----------------+---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ADMIN | INTEGER | ADDCUSTOMER | f | (INTEGER, CHARACTER VARYING(25), INTEGER, CHARACTER VARYING(15)) | LABADMIN | t | f | | DECLARE C_KEY ALIAS FOR \\$1; C_NAME ALIAS FOR \\$2; N_KEY ALIAS FOR \\$3; PHONE ALIAS FOR \\$4; REC RECORD; BEGIN -- SELECT * INTO REC FROM CUSTOMER WHERE C_CUSTKEY = C_KEY; -- IF FOUND REC THEN -- RAISE EXCEPTION \\'Customer with key % already exists\\', C_KEY; -- END IF; -- -- SELECT * INTO REC FROM NATION WHERE N_NATIONKEY = N_KEY; -- IF NOT FOUND REC THEN -- RAISE EXCEPTION \\'No Nation with nation key %\\', N_KEY; -- END IF; INSERT INTO CUSTOMER VALUES (C_KEY, C_NAME, \\'\\', N_KEY, PHONE, 0 ,\\'\\', \\'\\'); END; (1 row) You can see the input and output arguments, procedure name, owner, if it is executed as owner or caller and other details. Verbose also shows you the source code of the stored procedure. We see that the description field is still empty so lets add a comment to the stored procedure. This is important to do if you have a big number of stored procedures in your system. Add a description to the stored procedure: It is necessary to specify the exact stored procedure signature including the input arguments, these can be cut& pasted from the output of the show procedures command. The COMMENT ON command can be used to add descriptions to more or less all database objects you own from procedures, tables till columns. Verify that your description has been set: LABDB(ADMIN)=> show procedure addcustomer verbose; The description field will now contain your comment: We will now alter the stored procedure to be executed as the caller instead of the owner. This means that whoever executes the stored procedure needs to have access rights to all the objects that are touched in the stored procedure otherwise it will fail. This should be the default for stored procedures that encapsulate business logic and do not do extensive data checking: LABDB(ADMIN)=> alter procedure addcustomer(INTEGER, CHARACTER VARYING(25), LABDB(ADMIN)-> INTEGER,CHARACTER VARYING(15)) execute as caller; 20. Since the admin user has access to the customer table he will be able to execute the stored procedure: LABDB(ADMIN)=> [call addCustomer(999996,\\'Karl Schwarz\\', 2, \\'555-5553\\');]{.mark} Switch to the CUSTADMIN user: LABDB(ADMIN)=> [\\c labdb custadmin]{.mark} Try to add another customer as CUSTADMIN: LABDB(CUSTADMIN)=> [call addCustomer(999995, \\'John Schwarz\\', 2, \\'555-5553\\');]{.mark} You should see the following results: As expected, the stored procedure fails now. The user custadmin has read access to the CUSTOMER table but no read access to the NATION table, therefore this check results in an exception. While EXECUTE AS CALLER is more secure in some circumstances it doesn't fit our usecase where we specifically want to expose some data modification ability to a user who shouldn't be able to modify a table otherwise. Therefore we will change the stored procedure back: First switch back to the admin user: LABDB(CUSTADMIN)=> [\\c labdb admin]{.mark} 24. Change the stored procedure back to being executed as owner: LABDB(ADMIN)=> alter procedure addcustomer(INTEGER, CHARACTER VARYING(25), LABDB(ADMIN)-> INTEGER,CHARACTER VARYING(15)) execute as owner; In this chapter you setup the permissions for the addCustomer stored procedure and the user CUSTADMIN who is supposed to use it. You also added comments to the stored procedure. Implementing the checkRegions stored procedure In this chapter we will implement a stored procedure that performs a check on all rows of the regions table. The call of the stored procedure will be very simple and will not contain input arguments. The stored procedure is used to encapsulate a sanity check of the regions table that is executed regularly in the IBM Performance Server's system for administrative purposes. Our stored procedure will check each row of the REGION table for three things: If the region key is smaller than 1 If the name string is empty If the description is lower case only this is needed for application reasons. The procedure will return each row of the region table together with additional columns that describe if the above constraints are broken. It will also return a notice with the number of faulty rows. This chapter will teach you to use loops in a stored procedure and to return table results. You will also use dynamic query execution to create queries on the fly. Exit the NZSQL console by executing the \\q command and open the already existing empty file checkRegion.sql with the following command (note you can tab out the filename): [nz@netezza storedProcedure]\\$ vi checkRegion.sql You are now in the familiar vi interface and you can edit the file. Switch to INSERT mode by pressing i First, we will define the stored procedure header similar to the last procedure. It will be very simple since we will not use any input arguments. Enter the following code to the editor: Let's have a detailed look at the RETURNS section. We want to return a result set but do not have to describe the column names or datatypes of the table object that is returned. Instead we reference an existing table, which needs to exist at the time the stored procedure is created. This means we will need to create the table TB1 before executing the CREATE PROCEDURE command. Once the stored procedure is executed the stored procedure will create under the cover an empty temporary table that has the same definition as the referenced table. So, the results will not actually be saved in the referenced table, which is only used for the definition. This means that multiple stored procedures can be executed at the same time without influencing each other. Since the created table is temporary it will be cleaned up once the connection to the database is aborted. Note: If the referenced table contains rows they will neither be changed nor copied over to the temporary table, the table is strictly used for reference. For our stored procedure we need four variables, add the following lines after the BEGIN_PROC statement: DECLARE rec RECORD; errorRows INTEGER; fieldEmpty BOOLEAN; descUpper BOOLEAN; The four variables needed for our stored procedure: rec, is a RECORD structure while we loop through the rows of the table we will use it to save and access the values of each row and check them with our constraints errorRows will be used to contain the total number of rows that violate our constraints fieldEmpty will be used to store if the row violates either the constraint that the name is empty or the record code is smaller than 1, this is appropriate since values of -1 or 0 in the region code are used to denote that it is empty descUpper will be true if a record violates the constraint that the description needs to be lowercase We will now add the main BEGIN..END clause and initialize the errorRows variable. Add the following rows after the DECLARE section: BEGIN RAISE NOTICE \\'Start check of Region\\'; errorRows := 0; END; Each stored procedure must at least contain one BEGIN..END clause, which encapsulates the executed commands. We also initially set the number of error rows to 0 and display a short sentence. We will now add the main loop. It will iterate through all rows of the REGION table and store each row in the rec variable. Add the following lines before the END statement FOR rec IN SELECT * FROM REGION ORDER BY R_REGIONKEY LOOP fieldEmpty := false; descUpper := false; END LOOP; RAISE NOTICE \\' % rows had an error see result set\\', errorRows; The FOR rec IN expression LOOP..END LOOP command is used to iterate through a result set, in our case a SELECT * on the REGION table. The loop body is executed once for every row in the expression and the current row is saved in the rec field. The loop needs to be ended with the END LOOP keyword. There are many other types of loops in NZPLSQL, for a complete set refer to the stored procedure guide. For each iteration of the loop we initially set the value of the fieldEmpty and descUpper to false. Variables can be assigned with the := operator. Finally, we will display a notice that shows the number of rows that either had an empty field or upper case expression. This number will be saved in the errorRows variable. Now it's time to check the rows for our constraints and set our variables accordingly. Enter the following rows behind the variable initialization and before the END LOOP keyword: IF rec.R_NAME = \\'\\' OR rec.R_REGIONKEY \\< 1 THEN fieldEmpty := true; END IF; IF rec.R_COMMENT \\<> LOWER(rec.R_COMMENT) THEN descUpper := true; END IF; IF (fieldEmpty = true) OR (descUpper = true) THEN errorRows := errorRows + 1; END IF; In this section we check our constraints for each row and set our three variables accordingly. First, we check if the name field of the row is the empty string or if the region key is smaller than one. In that case the fieldEmpty field is set to true. Note how we can access the fields by adding the fieldname to our loop record. The second IF statement checks if the comment field of the row is different to the lower case version of the comment field. This would be the case if it contains uppercase characters. Note that we can use the available Netezza Performance Server functions like LOWER in the stored procedure, as if it were a SQL statement. Finally, if one of these variables has been set to true by the previous checks, we increase the value of the errorRows variable by one. The final number will in the end be displayed by the RAISE NOTICE statement we already added to the stored procedure. Finally add the following lines after the lines you just added and before the END LOOP statement: EXECUTE IMMEDIATE \\'INSERT INTO \\'|| REFTABLENAME ||\\' VALUES (\\' || rec.R_REGIONKEY ||\\',\\'\\'\\' || trim(rec.R_NAME) ||\\'\\'\\',\\'\\'\\' || trim(rec.R_COMMENT) ||\\'\\'\\',\\' || fieldEmpty ||\\',\\' || descUpper ||\\')\\'; These lines add the row of the REGION table to the result set of our stored procedure adding two columns containing the fieldEmpty and descUpper flags for this row. There are a couple of important points here: For each call of a stored procedure with a result set as return value a temporary table is created that is later returned to the caller. Since the name is unique it needs to be referenced through a variable. This is the REFTABLENAME variable. Apart from that, adding values to the result set is identical to other INSERT operations. Since the name of the table is dynamic we need to execute the INSERT operations as a dynamic statement. This means that the EXECUTE IMMEDIATE statement is used with a string that contains the query that is to be executed. To add variable values to the string the pipe symbol || is used. Note that the values for R_NAME and R_COMMENT are inserted as strings, which means they need to be surrounded by quotes. To add quotes to a string they need to be escaped with a second quote character. This is the reason that R_NAME and R_COMMENT is surrounded by triple quotes. Apart from that we trim them, so the inserted VARCHAR values are not blown up with empty characters. It can be tricky to construct a string like that and you will see the error only once it is executed. For debugging it can be useful to construct the string and display it with a RAISE NOTICE statement. Your vi should now look like that, containing the complete stored procedure: CREATE OR REPLACE PROCEDURE checkRegions() LANGUAGE NZPLSQL RETURNS REFTABLE(tb1) AS BEGIN_PROC DECLARE rec RECORD; errorRows INTEGER; fieldEmpty BOOLEAN; descUpper BOOLEAN; BEGIN RAISE NOTICE \\'Start check of Region\\'; errorRows := 0; FOR rec IN SELECT * FROM REGION ORDER BY R_REGIONKEY LOOP fieldEmpty := false; descUpper := false; IF rec.R_NAME = \\'\\' OR rec.R_REGIONKEY \\< 1 THEN fieldEmpty := true; END IF; IF rec.R_COMMENT \\<> lower(rec.R_COMMENT) THEN descUpper := true; END IF; IF (fieldEmpty = true) OR (descUpper = true) THEN errorRows := errorRows + 1; END IF; EXECUTE IMMEDIATE \\'INSERT INTO \\'|| REFTABLENAME ||\\' VALUES (\\' || rec.R_REGIONKEY ||\\',\\'\\'\\' || trim(rec.R_NAME) ||\\'\\'\\',\\'\\'\\' || trim(rec.R_COMMENT) ||\\'\\'\\',\\' || fieldEmpty ||\\',\\' || descUpper ||\\')\\'; END LOOP; RAISE NOTICE \\' % rows had an error see result set\\', errorRows; END; END_PROC; Save and exit vi. Press ESC to enter the command mode, enter :wq! to save and force quit and press enter. Enter nzsql and connect to LABDB as user LABADMIN. [nz@netezza storedProcedure]\\$ [nzsql LABDB ADMIN]{.mark} To create the stored procedure the table reference TB1 needs to exist. Create the table with the following statement: LABDB(ADMIN)=> [create table TB1 as]{.mark} LABDB(ADMIN)-> [select *, false AS FIELDEMPTY, false as DESCUPPER]{.mark} LABDB(ADMIN)-> [from region limit 0;]{.mark} This command creates a table TB1 that has all the rows of the REGION table and two additional BOOLEAN fields FIELDNULL and DESCUPPER. It will also be empty because we used the LIMIT 0 clause. Describe the reference table with LABDB(ADMIN)=> \\d TB1 You should see the following result: Table \\\"TB1\\\" Attribute | Type | Modifier | Default Value -------------+------------------------+----------+--------------- R_REGIONKEY | INTEGER | NOT NULL | R_NAME | CHARACTER(40) | | R_COMMENT | CHARACTER VARYING(152) | | FIELDEMPTY | BOOLEAN | | DESCUPPER | BOOLEAN | | Distributed on hash: \\\"R_REGIONKEY\\\" You can see the three columns of the REGION table and the two additional BOOLEAN fields that will contain for each row if the row violates the specified constraints. Note this table needs to exist before the procedure can be created. Now create the stored procedure. Execute the script you just created with the following command: LABDB(ADMIN)=> \\i checkRegion.sql You should successfully create your stored procedure. Now let's have a look at our REGION table, select all rows: LABDB(ADMIN)=> SELECT * FROM REGION; You will get the following results: LABDB(ADMIN)=> SELECT * FROM REGION; R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 2 | sa | south america 1 | na | north america 4 | ap | asia pacific 3 | emea | europe, middle east, africa (4 rows) We can see that none of the rows would violate the constraints we defined which would be pretty boring. So lets test our stored procedure by adding two rows that violate our constraints. Add the two violating rows with the following commands: LABDB(ADMIN)=> INSERT INTO REGION VALUES (0, \\'as\\', \\'Australia\\'); This row violates the lower case constraints for the comment field and the empty field constraint for the region key LABDB(ADMIN)=> INSERT INTO REGION VALUES (6, \\'\\', \\'mongolia\\'); This row violates the empty field constraint for the region name. Now finally let's try our checkRegions stored procedure: LABDB(ADMIN)=> call checkRegions(); You should see the following output: NOTICE: Start check of Region NOTICE: 2 rows had an error see result set R_REGIONKEY | R_NAME | R_COMMENT | FIELDEMPTY | DESCUPPER -------------+---------------------------+-----------------------------+------------+----------- 1 | na | north america | f | f 3 | emea | europe, middle east, africa | f | f 0 | as | Australia | t | t 4 | ap | asia pacific | f | f 2 | sa | south america | f | f 6 | | mongolia | t | f (6 rows) You can see the expected results. Our stored procedure has found two rows that violated the constraints we check for. In the FIELDNULL and DESCUPPER columns we can easily see that the row with the key 0 has both an empty field and uppercase comment. We can also see that row 6 only violated the empty field constraint. Note that the TB1 table we created doesn't contain any rows, it is only used as a template. Finally let's cleanup our REGION table again: LABDB(ADMIN)=> DELETE FROM REGION WHERE R_REGIONKEY = 0 OR R_REGIONKEY = 6; And let's run our checkRegions procedure again: LABDB(ADMIN)=> call checkRegions(); You will see the following results: NOTICE: Start check of Region NOTICE: 0 rows had an error see result set R_REGIONKEY | R_NAME. | R_COMMENT | FIELDEMPTY | DESCUPPER -------------+---------------------------+-----------------------------+------------+----------- 3 | emea | europe, middle east, africa | f | f 4 | ap | asia pacific. | f | f 1 | na | north America | f | f 2 | sa. | south america | f | f (4 rows) You can see that the table now is error free and all constraint violation fields are false. Congratulations you have finished the stored procedure lab and created two stored procedures that help you to manage your database.","title":"Stored Procedures"},{"location":"nz-10-Stored-Proc/#stored-procedures","text":"Stored Procedures are subroutines that are saved in IBM Performance Server. They are executed inside the database server and are only available by accessing the NPS system. They combine the capabilities of SQL to query and manipulate database information with capabilities of procedural programming languages, like branching and iterations. This makes them an ideal solution for tasks like data validation, writing event logs or encrypting data. They are especially suited for repetitive tasks that can be easily encapsulated in a sub-routine.","title":"Stored Procedures"},{"location":"nz-10-Stored-Proc/#objectives","text":"In the last labs we have created our database, loaded the data and we have done some optimization and administration tasks. In this lab we will enhance the database by a couple of stored procedures. As we mentioned in a previous chapter Netezza Performance Serverdoesn't check referential or unique constraints. This is normally not critical since data loading in a data warehousing environment is a controlled task. In our Netezza Performance Serverimplementation we get the requirement to allow some non-administrative database users to add new customers to the customer table. This happens rarely so there are no performance requirements and we have decided to implement this with a stored procedure that is accessible for these users and checks the input values and referential constraints. In a second part we will implement a business logic function as a stored procedure returning a result set. TODO describe function. {width=\"4.248510498687664in\" height=\"3.9270833333333335in\"} Figure 1 LABDB database","title":"Objectives"},{"location":"nz-10-Stored-Proc/#lab-setup","text":"This lab uses an initial setup script to make sure the correct user and database exist for the remainder of the lab. Follow the instructions below to run the setup script. Login to NPS Command Line using one of these two methods. a. Login to the VM directly and use the terminal application available inside the VM. b. Connect to your Netezza Performance Server image using putty If you are continuing from the previous lab and are already connected to NZSQL quit the NZSQL console with the [\\q]{.mark} command. Prepare for this lab by running the setup script. To do this use the following two commands: Input: [nz@localhost labs]\\$ [cd \\~/labs/storedProcedure/setupLab]{.mark} [nz@localhost setupLab]\\$ [./setupLab.sh]{.mark} Output: DROP DATABASE CREATE DATABASE ERROR: CREATE USER: object LABADMIN already exists as a USER. ALTER USER ALTER DATABASE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE CREATE TABLE Load session of table \\'NATION\\' completed successfully Load session of table \\'REGION\\' completed successfully Load session of table \\'CUSTOMER\\' completed successfully Load session of table \\'SUPPLIER\\' completed successfully Load session of table \\'PART\\' completed successfully Load session of table \\'PARTSUPP\\' completed successfully Load session of table \\'ORDERS\\' completed successfully Load session of table \\'LINEITEM\\' completed successfully There may be error message at the beginning of the output since the script tries to clean up existing databases and users.","title":"Lab Setup"},{"location":"nz-10-Stored-Proc/#implementing-the-addcustomer-stored-procedure","text":"In this chapter we will create a stored procedure to insert data into the CUSTOMER table. The information that is added for a new customer will be the customer key (C_CUSTKEY), name (C_NAME), phone number (C_PHONE) and nation key (C_NATIONKEY), the rest of the information is updated through other processes.","title":"Implementing the addCustomer stored procedure"},{"location":"nz-10-Stored-Proc/#create-insert-stored-procedure","text":"First, review the customer table and define the interface of the insert stored procedure. Connect to your Netezza image using a terminal application (i.e.: PuTTY or Terminal.app). Login to \\<ip-provided-by-your-instructor> as user nz with password nz. Access the lab directory for this lab with the following command, this folder already contains empty files for the stored procedure scripts we will later create. If you want review them with the ls command: [nz@localhost \\~]\\$ [cd labs/storedProcedure/]{.mark} Enter nzsql and connect to LABDB as user LABADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql LABDB LABADMIN]{.mark} Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit Describe the customer table with the following command \\d customer You should see the following: Input: [nz@localhost storedProcedure]\\$ \\d customer Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit We will now create a stored procedure that adds a new customer entry and sets the 4 fields: C_CUSTKEY, C_NAME, C_NATIONKEY, and C_PHONE, all other fields will be set with an empty value or 0, since the fields are flagged as NOT NULL. Exit the nzsql console by executing the \\q command. Input: LABDB.ADMIN(LABADMIN)=> \\q Output: [nz@localhost storedProcedure]\\$ To create a stored procedure we will use the internal vi editor. Open the already existing empty file addCustomer.sql with the following command: [nz@netezza storedProcedure]\\$ vi addCustomer.sql You are now in the familiar vi interface and you can edit the file. Switch to INSERT mode by pressing i. We will now create the interface of the stored procedure so we can test creating it. We need the 4 input field mentioned above and will return an integer return code. Enter the text as seen in the following, then exit the insert mode by pressing ESC and enter wq! and enter to save the file and quit vi. [CREATE OR REPLACE PROCEDURE addCustomer(integer, varchar(25), integer, varchar(15))]{.mark} [LANGUAGE NZPLSQL RETURNS INT4 AS]{.mark} [BEGIN_PROC]{.mark} [END_PROC;]{.mark} The minimal stored procedure we create here doesn't yet do anything, since it has an empty body. We simply create the signature with the input and output variables. We use the command CREATE OR REPLACE so we can later execute the same command multiple times to update the stored procedure with more code. The input variables cannot be given names so we only add the datatypes for our input parameters key, name, nation and phone. We also return an integer return code. Note that we have to specify the procedure language even though nzplsql is the only available option in Netezza Performance Server system. Enter nzsql and connect to LABDB as user LABADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql LABDB LABADMIN]{.mark} Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit Back in the nzsql command line execute the script we just created with \\i addCustomer.sql You should see, that the procedure has been created successfully Input: LABDB.ADMIN(LABADMIN)=> [\\i addCustomer.sql]{.mark} Output: CREATE PROCEDURE Display all stored procedures in the LABDB database with the following command: Input: LABDB.ADMIN(LABADMIN)=> [SHOW PROCEDURE;]{.mark} Output: SCHEMA | RESULT | PROCEDURE | BUILTIN | ARGUMENTS --------+---------+-------------+---------+------------------------------------------------------------------ ADMIN | INTEGER | ADDCUSTOMER | f | (INTEGER, CHARACTER VARYING(25), INTEGER, CHARACTER VARYING(15)) (1 row) You can see the procedure ADDCUSTOMER with the arguments we specified. Execute the stored procedure with the following dummy input parameters: Input: LABDB.ADMIN(LABADMIN)=> [call addcustomer(1,\\'test\\', 2, \\'test\\');]{.mark} Output: NOTICE: plpgsql: ERROR during compile of ADDCUSTOMER near line 1 ERROR: syntax error, unexpected \\<EOF>, expecting BEGIN at or near \\\"\\\" The result shows that we have a syntax error in our stored procedure. Every stored procedure needs at least one BEGIN .. END block that encapsulates the code that is to be executed. Stored procedures are compiled when they are first executed not when they are created, therefore errors in the code can only be seen during execution. Exit the nzsql console by executing the \\q command. Input: LABDB.ADMIN(LABADMIN)=> [\\q]{.mark} Output: [nz@localhost storedProcedure]\\$ Edit the addCustomer.sql file with vi with the following command nz@netezza storedProcedure]\\$ [vi addCustomer.sql]{.mark} We will now create a simple stored procedure that inserts the new entry into the customer table. But first we will add some variables that alias the input variables \\$1, \\$2 etc. After the BEGIN_PROC statement enter the following lines (open a line by pressing o while the cursor is positioned on the line BEGIN_PROC, this will enter you into the INSERT mode of vi.): [DECLARE]{.mark} [C_KEY ALIAS FOR \\$1;]{.mark} [C_NAME ALIAS FOR \\$2;]{.mark} [N_KEY ALIAS FOR \\$3;]{.mark} [PHONE ALIAS FOR \\$4;]{.mark} Each BEGIN..END block in the stored procedure can have its own DECLARE section. Variables are valid in the block they belong to. It is a good best practice to change the input parameters into readable variable names to make the stored procedure code maintainable. We will later add some additional parameters to our procedures as well. Be careful not to use variable names that are restricted by Netezza Performance Server system, for example NAME. Next we will add the BEGIN..END block with the INSERT statement. [BEGIN]{.mark} [INSERT INTO CUSTOMER VALUES (C_KEY, C_NAME, \\'\\', N_KEY, PHONE, 0, \\'\\', \\'\\');]{.mark} [END;]{.mark} This statement will add a new row to the customer table using the input variables. It will replace the remaining fields like account balance with default values that can be later filled. It is also possible to execute dynamic SQL queries which we will do in a later chapter. Your complete stored procedure should now look like the following: [CREATE OR REPLACE PROCEDURE addCustomer(integer, varchar(25), integer, varchar(15))]{.mark} [LANGUAGE NZPLSQL RETURNS INT4 AS]{.mark} [BEGIN_PROC]{.mark} [DECLARE]{.mark} [C_KEY ALIAS FOR \\$1;]{.mark} [C_NAME ALIAS FOR \\$2;]{.mark} [N_KEY ALIAS FOR \\$3;]{.mark} [PHONE ALIAS FOR \\$4;]{.mark} [BEGIN]{.mark} [INSERT INTO CUSTOMER VALUES (C_KEY, C_NAME, \\'\\', N_KEY, PHONE, 0 \\'\\', \\'\\');]{.mark} [END;]{.mark} [END_PROC;]{.mark} Save and exit vi again by pressing ESC to enter the command mode and entering wq! and pressing enter. This will bring you back to the Linux command line. 17. Enter NZSQL and connect to LABDB as user LABADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql LABDB LABADMIN]{.mark} Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit Execute the stored procedure script with the following command: \\i addCustomer.sql Input: LABDB.ADMIN(LABADMIN)=> [\\i addCustomer.sql]{.mark} Output: CREATE PROCEDURE Now try the stored procedure lets add a new customer John Smith with customer key 999999, phone number 555-5555 and nation 2 (which is the key for the United States in our NATION table). You can also check first that the customer doesn't yet exist if you want. Input: LABDB.ADMIN(LABADMIN)=> [CALL addCustomer(999999,\\'John Smith\\', 2, \\'555-5555\\');]{.mark} Output: ADDCUSTOMER ------------- (1 row) Check if the insert was successful: Input: LABDB.ADMIN(LABADMIN)=> [SELECT * FROM CUSTOMER WHERE C_CUSTKEY = 999999;]{.mark} Output: C_CUSTKEY | C_NAME | C_ADDRESS | C_NATIONKEY | C_PHONE | C_ACCTBAL | C_MKTSEGMENT | C_COMMENT -----------+------------+-----------+-------------+-----------------+-----------+--------------+----------- 999999 | John Smith | | 2 | 555-5555 | 0.00 | | (1 row) Congratulations, you have built your first Netezza Performance Serverstored procedure.","title":"Create Insert Stored Procedure"},{"location":"nz-10-Stored-Proc/#adding-integrity-checks","text":"In this chapter we will add integrity checks to the stored procedure we just created. We will make sure that no duplicate customer is entered into the CUSTOMER table by querying it before the insert. We will then check with an IF condition if the value had already been inserted into the CUSTOMER table and abort the insert in that case. We will also check the foreign key relationship to the nation table and make sure that no customer is inserted for a nation that doesn't exist. If any of these conditions aren't met the procedure will abort and display an error message. Exit the nzsql console by executing the \\q command. Input: LABDB.ADMIN(LABADMIN)=> [\\q]{.mark} Output: [nz@localhost storedProcedure]\\$ Edit the addCustomer.sql file with vi with the following command nz@netezza storedProcedure]\\$ [vi addCustomer.sql]{.mark} In case of a message warning about duplicate files press enter. Add a new variable REC with the type RECORD in the DECLARE section (open a line by pressing o while the cursor is positioned on the line with the variable PHONE, this will enter you into the INSERT mode of vi.): [REC RECORD;]{.mark} A RECORD type is a row set with dynamic fields. It can refer to any row that is selected in a SELECT INTO statement. You can later refer to fields with for example REC.C_PHONE. Add the following statement before the INSERT statement, press ESC and move your cursor to the line with BEGIN and press o to open a line and enter INSERT mode: SELECT * INTO REC FROM CUSTOMER WHERE C_CUSTKEY = C_KEY; This statement fills the REC variable with the results of the query. If there are already one or more customers with the specified key it will contain the first. Otherwise the variable will be null. Now we add the IF condition to abort the stored procedure in case a record already exists. After the newly added SELECT statement add the following lines. Press ESC and move your cursor to the line with SELECT and press o to open a line and enter INSERT mode: IF FOUND REC THEN RAISE EXCEPTION \\'Customer with key % already exists\\', C_KEY; END IF; In this case we use an IF condition to check if a customer record with the key already exists and has been selected by the previous SELECT condition. We could do an implicit check on the record or any of its fields and see if it compares to the null value, but Netezza Performance Serverprovides a number of special variables that make this more convenient. FOUND specifies if the last SELECT INTO statement has returned any records ROW_COUNT contains the number of found rows in the last SELECT INTO statement LAST_OID is the object id of the last inserted row, this variable is not very useful unless used for catalog tables. Finally, we use a RAISE EXCEPTION statement to throw an error and abort the stored procedure. To add variable values to the return string use the % symbol anywhere in the string. This is a similar approach as used for example by the C printf statement. We will also check the foreign key relationship to NATION, add the following lines after the last END IF:. Press ESC and move your cursor to the line with END IF and press o to open a line and enter INSERT mode: SELECT * INTO REC FROM NATION WHERE N_NATIONKEY = N_KEY; IF NOT FOUND REC THEN RAISE EXCEPTION \\'No Nation with nation key %\\', N_KEY; END IF; This is very similar to the last check, only that we this time check if a record was NOT FOUND. Notice that we can reuse the REC record since it is not typed to a particular table. Your stored procedure should now look like the following: CREATE OR REPLACE PROCEDURE addCustomer(integer, varchar(25), integer, varchar(15)) LANGUAGE NZPLSQL RETURNS INT4 AS BEGIN_PROC DECLARE C_KEY ALIAS FOR \\$1; C_NAME ALIAS FOR \\$2; N_KEY ALIAS FOR \\$3; PHONE ALIAS FOR \\$4; REC RECORD; BEGIN SELECT * INTO REC FROM CUSTOMER WHERE C_CUSTKEY = C_KEY; IF FOUND REC THEN RAISE EXCEPTION \\'Customer with key % already exists\\', C_KEY; END IF; SELECT * INTO REC FROM NATION WHERE N_NATIONKEY = N_KEY; IF NOT FOUND REC THEN RAISE EXCEPTION \\'No Nation with nation key %\\', N_KEY; END IF; INSERT INTO CUSTOMER VALUES (C_KEY, C_NAME, \\'\\', N_KEY, PHONE, 0 ,\\'\\', \\'\\'); END; END_PROC; Save the stored procedure by pressing ESC, and then entering wq! and pressing Enter. Enter nzsql and connect to LABDB as user ADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql labdb admin]{.mark} Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit In nzsql create the stored procedure from the script by executing the following command (remember that you can cycle through previous commands by pressing the UP key) Input: LABDB.ADMIN(LABADMIN)=> [\\i addCustomer.sql]{.mark} Output: CREATE PROCEDURE Now test the check for duplicate customer ids by repeating our last CALL statement, we already know that a customer record with the id 999999 already exists: Input: LABDB.ADMIN(LABADMIN)=> CALL addCustomer(999999,\\'John Smith\\', 2, \\'555-5555\\'); Output: ERROR: Customer with key 999999 already exists This is expected, the key value already exists, and an error condition is thrown. Now let's check the foreign key integrity by executing the following command with a customer id that does not yet exist and a nation key that doesn't exist in the NATION table as well. You can double check this using SELECT statements if you want: Input: LABDB.ADMIN(LABADMIN)=> CALL addcustomer(999998,\\'James Brown\\', 99, \\'555-5555\\'); Output: ERROR: No Nation with nation key 99 This is also as we have expected. The customer key does not yet exist so the first IF condition is not thrown but the check for the nation key table throws an error. Finally let's try a working example, execute the following command with a customer id that doesn't yet exist and the NATION key 2 for United States. Input: LABDB.ADMIN(LABADMIN)=> CALL addCustomer(999998,\\'James Brown\\', 2, \\'555-5555\\'); Output: ADDCUSTOMER ------------- (1 row) You should see a successful execution. Check that the value was correctly inserted: Input: LABDB.ADMIN(LABADMIN)=> [SELECT C_CUSTKEY, C_NAME FROM CUSTOMER WHERE C_CUSTKEY = 999998;]{.mark} Output: C_CUSTKEY | C_NAME -----------+------------- 999998 | James Brown (1 row) You have successfully created a stored procedure that can be used to insert values into the CUSTOMER table and checks for unique and foreign key constraints. You should remember that Netezza Performance Server system isn't optimized to do lookup queries so this will be a pretty slow operation and shouldn't be used for thousands of inserts. But for the occasional management it is a perfectly valid solution to the problem of missing constraints in Netezza Performance Server system.","title":"Adding integrity checks"},{"location":"nz-10-Stored-Proc/#managing-your-stored-procedure","text":"In the last chapters we have created a stored procedure that inserts values to the CUSTOMER table and does check constraints. We will now give rights to execute this procedure to a user and we will use the management functions to make changes to the stored procedure and verify them. First, we will create a user CUSTADMIN database ID will be responsible for adding customers, to do this we will need to switch to the admin user since users are global objects. Enter nzsql and connect to LABDB as user ADMIN. Input: [nz@localhost storedProcedure]\\$ [nzsql LABDB ADMIN]{.mark} Note: if your Linux environment variable NZ_USER is set to ADMIN you can omit ADMIN. Output: Welcome to nzsql, the IBM Netezza SQL interactive terminal. Type: \\h for help with SQL commands \\? for help on internal slash commands \\g or terminate with semicolon to execute query \\q to quit LABDB.ADMIN(ADMIN)=> Create the CUSTADMIN user using the following command: Input: LABDB.ADMIN(ADMIN)=> [create user custadmin with password \\'password\\';]{.mark} Output: CREATE USER You can see that he has the same password as the other users in our labs. We do this for simplification, since it allows us to omit the nzsql password switch/option, this would of course not be done in a production environment. [nz@localhost storedProcedure]\\$ printenv | grep NZ_PASS NZ_PASSWORD=password Now grant access to the LABDB database, otherwise CUSTADMIN cannot connect or read tables. Use the following data control language (DCL) to grant permissions to the user CUSTADMIN to allow connect on the database LABDB. Input: LABDB.ADMIN(ADMIN)=> [grant list, select on labdb to custadmin;]{.mark} Output: GRANT Grant CUSTADMIN the right to SELECT, INSERT from the CUSTOMER table: Input: LABDB.ADMIN(ADMIN)=> [grant select, insert on customer to custadmin;]{.mark} Output: GRANT Grant CUSTADMIN the right to SELECT from the NATION table: Input: LABDB.ADMIN(ADMIN)=> [grant select on nation to custadmin;]{.mark} Output: GRANT Test the connect permission on LABDB for the user CUSTADMIN: Input: LABDB.ADMIN(ADMIN)=> [\\c labdb custadmin password]{.mark} Output: You are now connected to database labdb as user custadmin. LABDB.ADMIN(CUSTADMIN)=> SELECT something from the NATION table to verify that the user only has access to the CUSTOMER table: Input: LABDB.ADMIN(CUSTADMIN)=> [select * from nation;]{.mark} Output: LABDB.ADMIN(CUSTADMIN)=> select * from nation; N_NATIONKEY | N_NAME | N_REGIONKEY | N_COMMENT -------------+---------------------------+-------------+---------------------------------- 1 | canada | 1 | canada 2 | united states | 1 | united states of america 3 | brazil | 2 | brasil 4 | guyana | 2 | guyana 5 | venezuela | 2 | venezuela 6 | united kingdom | 3 | united kingdom 7 | portugal | 3 | portugal 8 | united arab emirates | 3 | al imarat al arabiyah multahidah 9 | south africa | 3 | south africa 10 | australia | 4 | australia 11 | japan | 4 | nippon 12 | macau | 4 | aomen 13 | hong kong | 4 | xianggang 14 | new zealand | 4 | new zealand (14 rows) SELECT C_CUSTKEY 999998 from the CUSTOMER table: Input: LABDB.ADMIN(CUSTADMIN)=> [select c_custkey, c_name]{.mark} LABDB.ADMIN(CUSTADMIN)-> [from customer]{.mark} LABDB.ADMIN(CUSTADMIN)-> [where c_custkey = 999998]{.mark} LABDB.ADMIN(CUSTADMIN)-> [;]{.mark} Notice that multiple lines can be used with nzsql and the semicolon terminates the statement. Press enter to continue to the next line. => Start of the SQL statement -> Continuation of the SQL statement -> ; End of SQL statement Output: C_CUSTKEY | C_NAME -----------+------------- 999998 | James Brown (1 row) The user should be able to select the row from the CUSTOMER table. Now connect as the ADMIN user to give CUSTADMIN the rights to execute the stored procedure: Input: LABDB.ADMIN(CUSTADMIN)=> [\\c labdb admin password]{.mark} Output: You are now connected to database labdb as user admin. LABDB.ADMIN(ADMIN)=> To grant the right to execute a specific stored procedure we need to specify the full name including all input parameters. The easiest way to get these in the correct syntax is to first list them with the SHOW PROCEDURE command: Input: LABDB.ADMIN(CUSTADMIN)=> [show procedure all;]{.mark} Output: SCHEMA | RESULT | PROCEDURE | BUILTIN | ARGUMENTS --------+---------+-------------+---------+------------------------------------------------------------------ ADMIN | INTEGER | ADDCUSTOMER | f | (INTEGER, CHARACTER VARYING(25), INTEGER, CHARACTER VARYING(15)) (1 row) Cut&paste the arguments or copy them manually for the next command. Grant the right to execute this stored procedure to CUSTADMIN: Input: LABDB.ADMIN(ADMIN)=> [grant execute on addcustomer]{.mark} LABDB.ADMIN(ADMIN)-> [(INTEGER, CHARACTER VARYING(25), INTEGER, CHARACTER VARYING(15))]{.mark} LABDB.ADMIN(ADMIN)-> [to custadmin;]{.mark} Output: GRANT Check the rights of the CUSTADMIN user now with \\dpu custadmin You should get the following results: Input: LABDB.ADMIN(ADMIN)=> \\dpu custadmin Output: User object permissions for user \\'CUSTADMIN\\' Database Name | Schema Name | Object Name | L S I U D T L A D B L G O E C R X A | D G U S T E X Q Y V M I B R C S H F A L P N S R ---------------+-------------+-------------+-------------------------------------+------------------------------------------------- LABDB | ADMIN | CUSTOMER | X X | LABDB | ADMIN | NATION | X X | LABDB | ADMIN | ADDCUSTOMER | X | GLOBAL | GLOBAL | LABDB | X X | (4 rows) Object Privileges (L)ist (S)elect (I)nsert (U)pdate (D)elete (T)runcate (L)ock (A)lter (D)rop a(B)ort (L)oad (G)enstats Gr(O)om (E)xecute Label-A(C)cess Label-(R)estrict Label-E(X)pand Execute-(A)s Administration Privilege (D)atabase (G)roup (U)ser (S)chema (T)able T(E)mp E(X)ternal Se(Q)uence S(Y)nonym (V)iew (M)aterialized View (I)ndex (B)ackup (R)estore va(C)uum (S)ystem (H)ardware (F)unction (A)ggregate (L)ibrary (P)rocedure U(N)fence (S)ecurity Scheduler (R)ule You can see that the CUSTADMIN user has only the rights granted. Test procedure as CUSTADMIN user, first connect to LABDB with the following command: Input: LABDB.ADMIN(ADMIN)=> [\\c labdb custadmin password]{.mark} Output: You are now connected to database labdb as user custadmin. LABDB.ADMIN(CUSTADMIN)=> INSERT another customer to the CUSTOMER table: Input: LABDB.ADMIN(CUSTADMIN)=> [call addcustomer(999997,\\'Jake Jones\\', 2, \\'555-5554\\');]{.mark} Output: ADDCUSTOMER ------------- (1 row) The insert will have been successful, and you will have another row in your table, you can check this with a SELECT query if you want. 15. Now make some changes to the stored procedure to do this connect to LABDB as ADMIN: Input: LABDB.ADMIN(CUSTADMIN)=> [\\c labdb admin]{.mark} Output: You are now connected to database labdb as user admin. LABDB.ADMIN(ADMIN)=> Modify the stored procedure but first look at the details. Input: LABDB.ADMIN(ADMIN)=> [show procedure addcustomer verbose;]{.mark} Output: SCHEMA | RESULT | PROCEDURE | BUILTIN | ARGUMENTS | OWNER | EXECUTEDASOWNER | VARARGS | DESCRIPTION | PROCEDURESOURCE --------+---------+-------------+---------+------------------------------------------------------------------+----------+-----------------+---------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ADMIN | INTEGER | ADDCUSTOMER | f | (INTEGER, CHARACTER VARYING(25), INTEGER, CHARACTER VARYING(15)) | LABADMIN | t | f | | DECLARE C_KEY ALIAS FOR \\$1; C_NAME ALIAS FOR \\$2; N_KEY ALIAS FOR \\$3; PHONE ALIAS FOR \\$4; REC RECORD; BEGIN -- SELECT * INTO REC FROM CUSTOMER WHERE C_CUSTKEY = C_KEY; -- IF FOUND REC THEN -- RAISE EXCEPTION \\'Customer with key % already exists\\', C_KEY; -- END IF; -- -- SELECT * INTO REC FROM NATION WHERE N_NATIONKEY = N_KEY; -- IF NOT FOUND REC THEN -- RAISE EXCEPTION \\'No Nation with nation key %\\', N_KEY; -- END IF; INSERT INTO CUSTOMER VALUES (C_KEY, C_NAME, \\'\\', N_KEY, PHONE, 0 ,\\'\\', \\'\\'); END; (1 row) You can see the input and output arguments, procedure name, owner, if it is executed as owner or caller and other details. Verbose also shows you the source code of the stored procedure. We see that the description field is still empty so lets add a comment to the stored procedure. This is important to do if you have a big number of stored procedures in your system. Add a description to the stored procedure: It is necessary to specify the exact stored procedure signature including the input arguments, these can be cut& pasted from the output of the show procedures command. The COMMENT ON command can be used to add descriptions to more or less all database objects you own from procedures, tables till columns. Verify that your description has been set: LABDB(ADMIN)=> show procedure addcustomer verbose; The description field will now contain your comment: We will now alter the stored procedure to be executed as the caller instead of the owner. This means that whoever executes the stored procedure needs to have access rights to all the objects that are touched in the stored procedure otherwise it will fail. This should be the default for stored procedures that encapsulate business logic and do not do extensive data checking: LABDB(ADMIN)=> alter procedure addcustomer(INTEGER, CHARACTER VARYING(25), LABDB(ADMIN)-> INTEGER,CHARACTER VARYING(15)) execute as caller; 20. Since the admin user has access to the customer table he will be able to execute the stored procedure: LABDB(ADMIN)=> [call addCustomer(999996,\\'Karl Schwarz\\', 2, \\'555-5553\\');]{.mark} Switch to the CUSTADMIN user: LABDB(ADMIN)=> [\\c labdb custadmin]{.mark} Try to add another customer as CUSTADMIN: LABDB(CUSTADMIN)=> [call addCustomer(999995, \\'John Schwarz\\', 2, \\'555-5553\\');]{.mark} You should see the following results: As expected, the stored procedure fails now. The user custadmin has read access to the CUSTOMER table but no read access to the NATION table, therefore this check results in an exception. While EXECUTE AS CALLER is more secure in some circumstances it doesn't fit our usecase where we specifically want to expose some data modification ability to a user who shouldn't be able to modify a table otherwise. Therefore we will change the stored procedure back: First switch back to the admin user: LABDB(CUSTADMIN)=> [\\c labdb admin]{.mark} 24. Change the stored procedure back to being executed as owner: LABDB(ADMIN)=> alter procedure addcustomer(INTEGER, CHARACTER VARYING(25), LABDB(ADMIN)-> INTEGER,CHARACTER VARYING(15)) execute as owner; In this chapter you setup the permissions for the addCustomer stored procedure and the user CUSTADMIN who is supposed to use it. You also added comments to the stored procedure.","title":"Managing your stored procedure"},{"location":"nz-10-Stored-Proc/#implementing-the-checkregions-stored-procedure","text":"In this chapter we will implement a stored procedure that performs a check on all rows of the regions table. The call of the stored procedure will be very simple and will not contain input arguments. The stored procedure is used to encapsulate a sanity check of the regions table that is executed regularly in the IBM Performance Server's system for administrative purposes. Our stored procedure will check each row of the REGION table for three things: If the region key is smaller than 1 If the name string is empty If the description is lower case only this is needed for application reasons. The procedure will return each row of the region table together with additional columns that describe if the above constraints are broken. It will also return a notice with the number of faulty rows. This chapter will teach you to use loops in a stored procedure and to return table results. You will also use dynamic query execution to create queries on the fly. Exit the NZSQL console by executing the \\q command and open the already existing empty file checkRegion.sql with the following command (note you can tab out the filename): [nz@netezza storedProcedure]\\$ vi checkRegion.sql You are now in the familiar vi interface and you can edit the file. Switch to INSERT mode by pressing i First, we will define the stored procedure header similar to the last procedure. It will be very simple since we will not use any input arguments. Enter the following code to the editor: Let's have a detailed look at the RETURNS section. We want to return a result set but do not have to describe the column names or datatypes of the table object that is returned. Instead we reference an existing table, which needs to exist at the time the stored procedure is created. This means we will need to create the table TB1 before executing the CREATE PROCEDURE command. Once the stored procedure is executed the stored procedure will create under the cover an empty temporary table that has the same definition as the referenced table. So, the results will not actually be saved in the referenced table, which is only used for the definition. This means that multiple stored procedures can be executed at the same time without influencing each other. Since the created table is temporary it will be cleaned up once the connection to the database is aborted. Note: If the referenced table contains rows they will neither be changed nor copied over to the temporary table, the table is strictly used for reference. For our stored procedure we need four variables, add the following lines after the BEGIN_PROC statement: DECLARE rec RECORD; errorRows INTEGER; fieldEmpty BOOLEAN; descUpper BOOLEAN; The four variables needed for our stored procedure: rec, is a RECORD structure while we loop through the rows of the table we will use it to save and access the values of each row and check them with our constraints errorRows will be used to contain the total number of rows that violate our constraints fieldEmpty will be used to store if the row violates either the constraint that the name is empty or the record code is smaller than 1, this is appropriate since values of -1 or 0 in the region code are used to denote that it is empty descUpper will be true if a record violates the constraint that the description needs to be lowercase We will now add the main BEGIN..END clause and initialize the errorRows variable. Add the following rows after the DECLARE section: BEGIN RAISE NOTICE \\'Start check of Region\\'; errorRows := 0; END; Each stored procedure must at least contain one BEGIN..END clause, which encapsulates the executed commands. We also initially set the number of error rows to 0 and display a short sentence. We will now add the main loop. It will iterate through all rows of the REGION table and store each row in the rec variable. Add the following lines before the END statement FOR rec IN SELECT * FROM REGION ORDER BY R_REGIONKEY LOOP fieldEmpty := false; descUpper := false; END LOOP; RAISE NOTICE \\' % rows had an error see result set\\', errorRows; The FOR rec IN expression LOOP..END LOOP command is used to iterate through a result set, in our case a SELECT * on the REGION table. The loop body is executed once for every row in the expression and the current row is saved in the rec field. The loop needs to be ended with the END LOOP keyword. There are many other types of loops in NZPLSQL, for a complete set refer to the stored procedure guide. For each iteration of the loop we initially set the value of the fieldEmpty and descUpper to false. Variables can be assigned with the := operator. Finally, we will display a notice that shows the number of rows that either had an empty field or upper case expression. This number will be saved in the errorRows variable. Now it's time to check the rows for our constraints and set our variables accordingly. Enter the following rows behind the variable initialization and before the END LOOP keyword: IF rec.R_NAME = \\'\\' OR rec.R_REGIONKEY \\< 1 THEN fieldEmpty := true; END IF; IF rec.R_COMMENT \\<> LOWER(rec.R_COMMENT) THEN descUpper := true; END IF; IF (fieldEmpty = true) OR (descUpper = true) THEN errorRows := errorRows + 1; END IF; In this section we check our constraints for each row and set our three variables accordingly. First, we check if the name field of the row is the empty string or if the region key is smaller than one. In that case the fieldEmpty field is set to true. Note how we can access the fields by adding the fieldname to our loop record. The second IF statement checks if the comment field of the row is different to the lower case version of the comment field. This would be the case if it contains uppercase characters. Note that we can use the available Netezza Performance Server functions like LOWER in the stored procedure, as if it were a SQL statement. Finally, if one of these variables has been set to true by the previous checks, we increase the value of the errorRows variable by one. The final number will in the end be displayed by the RAISE NOTICE statement we already added to the stored procedure. Finally add the following lines after the lines you just added and before the END LOOP statement: EXECUTE IMMEDIATE \\'INSERT INTO \\'|| REFTABLENAME ||\\' VALUES (\\' || rec.R_REGIONKEY ||\\',\\'\\'\\' || trim(rec.R_NAME) ||\\'\\'\\',\\'\\'\\' || trim(rec.R_COMMENT) ||\\'\\'\\',\\' || fieldEmpty ||\\',\\' || descUpper ||\\')\\'; These lines add the row of the REGION table to the result set of our stored procedure adding two columns containing the fieldEmpty and descUpper flags for this row. There are a couple of important points here: For each call of a stored procedure with a result set as return value a temporary table is created that is later returned to the caller. Since the name is unique it needs to be referenced through a variable. This is the REFTABLENAME variable. Apart from that, adding values to the result set is identical to other INSERT operations. Since the name of the table is dynamic we need to execute the INSERT operations as a dynamic statement. This means that the EXECUTE IMMEDIATE statement is used with a string that contains the query that is to be executed. To add variable values to the string the pipe symbol || is used. Note that the values for R_NAME and R_COMMENT are inserted as strings, which means they need to be surrounded by quotes. To add quotes to a string they need to be escaped with a second quote character. This is the reason that R_NAME and R_COMMENT is surrounded by triple quotes. Apart from that we trim them, so the inserted VARCHAR values are not blown up with empty characters. It can be tricky to construct a string like that and you will see the error only once it is executed. For debugging it can be useful to construct the string and display it with a RAISE NOTICE statement. Your vi should now look like that, containing the complete stored procedure: CREATE OR REPLACE PROCEDURE checkRegions() LANGUAGE NZPLSQL RETURNS REFTABLE(tb1) AS BEGIN_PROC DECLARE rec RECORD; errorRows INTEGER; fieldEmpty BOOLEAN; descUpper BOOLEAN; BEGIN RAISE NOTICE \\'Start check of Region\\'; errorRows := 0; FOR rec IN SELECT * FROM REGION ORDER BY R_REGIONKEY LOOP fieldEmpty := false; descUpper := false; IF rec.R_NAME = \\'\\' OR rec.R_REGIONKEY \\< 1 THEN fieldEmpty := true; END IF; IF rec.R_COMMENT \\<> lower(rec.R_COMMENT) THEN descUpper := true; END IF; IF (fieldEmpty = true) OR (descUpper = true) THEN errorRows := errorRows + 1; END IF; EXECUTE IMMEDIATE \\'INSERT INTO \\'|| REFTABLENAME ||\\' VALUES (\\' || rec.R_REGIONKEY ||\\',\\'\\'\\' || trim(rec.R_NAME) ||\\'\\'\\',\\'\\'\\' || trim(rec.R_COMMENT) ||\\'\\'\\',\\' || fieldEmpty ||\\',\\' || descUpper ||\\')\\'; END LOOP; RAISE NOTICE \\' % rows had an error see result set\\', errorRows; END; END_PROC; Save and exit vi. Press ESC to enter the command mode, enter :wq! to save and force quit and press enter. Enter nzsql and connect to LABDB as user LABADMIN. [nz@netezza storedProcedure]\\$ [nzsql LABDB ADMIN]{.mark} To create the stored procedure the table reference TB1 needs to exist. Create the table with the following statement: LABDB(ADMIN)=> [create table TB1 as]{.mark} LABDB(ADMIN)-> [select *, false AS FIELDEMPTY, false as DESCUPPER]{.mark} LABDB(ADMIN)-> [from region limit 0;]{.mark} This command creates a table TB1 that has all the rows of the REGION table and two additional BOOLEAN fields FIELDNULL and DESCUPPER. It will also be empty because we used the LIMIT 0 clause. Describe the reference table with LABDB(ADMIN)=> \\d TB1 You should see the following result: Table \\\"TB1\\\" Attribute | Type | Modifier | Default Value -------------+------------------------+----------+--------------- R_REGIONKEY | INTEGER | NOT NULL | R_NAME | CHARACTER(40) | | R_COMMENT | CHARACTER VARYING(152) | | FIELDEMPTY | BOOLEAN | | DESCUPPER | BOOLEAN | | Distributed on hash: \\\"R_REGIONKEY\\\" You can see the three columns of the REGION table and the two additional BOOLEAN fields that will contain for each row if the row violates the specified constraints. Note this table needs to exist before the procedure can be created. Now create the stored procedure. Execute the script you just created with the following command: LABDB(ADMIN)=> \\i checkRegion.sql You should successfully create your stored procedure. Now let's have a look at our REGION table, select all rows: LABDB(ADMIN)=> SELECT * FROM REGION; You will get the following results: LABDB(ADMIN)=> SELECT * FROM REGION; R_REGIONKEY | R_NAME | R_COMMENT -------------+---------------------------+----------------------------- 2 | sa | south america 1 | na | north america 4 | ap | asia pacific 3 | emea | europe, middle east, africa (4 rows) We can see that none of the rows would violate the constraints we defined which would be pretty boring. So lets test our stored procedure by adding two rows that violate our constraints. Add the two violating rows with the following commands: LABDB(ADMIN)=> INSERT INTO REGION VALUES (0, \\'as\\', \\'Australia\\'); This row violates the lower case constraints for the comment field and the empty field constraint for the region key LABDB(ADMIN)=> INSERT INTO REGION VALUES (6, \\'\\', \\'mongolia\\'); This row violates the empty field constraint for the region name. Now finally let's try our checkRegions stored procedure: LABDB(ADMIN)=> call checkRegions(); You should see the following output: NOTICE: Start check of Region NOTICE: 2 rows had an error see result set R_REGIONKEY | R_NAME | R_COMMENT | FIELDEMPTY | DESCUPPER -------------+---------------------------+-----------------------------+------------+----------- 1 | na | north america | f | f 3 | emea | europe, middle east, africa | f | f 0 | as | Australia | t | t 4 | ap | asia pacific | f | f 2 | sa | south america | f | f 6 | | mongolia | t | f (6 rows) You can see the expected results. Our stored procedure has found two rows that violated the constraints we check for. In the FIELDNULL and DESCUPPER columns we can easily see that the row with the key 0 has both an empty field and uppercase comment. We can also see that row 6 only violated the empty field constraint. Note that the TB1 table we created doesn't contain any rows, it is only used as a template. Finally let's cleanup our REGION table again: LABDB(ADMIN)=> DELETE FROM REGION WHERE R_REGIONKEY = 0 OR R_REGIONKEY = 6; And let's run our checkRegions procedure again: LABDB(ADMIN)=> call checkRegions(); You will see the following results: NOTICE: Start check of Region NOTICE: 0 rows had an error see result set R_REGIONKEY | R_NAME. | R_COMMENT | FIELDEMPTY | DESCUPPER -------------+---------------------------+-----------------------------+------------+----------- 3 | emea | europe, middle east, africa | f | f 4 | ap | asia pacific. | f | f 1 | na | north America | f | f 2 | sa. | south america | f | f (4 rows) You can see that the table now is error free and all constraint violation fields are false. Congratulations you have finished the stored procedure lab and created two stored procedures that help you to manage your database.","title":"Implementing the checkRegions stored procedure"},{"location":"nz-acknowledgements/","text":"Acknowledgments We would like to thank all the development team for helping to deliver this release given the tremendous deadlines and constraints that they have been under. The initial lab was created by Deepak Rangarao with contributions from development. Additional material was supplied by Daniel Hancock and feedback from the members of the watsonx.data activation community. Formatting and script development was done by George Baklarz. The contents of this eBook are the result of a lot of research and testing based on the contents of watsonx.data. Results are based on a specific version of watsonx.data, so you may have different results if using an older or newer version of the development kit. Support For any questions regarding the lab, including any suggestions, general comments, or bug reports, please contact: George Baklarz baklarz@ca.ibm.com Daniel Hancock daniel.hancock@us.ibm.com We would also appreciate any feedback on the successful use of the lab. Thanks for using watsonx.data! Dan, Deepak & George","title":"Acknowledgements"},{"location":"nz-acknowledgements/#acknowledgments","text":"We would like to thank all the development team for helping to deliver this release given the tremendous deadlines and constraints that they have been under. The initial lab was created by Deepak Rangarao with contributions from development. Additional material was supplied by Daniel Hancock and feedback from the members of the watsonx.data activation community. Formatting and script development was done by George Baklarz. The contents of this eBook are the result of a lot of research and testing based on the contents of watsonx.data. Results are based on a specific version of watsonx.data, so you may have different results if using an older or newer version of the development kit.","title":"Acknowledgments"},{"location":"nz-acknowledgements/#support","text":"For any questions regarding the lab, including any suggestions, general comments, or bug reports, please contact: George Baklarz baklarz@ca.ibm.com Daniel Hancock daniel.hancock@us.ibm.com We would also appreciate any feedback on the successful use of the lab. Thanks for using watsonx.data! Dan, Deepak & George","title":"Support"},{"location":"nz-disclaimer/","text":"Disclaimer Watson.data Copyright \u00a9 2024 by International Business Machines Corporation (IBM). All rights reserved. Printed in Canada. Except as permitted under the Copyright Act of 1976, no part of this publication may be reproduced or distributed in any form or by any means, or stored in a database or retrieval system, without the prior written permission of IBM, with the exception that the program listings may be entered, stored, and executed in a computer system, but they may not be reproduced for publication. The contents of this lab represent those features that may or may not be available in the current release of any products mentioned within this lab despite what the lab may say. IBM reserves the right to include or exclude any functionality mentioned in this lab for the current release of watsonx.data, or a subsequent release. In addition, any claims made in this lab are not official communications by IBM; rather, they are observed by the authors in unaudited testing and research. The views expressed in this lab is those of the authors and not necessarily those of the IBM Corporation; both are not liable for any of the claims, assertions, or contents in this lab. IBM's statements regarding its plans, directions, and intent are subject to change or withdrawal without notice and at IBM's sole discretion. Information regarding potential future products is intended to outline our general product direction and it should not be relied on in making a purchasing decision. The information mentioned regarding potential future products is not a commitment, promise, or legal obligation to deliver any material, code, or functionality. Information about potential future products may not be incorporated into any contract. The development, release, and timing of any future feature or functionality described for our products remains at our sole discretion. Performance is based on measurements and projections using standard IBM benchmarks in a controlled environment. The actual throughput or performance that any user will experience will vary depending upon many factors, including considerations such as the amount of multiprogramming in the user's job stream, the I/O configuration, the storage configuration, and the workload processed. Therefore, no assurance can be given that an individual user will achieve results like those stated here. U.S. Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM. Information in this eBook (including information relating to products that have not yet been announced by IBM) has been reviewed for accuracy as of the date of initial publication and could include unintentional technical or typographical errors. IBM shall have no responsibility to update this information. THIS DOCUMENT IS DISTRIBUTED \"AS IS\" WITHOUT ANY WARRANTY, EITHER EXPRESS OR IMPLIED. IN NO EVENT SHALL IBM BE LIABLE FOR ANY DAMAGE ARISING FROM THE USE OF THIS INFORMATION, INCLUDING BUT NOT LIMITED TO, LOSS OF DATA, BUSINESS INTERRUPTION, LOSS OF PROFIT OR LOSS OF OPPORTUNITY. IBM products and services are warranted according to the terms and conditions of the agreements under which they are provided. References in this document to IBM products, programs, or services does not imply that IBM intends to make such products, programs, or services available in all countries in which IBM operates or does business. Information concerning non-IBM products was obtained from the suppliers of those products, their published announcements, or other publicly available sources. IBM has not tested those products in connection with this publication and cannot confirm the accuracy of performance, compatibility or any other claims related to non-IBM products. Questions on the capabilities of non-IBM products should be addressed to the suppliers of those products. IBM does not warrant the quality of any third-party products, or the ability of any such third-party products to interoperate with IBM's products. IBM EXPRESSLY DISCLAIMS ALL WARRANTIES, EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. The provision of the information contained herein is not intended to, and does not, grant any right or license under any IBM patents, copyrights, trademarks, or other intellectual property right. IBM, the IBM logo, ibm.com, Aspera\u00ae, Bluemix, Blueworks Live, CICS, Clearcase, Cognos\u00ae, DOORS\u00ae, Emptoris\u00ae, Enterprise Document Management System\u2122, FASP\u00ae, FileNet\u00ae, Global Business Services \u00ae, Global Technology Services \u00ae, IBM ExperienceOne\u2122, IBM SmartCloud\u00ae, IBM Social Business\u00ae, Information on Demand, ILOG, Maximo\u00ae, MQIntegrator\u00ae, MQSeries\u00ae, Netcool\u00ae, OMEGAMON, OpenPower, PureAnalytics\u2122, PureApplication\u00ae, pureCluster\u2122, PureCoverage\u00ae, PureData\u00ae, PureExperience\u00ae, PureFlex\u00ae, pureQuery\u00ae, pureScale\u00ae, PureSystems\u00ae, QRadar\u00ae, Rational\u00ae, Rhapsody\u00ae, Smarter Commerce\u00ae, SoDA, SPSS, Sterling Commerce\u00ae, StoredIQ, Tealeaf\u00ae, Tivoli\u00ae, Trusteer\u00ae, Unica\u00ae, urban{code}\u00ae, Watson, WebSphere\u00ae, Worklight\u00ae, X-Force\u00ae and System z\u00ae Z/OS, are trademarks of International Business Machines Corporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the Web at \"Copyright and trademark information\" at: www.ibm.com/legal/copytrade.shtml. All trademarks or copyrights mentioned herein are the possession of their respective owners and IBM makes no claim of ownership by the mention of products that contain these marks.","title":"Disclaimer"},{"location":"nz-disclaimer/#disclaimer","text":"","title":"Disclaimer"},{"location":"nz-disclaimer/#watsondata","text":"Copyright \u00a9 2024 by International Business Machines Corporation (IBM). All rights reserved. Printed in Canada. Except as permitted under the Copyright Act of 1976, no part of this publication may be reproduced or distributed in any form or by any means, or stored in a database or retrieval system, without the prior written permission of IBM, with the exception that the program listings may be entered, stored, and executed in a computer system, but they may not be reproduced for publication. The contents of this lab represent those features that may or may not be available in the current release of any products mentioned within this lab despite what the lab may say. IBM reserves the right to include or exclude any functionality mentioned in this lab for the current release of watsonx.data, or a subsequent release. In addition, any claims made in this lab are not official communications by IBM; rather, they are observed by the authors in unaudited testing and research. The views expressed in this lab is those of the authors and not necessarily those of the IBM Corporation; both are not liable for any of the claims, assertions, or contents in this lab. IBM's statements regarding its plans, directions, and intent are subject to change or withdrawal without notice and at IBM's sole discretion. Information regarding potential future products is intended to outline our general product direction and it should not be relied on in making a purchasing decision. The information mentioned regarding potential future products is not a commitment, promise, or legal obligation to deliver any material, code, or functionality. Information about potential future products may not be incorporated into any contract. The development, release, and timing of any future feature or functionality described for our products remains at our sole discretion. Performance is based on measurements and projections using standard IBM benchmarks in a controlled environment. The actual throughput or performance that any user will experience will vary depending upon many factors, including considerations such as the amount of multiprogramming in the user's job stream, the I/O configuration, the storage configuration, and the workload processed. Therefore, no assurance can be given that an individual user will achieve results like those stated here. U.S. Government Users Restricted Rights - Use, duplication or disclosure restricted by GSA ADP Schedule Contract with IBM. Information in this eBook (including information relating to products that have not yet been announced by IBM) has been reviewed for accuracy as of the date of initial publication and could include unintentional technical or typographical errors. IBM shall have no responsibility to update this information. THIS DOCUMENT IS DISTRIBUTED \"AS IS\" WITHOUT ANY WARRANTY, EITHER EXPRESS OR IMPLIED. IN NO EVENT SHALL IBM BE LIABLE FOR ANY DAMAGE ARISING FROM THE USE OF THIS INFORMATION, INCLUDING BUT NOT LIMITED TO, LOSS OF DATA, BUSINESS INTERRUPTION, LOSS OF PROFIT OR LOSS OF OPPORTUNITY. IBM products and services are warranted according to the terms and conditions of the agreements under which they are provided. References in this document to IBM products, programs, or services does not imply that IBM intends to make such products, programs, or services available in all countries in which IBM operates or does business. Information concerning non-IBM products was obtained from the suppliers of those products, their published announcements, or other publicly available sources. IBM has not tested those products in connection with this publication and cannot confirm the accuracy of performance, compatibility or any other claims related to non-IBM products. Questions on the capabilities of non-IBM products should be addressed to the suppliers of those products. IBM does not warrant the quality of any third-party products, or the ability of any such third-party products to interoperate with IBM's products. IBM EXPRESSLY DISCLAIMS ALL WARRANTIES, EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. The provision of the information contained herein is not intended to, and does not, grant any right or license under any IBM patents, copyrights, trademarks, or other intellectual property right. IBM, the IBM logo, ibm.com, Aspera\u00ae, Bluemix, Blueworks Live, CICS, Clearcase, Cognos\u00ae, DOORS\u00ae, Emptoris\u00ae, Enterprise Document Management System\u2122, FASP\u00ae, FileNet\u00ae, Global Business Services \u00ae, Global Technology Services \u00ae, IBM ExperienceOne\u2122, IBM SmartCloud\u00ae, IBM Social Business\u00ae, Information on Demand, ILOG, Maximo\u00ae, MQIntegrator\u00ae, MQSeries\u00ae, Netcool\u00ae, OMEGAMON, OpenPower, PureAnalytics\u2122, PureApplication\u00ae, pureCluster\u2122, PureCoverage\u00ae, PureData\u00ae, PureExperience\u00ae, PureFlex\u00ae, pureQuery\u00ae, pureScale\u00ae, PureSystems\u00ae, QRadar\u00ae, Rational\u00ae, Rhapsody\u00ae, Smarter Commerce\u00ae, SoDA, SPSS, Sterling Commerce\u00ae, StoredIQ, Tealeaf\u00ae, Tivoli\u00ae, Trusteer\u00ae, Unica\u00ae, urban{code}\u00ae, Watson, WebSphere\u00ae, Worklight\u00ae, X-Force\u00ae and System z\u00ae Z/OS, are trademarks of International Business Machines Corporation, registered in many jurisdictions worldwide. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on the Web at \"Copyright and trademark information\" at: www.ibm.com/legal/copytrade.shtml. All trademarks or copyrights mentioned herein are the possession of their respective owners and IBM makes no claim of ownership by the mention of products that contain these marks.","title":"Watson.data"},{"location":"nz-lab-instructions/","text":"Lab Instructions Lab instructions may contain three types of information: Screen (UI) interactions Text commands Dialog Close Keyboard or Mouse Action When a keyboard or mouse action is required, the text will include a box with the instructions. Select the Infrastructure Icon in the watsonx.data UI Command Text Any text that needs to be typed into the system will be outlined in a grey box. Enter this text into the SQL window and Run the code SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; A copy icon is usually found on the far right-hand side of the command box. Use this to copy the text and paste it into your dialog or command window. You can also select the text and copy it that way. Once you have copied the text, paste the value into the appropriate dialog using the paste command or menu. Cut and Paste Considerations Some commands may span multiple lines, so make sure you copy everything in the box if you are not using the copy button. Commands pasted into a terminal window will require that you hit the Return or Enter key for the command to be executed. Commands pasted into a Presto CLI window will execute automatically. Dialog Close There are instructions that will tell you to close the current dialog. Close the dialog by pressing the [x] in the corner In many cases you will also be able to use the Escape key to close the current window. Icon Reference There are certain menu icons that are referred to throughout the lab that have specific names: Hamburger menu \u2261 This icon is used to display menu items that a user would select from. Kebab menu \u22ee This icon is usually used to indicate that there are specific actions that can be performed against an object. Twisty \u25ba and \u25bc Used to expand and collapse lists. URL Conventions Your TechZone reservation contains a number of URLs for the services provided in the watsonx.data server. The URL will contain the name of the server and the corresponding port number for the service. Throughout the documentation, the server name will be referred to as region.services.cloud.techzone.ibm.com and port number is referred to as port . Where you see these URLs, replace them with the values found in your reservation.","title":"Lab Instructions"},{"location":"nz-lab-instructions/#lab-instructions","text":"Lab instructions may contain three types of information: Screen (UI) interactions Text commands Dialog Close","title":"Lab Instructions"},{"location":"nz-lab-instructions/#keyboard-or-mouse-action","text":"When a keyboard or mouse action is required, the text will include a box with the instructions. Select the Infrastructure Icon in the watsonx.data UI","title":"Keyboard or Mouse Action"},{"location":"nz-lab-instructions/#command-text","text":"Any text that needs to be typed into the system will be outlined in a grey box. Enter this text into the SQL window and Run the code SELECT * FROM \"hive_data\".\"ontime\".\"ontime\" LIMIT 10; A copy icon is usually found on the far right-hand side of the command box. Use this to copy the text and paste it into your dialog or command window. You can also select the text and copy it that way. Once you have copied the text, paste the value into the appropriate dialog using the paste command or menu. Cut and Paste Considerations Some commands may span multiple lines, so make sure you copy everything in the box if you are not using the copy button. Commands pasted into a terminal window will require that you hit the Return or Enter key for the command to be executed. Commands pasted into a Presto CLI window will execute automatically.","title":"Command Text"},{"location":"nz-lab-instructions/#dialog-close","text":"There are instructions that will tell you to close the current dialog. Close the dialog by pressing the [x] in the corner In many cases you will also be able to use the Escape key to close the current window.","title":"Dialog Close"},{"location":"nz-lab-instructions/#icon-reference","text":"There are certain menu icons that are referred to throughout the lab that have specific names: Hamburger menu \u2261 This icon is used to display menu items that a user would select from. Kebab menu \u22ee This icon is usually used to indicate that there are specific actions that can be performed against an object. Twisty \u25ba and \u25bc Used to expand and collapse lists.","title":"Icon Reference"},{"location":"nz-lab-instructions/#url-conventions","text":"Your TechZone reservation contains a number of URLs for the services provided in the watsonx.data server. The URL will contain the name of the server and the corresponding port number for the service. Throughout the documentation, the server name will be referred to as region.services.cloud.techzone.ibm.com and port number is referred to as port . Where you see these URLs, replace them with the values found in your reservation.","title":"URL Conventions"}]}